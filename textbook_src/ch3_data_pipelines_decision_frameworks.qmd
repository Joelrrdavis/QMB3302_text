---
title: "Data Pipelines and Decision Frameworks"
---

## Data Pipelines and Decision Frameworks

(this chapter still has draft content, and lot's of typo's.)

### How Systems Acquire Data

Analytics and AI systems need data. The data that is available, how timely and appropriate it is for the problem at hand, it's reliability and how representative that data is are all shaped by the **data aquisition** process. 

**Operational systems** are common source of data. These include things like transaction records, customer relationship systems, resource planning platforms and web applications. In most of these examples, data could be thought of as the byproduct of the operation, rather than something collected for analysis. This is why many companies struggle to "bring their data together" into one cohesive form. It was not designed from the start to be a data pipeline, that happened later. This data is almost always rich, and high-volume, properties that AI systems like. It does however also reflect the incentives of the creators of those operation systems, the structures and limits they had in place when the system was first stood up. 

Another good source of data can be found in **instrumention and logging** systems. Businesses frequently have logs of events withing their IT systems, such as website clicks, application use, or sensor readings from devices. To be of value, this kind of data requires thoughtful and deliberate design decisions- what should we record? At what level of detail?

Data may also be acquired through **external sources**. These include public datasets, third-party data providers, APIs, and partner organizations. External data can enrich internal records by providing additional context, such as demographic information, market indicators, or environmental conditions. However, reliance on external sources introduces dependencies related to data quality, licensing, update frequency, and long-term availability.

Across these sources, data can be collected in different modes. **Batch acquisition** involves gathering data at regular intervals, such as daily or weekly extracts, and is common in reporting and strategic analysis. **Streaming or real-time acquisition** captures data continuously as events occur and is often used in monitoring, personalization, or fraud detection systems. The choice between batch and streaming acquisition affects system responsiveness, complexity, and infrastructure requirements.

Importantly, data acquisition is not a passive process. Decisions about what to collect, how to define variables, and how frequently to record observations embed assumptions into the system. These assumptions influence downstream analysis and decision-making, sometimes in subtle ways. Understanding how data enters a system is therefore a critical step in evaluating both the capabilities and the limitations of analytics and AI applications.

### Data Preprocessing in Pipelines

Raw data as it is collected is rarely ready for analysis or modeling. It may be noisy, incomplete, duplicated, or inconsistently formatted. **Data preprocessing** refers to the set of steps that transform raw inputs into data that is usable, reliable, and aligned with downstream decision needs.

Typical preprocessing activities include:

- **Data cleaning** – Fixing or removing errors, handling missing values, resolving inconsistent labels, and removing clear duplicates.  
- **Data integration** – Combining data from multiple systems (e.g., CRM, ERP, web analytics) into a unified view, aligning schemas and resolving conflicts.  
- **Data transformation** – Converting data into appropriate formats, encodings, or scales, such as normalizing numeric fields or standardizing date formats.  
- **Data reduction** – Optionally reducing the number of variables or records to those that are relevant for the task.

Preprocessing is often where the “**garbage in, garbage out**” principle is felt most strongly. Even sophisticated models cannot compensate for fundamentally low-quality inputs. In practice, a substantial portion of analytical effort is spent at this stage, ensuring that data is a trustworthy representation of the phenomena an organization intends to reason about.

### Data Integration and Joinability

Many high-value use cases depend not on a single source, but on **joining** multiple sources together. For example, an organization might combine:

- customer attributes from a CRM system,  
- transaction history from an order management system, and  
- behavior data from a website or mobile app.

For this to work, data must be **joinable**. That usually requires:

- stable **identifiers** (such as customer_id, account_id, or device_id),  
- compatible **time representations** (e.g., time zones, formats, and granularities), and  
- aligned **schemas** (column meanings, units, and encodings).

Breakdowns in integration—such as mismatched IDs or inconsistent definitions—can lead to partial or fragmented views of the same entity. From the perspective of a pipeline, this means the model and decision framework are operating on an incomplete or distorted view of reality, even if each underlying source system is functioning as designed.

### Data Quality Dimensions

Data quality is not a single property; it is multi-dimensional. Several recurring dimensions are especially important in analytics and AI contexts:

| **Dimension**   | **Description**                                      | **Impact on AI Models**                                           |
|----------------|-------------------------------------------------------|-------------------------------------------------------------------|
| **Accuracy**    | Data correctly reflects real-world values.           | Inaccurate data leads to systematically wrong predictions.        |
| **Completeness**| All required fields and records are present.         | Missing data creates gaps, biasing learning and evaluation.       |
| **Consistency** | Data is uniform and free of internal contradictions. | Inconsistent values confuse models and degrade learned patterns.  |
| **Timeliness**  | Data is up-to-date and available when needed.        | Stale data produces predictions that are misaligned with reality. |
| **Validity**    | Data conforms to expected formats and allowed ranges.| Invalid values can break pipelines or skew learned relationships. |
| **Uniqueness**  | Each entity is represented once (no unintended duplicates). | Duplicates can overweight specific patterns and induce bias. |

Well-designed pipelines monitor and enforce these dimensions explicitly. From a systems perspective, **data quality is part of system design**, not an afterthought. Decisions about how strictly to enforce each dimension reflect domain constraints, regulatory requirements, and the tolerance for risk in downstream decisions.

### Pipelines to Decision Frameworks

A data pipeline does not exist in isolation. Its purpose is to support **decisions** by moving raw data through a sequence of steps that make it usable, interpretable, and actionable. Understanding how pipelines connect to decision frameworks helps clarify how analytics and AI systems translate information into outcomes.

A typical data pipeline begins with acquisition and continues through stages such as cleaning, transformation, storage, and aggregation. Each stage prepares the data for the next, addressing issues such as missing values, inconsistent formats, or incompatible sources. While these steps are often treated as technical details, they play a central role in shaping what information ultimately reaches models and decision-makers.

Once data has been processed, it enters the **decision framework**. This framework defines what decision is being supported or automated, what objectives are being pursued, and what constraints must be respected. In analytical settings, the output of the pipeline may feed dashboards or reports that inform human judgment. In AI-driven settings, processed data may be passed directly to models whose outputs trigger actions or recommendations.

Decision frameworks also specify **how outputs are evaluated and acted upon**. This includes defining thresholds, priorities, costs, and trade-offs. For example, a predictive model may estimate risk, but the decision framework determines what level of risk warrants intervention, how limited resources are allocated, and what actions are permissible. These choices are rarely purely technical; they reflect organizational goals and values.

The connection between pipelines and decision frameworks is often iterative rather than linear. As decisions are made and actions are taken, new data is generated and fed back into the pipeline. This feedback loop can be used to monitor performance, update models, or revise decision rules over time. Effective systems are designed with this dynamic interaction in mind rather than treating pipelines as one-time processes.

<figure>
  <img src="../assets/images/1_feedback_loop.svg"
       alt="Feedback loop diagram showing a decision system where data and a model produce actions, actions lead to outcomes or behavior, and a dashed arrow returns from outcomes back to data to indicate that deployment changes future data.">
  <figcaption>Deployment creates feedback: actions change outcomes, and outcomes change the data the system learns from next.</figcaption>
</figure>

Viewing pipelines and decision frameworks together reinforces a systems-level perspective. Data pipelines make information available, but decision frameworks determine how that information is used. Both are necessary for analytics and AI systems to function effectively, and weaknesses in either can undermine the overall quality of decisions.

### Common Failure Modes

Even when individual components appear well designed, analytics and AI systems can fail in predictable ways. Many of these failures arise not from a single mistake, but from **misalignments between data pipelines, models, and decision frameworks**. Recognizing common failure modes makes it easier to diagnose problems and to design systems that are more resilient.

One common failure occurs when **data pipelines drift away from decision needs**. Data may be collected because it is easy to capture rather than because it is relevant to the decision at hand. Over time, pipelines can accumulate variables and transformations that no longer align with current objectives, leading to analyses that are technically correct but operationally unhelpful.

Another frequent issue is **stale or delayed data**. When pipelines operate on batch schedules that are too slow for the decisions they support, outputs may be outdated by the time they are used. This is especially problematic in environments where conditions change rapidly. In such cases, system performance degrades not because models are inaccurate, but because they are responding to yesterday’s information.

Failures also occur when **feedback loops are ignored or misunderstood**. Decisions based on model outputs often influence future data, which in turn affects subsequent model behavior. If these feedback effects are not anticipated, systems can reinforce undesirable patterns, amplify noise, or create misleading signals that appear as genuine trends.

A further source of failure lies in **overconfidence in automation**. When decision frameworks rely too heavily on model outputs without sufficient monitoring or human oversight, small errors can scale quickly. Conversely, overly cautious frameworks that ignore model outputs may negate potential benefits. Balancing automation and control is therefore a persistent design challenge.

Finally, failure can result from **organizational misalignment**. Even well-designed pipelines and models can produce poor outcomes if incentives, responsibilities, or governance structures are unclear. Decisions about who owns the data, who is accountable for outcomes, and how performance is evaluated play a critical role in system success.

Effective analytics and AI systems depend on alignment across technical and organizational dimensions. Addressing failures requires looking beyond individual components to the system as a whole. When data pipelines fail, the resulting problems often appear downstream, sometimes much later anbd in different areas of the business,  as inconsistent summaries, unstable predictions, or incorrect decisions.