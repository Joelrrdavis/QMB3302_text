[
  {
    "objectID": "guides/using_colab.html",
    "href": "guides/using_colab.html",
    "title": "Using Google Colab for This Course",
    "section": "",
    "text": "Google Colab is a free, cloud-based environment for running Python notebooks.\nYou can use it from any computer without installing Python locally. It is especially helpful if:\n\nYour computer cannot install packages reliably\n\nYou are troubleshooting your Python installation\n\nYou want quick access to a working Python environment\n\nThis guide explains how to open Colab, upload or access notebooks, install packages, work with files, and download your work."
  },
  {
    "objectID": "guides/using_colab.html#temporary-upload",
    "href": "guides/using_colab.html#temporary-upload",
    "title": "Using Google Colab for This Course",
    "section": "Temporary Upload",
    "text": "Temporary Upload\nfrom google.colab import files\nuploaded = files.upload()"
  },
  {
    "objectID": "guides/using_colab.html#mount-google-drive",
    "href": "guides/using_colab.html#mount-google-drive",
    "title": "Using Google Colab for This Course",
    "section": "Mount Google Drive",
    "text": "Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\nYour Drive will appear at:\n/content/drive/MyDrive/"
  },
  {
    "objectID": "guides/install_requirements.html",
    "href": "guides/install_requirements.html",
    "title": "Installing the Course Requirements and Checking Your Environment",
    "section": "",
    "text": "This guide will walk you through:\n\nDownloading the course requirements file\n\nUsing the terminal / command line to navigate to your course folder\n\nInstalling the required Python packages with pip install -r requirements.txt\n\nRunning a small Python script to confirm that everything is set up correctly\n\nBy the end of this guide, your Python environment should be ready for the rest of the semester."
  },
  {
    "objectID": "guides/install_requirements.html#download-the-files",
    "href": "guides/install_requirements.html#download-the-files",
    "title": "Installing the Course Requirements and Checking Your Environment",
    "section": "1.1 — Download the files",
    "text": "1.1 — Download the files\n\nGo to the location where your instructor has shared the files.\n\nDownload requirements.txt.\n\nDownload check_environment.py."
  },
  {
    "objectID": "guides/install_requirements.html#save-them-to-your-week-1-folder",
    "href": "guides/install_requirements.html#save-them-to-your-week-1-folder",
    "title": "Installing the Course Requirements and Checking Your Environment",
    "section": "1.2 — Save them to your Week 1 folder",
    "text": "1.2 — Save them to your Week 1 folder\nSave both files in:\nAI_Course/week01/\nSo your folder structure looks like:\nAI_Course/\n  week01/\n    requirements.txt\n    check_environment.py\n  week02/\n  ..."
  },
  {
    "objectID": "guides/install_requirements.html#windows-vs-code-recommended",
    "href": "guides/install_requirements.html#windows-vs-code-recommended",
    "title": "Installing the Course Requirements and Checking Your Environment",
    "section": "Windows (VS Code recommended)",
    "text": "Windows (VS Code recommended)\n\nOpen VS Code.\n\nClick File → Open Folder… → select AI_Course/week01/.\n\nGo to View → Terminal."
  },
  {
    "objectID": "guides/install_requirements.html#macos-linux",
    "href": "guides/install_requirements.html#macos-linux",
    "title": "Installing the Course Requirements and Checking Your Environment",
    "section": "macOS / Linux",
    "text": "macOS / Linux\n\nOpen Terminal.\n\nUse cd to navigate to your Week 1 folder (see next step)."
  },
  {
    "objectID": "textbook_src/week03_main.html",
    "href": "textbook_src/week03_main.html",
    "title": "Week 3 – Data Structures & Pandas",
    "section": "",
    "text": "This chapter will introduce essential data structures and basic data manipulation.\n\n\n\nLists, dictionaries, and tuples\n\nPandas DataFrames\n\nImporting, cleaning, and selecting data\n\n\n\n\n(links will be added later)"
  },
  {
    "objectID": "textbook_src/week03_main.html#week-3-overview",
    "href": "textbook_src/week03_main.html#week-3-overview",
    "title": "Week 3 – Data Structures & Pandas",
    "section": "",
    "text": "This chapter will introduce essential data structures and basic data manipulation.\n\n\n\nLists, dictionaries, and tuples\n\nPandas DataFrames\n\nImporting, cleaning, and selecting data\n\n\n\n\n(links will be added later)"
  },
  {
    "objectID": "textbook_src/week01_main.html",
    "href": "textbook_src/week01_main.html",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "",
    "text": "What “analytics” means, and the four common types: descriptive, diagnostic, predictive, and prescriptive.\nWhat “AI” means in practice—and why it often involves judgment, uncertainty, and automation.\nHow analytics and AI overlap, and how they differ based on how results are used in decision-making.\nA simple way to break down AI systems into data, models, and logic.\nThe major AI approaches (symbolic, machine learning, deep learning) and what each is good at.\nHow data gets into real systems, and why pipelines matter for decision-making.\n\n\n\n\n\nA working course project folder with a .venv created via uv\nSeveral runnable Python scripts (.py) you can execute from the terminal\n\n\n\n\nBy the end of this week, you should be able to:\n\nExplain what analytics is and how it differs from (and overlaps with) artificial intelligence.\nDescribe AI systems using the Data + Models + Logic framework.\nIdentify the major AI paradigms (symbolic, statistical, neural) and explain how they differ conceptually.\nExplain how data pipelines support decision-making in modern analytics and AI systems.\n\n\n\n\nBy the end of this week, you should be able to:\n\nRun a Python script (.py) from the terminal.\nNavigate folders and files using basic shell commands.\nUnderstand the role of virtual environments in Python workflows.\nCreate and activate a virtual environment using uv.\nVerify that your local Python environment is correctly configured for the course."
  },
  {
    "objectID": "textbook_src/week01_main.html#week-1-overview",
    "href": "textbook_src/week01_main.html#week-1-overview",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "",
    "text": "What “analytics” means, and the four common types: descriptive, diagnostic, predictive, and prescriptive.\nWhat “AI” means in practice—and why it often involves judgment, uncertainty, and automation.\nHow analytics and AI overlap, and how they differ based on how results are used in decision-making.\nA simple way to break down AI systems into data, models, and logic.\nThe major AI approaches (symbolic, machine learning, deep learning) and what each is good at.\nHow data gets into real systems, and why pipelines matter for decision-making.\n\n\n\n\n\nA working course project folder with a .venv created via uv\nSeveral runnable Python scripts (.py) you can execute from the terminal\n\n\n\n\nBy the end of this week, you should be able to:\n\nExplain what analytics is and how it differs from (and overlaps with) artificial intelligence.\nDescribe AI systems using the Data + Models + Logic framework.\nIdentify the major AI paradigms (symbolic, statistical, neural) and explain how they differ conceptually.\nExplain how data pipelines support decision-making in modern analytics and AI systems.\n\n\n\n\nBy the end of this week, you should be able to:\n\nRun a Python script (.py) from the terminal.\nNavigate folders and files using basic shell commands.\nUnderstand the role of virtual environments in Python workflows.\nCreate and activate a virtual environment using uv.\nVerify that your local Python environment is correctly configured for the course."
  },
  {
    "objectID": "textbook_src/week01_main.html#welcome-to-the-course-and-how-it-works",
    "href": "textbook_src/week01_main.html#welcome-to-the-course-and-how-it-works",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "1. Welcome to the Course and How It Works",
    "text": "1. Welcome to the Course and How It Works\nThis section is not intended to replace the syllabus. It is a brief outline of some of the course philosophy.\n\n1.1 Course outcomes and the two-track approach (AI literacy + job skills)\nThis course is designed around a simple but intentional idea: understanding AI is not the same as being able to work with it. Many courses emphasize one at the expense of the other—either focusing heavily on conceptual discussions with little hands-on skill development, or emphasizing tools and code without building a durable mental model of what those tools are actually doing.\nThis course explicitly pursues two parallel outcomes, which we will refer to throughout the semester as AI literacy and job-ready skills.\nAI literacy refers to your ability to reason about analytics and AI systems. This includes understanding what problems these systems are designed to solve, how they are structured, where they tend to fail, and how their outputs should (and should not) be used in decision-making. AI literacy is not about memorizing algorithms or mathematical formulas. Instead, it is about developing a clear conceptual framework that allows you to ask good questions, interpret results critically, and communicate effectively with both technical and non-technical stakeholders.\nIn parallel, job-ready skills focus on your ability to work with analytics and AI tools in practice. This includes writing and running Python code, managing files and environments, and following workflows that resemble how analytics work is actually performed in organizational settings. These skills are intentionally practical: the goal is not to turn you into a software engineer, but to ensure that you can confidently execute, modify, and troubleshoot analytical work rather than treating code as a black box.\nImportantly, these two tracks are not separate parts of the course—they are interwoven each week. Conceptual material will often appear before coding so that you understand why a tool or technique exists before learning how to use it. Conversely, hands-on work will frequently reinforce conceptual ideas by making them concrete. When something breaks, behaves unexpectedly, or produces a surprising result, that moment is not a failure—it is an opportunity to deepen both your literacy and your skill.\nBy the end of the course, success will not be defined by how many lines of code you can write or how many definitions you can recite. Instead, success means that you can explain what an analytics or AI system is doing, execute it reliably, and evaluate its output with informed judgment. That combination—understanding plus execution—is the core outcome this course is designed to deliver.\n\n\n1.2 What you will build each week\nEach week in this course follows a consistent production pattern. Rather than treating readings, code, and assignments as separate activities, you will build a small, coherent set of artifacts that work together. This structure is intentional: it mirrors how analytics and AI work is typically organized in practice, where documentation, code, and reasoning evolve together.\nFirst, you will work through a weekly textbook chapter, delivered as an HTML document. These chapters serve as both your primary learning resource and a long-term reference. They introduce concepts, explain why techniques exist, and provide annotated examples. You are encouraged to revisit these chapters throughout the semester, especially when later topics build on earlier ideas.\nAlongside most but not all chapters, you will create and run one or more Python scripts (.py). These scripts are not isolated exercises, even if at times they feel that way; they are the hands-on implementation of the ideas discussed in the chapter and/or the lecture. All code in the textbook is written so that it can be copied directly into a script file and executed in your Python environment. Over time, these scripts are intended to create a small personal codebase that reflects your growing capability using these tools.\nBy the end of each week, you should be able to look at what you have built and answer two questions confidently:\n(1) Do I understand what this code is doing and why it exists?\n(2) Can I run it, modify it, and explain its output?\nIf the answer to both is yes, you are doing great.\n\n\n1.3 How to succeed in this course\nSuccess in this course is less about prior experience and more about how you approach the work. Analytics and AI involve working with systems that are complex, imperfect, and sometimes frustrating. Learning to make progress in that environment is a skill in itself, and this course is designed to help you develop it.\nFirst, treat confusion as a normal part of the process. You will encounter unfamiliar terms, error messages that do not immediately make sense, and code that does not work the first time you run it. This is not a sign that you are “bad at Python” or “not technical enough.” It is simply how learning in this space works. Progress will come from narrowing the problem, reading error messages carefully, and making small, deliberate changes rather than trying to fix everything at once. The struggle is what will cement these skills for you.\nSecond, focus on understanding before optimization. You are not expected to write elegant or highly efficient code early in the course, it is not the point of the course at all. Instead we focus on prioritizing clarity: writing code you can read, explain, and reason about. If you can explain what each line is doing and why it is there, you are on the right track—even if the solution feels simple or verbose. This sometimes means the course code is not written in what would be considered “textbook Python”, and as your skills progress in this area during and after the course, you will probably grow out of the patterns and approaches used here. That is normal.\nThird, develop a consistent workflow. Each week, you should expect to read the chapter, run the provided code, modify it, and verify that it behaves as expected. Running code, managing files deliberately, and using your programming environment consistently will save you significant time and frustration later in the semester. Small habits—such as running scripts frequently, saving your work often, and keeping your project folders organized—compound quickly.\nFinally, use available resources strategically. When you get stuck, start by re-reading the relevant section of the chapter and carefully examining any error messages. If you consult external resources or AI tools, do so with intention: use them to clarify concepts or suggest possible fixes, but always make sure you understand the solution before moving on. Being able to explain why a fix works is more important than finding one quickly."
  },
  {
    "objectID": "textbook_src/week01_main.html#what-is-analytics-and-what-is-ai",
    "href": "textbook_src/week01_main.html#what-is-analytics-and-what-is-ai",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "2. What Is Analytics and What Is AI?",
    "text": "2. What Is Analytics and What Is AI?\n\n2.1 Analytics: descriptive → diagnostic → predictive → prescriptive\nAnalytics is best understood not as a single technique, but as a progression of questions organizations ask about their data. These questions range from understanding what has already happened to deciding what action should be taken next. A useful way to organize this progression is through four common categories: descriptive, diagnostic, predictive, and prescriptive analytics.\nDescriptive analytics answers the question: What happened?\nThis is the most familiar form of analytics and focuses on summarizing historical data. Examples include reports, dashboards, averages, totals, and trends over time. Descriptive analytics does not attempt to explain why something occurred or what will happen next—it provides a clear picture of past outcomes. In practice, this might look like a weekly sales report, website traffic summary, or inventory count.\nDiagnostic analytics asks: Why did it happen?\nOnce an outcome is observed, the next step is often to understand its cause. Diagnostic analytics explores relationships, comparisons, and breakdowns in the data to identify contributing factors. This might involve segmenting customers, comparing performance across regions, or examining changes before and after a specific event. While still focused on historical data, diagnostic analytics moves beyond description toward explanation.\nPredictive analytics shifts the focus to the future by asking: What is likely to happen next?\nHere, statistical models and machine learning techniques are often used to estimate future outcomes based on patterns in past data. Examples include forecasting demand, predicting customer churn, or estimating the probability that an event will occur. Predictive analytics does not guarantee what will happen—it produces probabilistic estimates that support informed planning and risk management.\nPrescriptive analytics addresses the final question: What should we do about it?\nPrescriptive analytics builds on predictions by incorporating goals, constraints, and trade-offs to recommend actions. This may involve optimization models, business rules, or simulation. For example, a system might recommend how much inventory to reorder, which customers to target with a promotion, or how to allocate limited resources. At this stage, analytics becomes tightly connected to decision-making rather than analysis alone.\nIt is important to recognize that these categories are not mutually exclusive and are often combined in real systems. A single workflow might summarize past performance (descriptive), identify a problem area (diagnostic), estimate future risk (predictive), and recommend an action (prescriptive). Understanding this progression provides a foundation for seeing where AI fits—and where it extends beyond traditional analytics—later in the course.\n\n\n2.2 AI: systems that perform tasks requiring human-like judgment\nArtificial intelligence (AI) refers to a class of systems designed to perform tasks that would normally require human judgment, interpretation, or decision-making. Unlike traditional analytics, which primarily focuses on summarizing data or supporting decisions, AI systems are often embedded directly into processes where they make or influence decisions in real time.\nA defining feature of AI systems is that they operate in environments where rules are incomplete, uncertainty is present, or inputs are too complex to handle with simple, hand-coded logic. Examples include recognizing objects in images, understanding natural language, detecting fraudulent transactions, or recommending products to users. In each case, the system must evaluate patterns, weigh evidence, and produce an output that resembles what a human might do in a similar situation.\nAI systems typically rely on models trained from data rather than explicit instructions for every possible scenario. Instead of being told exactly how to respond in each case, the system learns statistical relationships or representations from historical examples. When presented with new inputs, it uses those learned patterns to generate predictions, classifications, or actions. This learning-based approach allows AI systems to scale to complex tasks, but it also introduces uncertainty and the possibility of error.\nAnother important characteristic of AI is that its outputs are often probabilistic rather than definitive. An AI system may estimate the likelihood that an email is spam, that a customer will stop using a service, or that an image contains a particular object. These estimates are then combined with thresholds, business rules, or human oversight to determine what action is taken. As a result, AI should be understood as a component within a broader decision system rather than as an autonomous replacement for human judgment.\nFinally, it is useful to distinguish between narrow AI and broader notions of intelligence. The systems discussed in this course are narrow by design: they are built to perform specific tasks under specific conditions, often very well, but they do not possess general understanding or awareness. Recognizing both the strengths and limitations of these systems is essential for using them responsibly and effectively in organizational settings.\nIn the next sections, we will compare analytics and AI more directly, highlighting where they overlap, where they differ, and how they are often combined in modern decision-making systems.\n\n\n2.3 Analytics vs AI: overlap and differences\nAnalytics and AI are closely related, but they are not interchangeable. In practice, many systems labeled as “AI” rely heavily on analytical techniques, and many analytical workflows now incorporate AI-based models. Understanding how these two domains overlap and differ is essential for making sense of how modern decision systems are designed and deployed.\nAt a high level, analytics is primarily concerned with supporting human decision-making. It focuses on extracting insight from data, identifying patterns, and presenting information in ways that help people understand what is happening and decide what to do. Even in advanced forms such as predictive or prescriptive analytics, the output is often intended to inform a human decision-maker, who retains responsibility for interpreting the results and acting on them.\nAI, by contrast, is often designed to participate directly in the decision process. AI systems may classify, recommend, prioritize, or trigger actions with minimal human involvement, especially in high-volume or time-sensitive contexts. While humans still define objectives, constraints, and oversight mechanisms, AI systems are frequently embedded into operational workflows where their outputs have immediate consequences.\nThere is, however, a significant area of overlap. Both analytics and AI: - rely on data as their primary input, - use models to represent relationships or patterns, - and produce outputs that influence decisions.\nMany predictive analytics techniques—such as regression or classification models—are also foundational components of AI systems. The difference often lies not in the mathematics, but in how the results are used. A churn prediction model displayed on a dashboard for managers is typically considered analytics; the same model automatically triggering retention offers may be considered AI.\nAnother key distinction is the role of interpretability and automation. Analytics tools often emphasize transparency, explainability, and exploration, allowing users to drill down into results and ask follow-up questions. AI systems, especially those based on complex models, may prioritize performance and scalability over interpretability, requiring additional governance and monitoring to ensure appropriate use.\nRather than viewing analytics and AI as competing approaches, it is more accurate to see them as points along a continuum. Many real-world systems combine analytical reporting, predictive modeling, and AI-driven automation into a single pipeline. Recognizing where a system sits along this continuum helps clarify expectations, risks, and responsibilities associated with its use.\nThis distinction will be especially important as we move deeper into topics such as machine learning and generative AI, where the same underlying techniques can support very different organizational roles depending on how they are deployed.\n\n\n2.4 Prediction as a component, not the whole system\nMany discussions of AI—especially those focused on machine learning—treat prediction as the central task. While prediction is a critical capability, it is important to recognize that prediction alone does not make a complete AI system. In practice, prediction is just one component within a larger structure that connects data, models, and decisions.\nA prediction answers a narrow question such as “What is the likelihood that this event will occur?” or “Which category does this input most likely belong to?” For example, a model might predict the probability that a customer will churn, that a transaction is fraudulent, or that a document belongs to a particular topic. These outputs are useful, but on their own they do not specify what action should be taken.\nWhat turns a prediction into something operational is the surrounding decision logic. Organizations must decide how to interpret a predicted probability, what threshold to apply, what constraints exist, and what costs are associated with different actions. A churn prediction of 70%, for instance, does not automatically imply the same response as a churn prediction of 40%—and even the same prediction may lead to different actions depending on business priorities or resource availability.\nThis distinction highlights why AI systems should be understood as socio-technical systems, not just models. Data pipelines determine what information is available to the model. Models generate predictions based on learned patterns. Decision frameworks translate those predictions into actions, often with human oversight and governance layered on top. Focusing exclusively on prediction risks ignoring these equally important components.\nRecognizing prediction as a component rather than the whole system also helps clarify common misunderstandings about AI capability. High predictive accuracy does not guarantee good decisions, ethical outcomes, or organizational value. Poorly designed thresholds, misaligned incentives, or unexamined assumptions can undermine even the most accurate model.\nThroughout this course, we will return to this idea repeatedly: models produce predictions, but systems produce decisions. Developing the ability to evaluate and design the full system—not just the model—will be a central goal as we move from foundational concepts into more advanced analytics and AI techniques.\nQuick check: Analytics, AI, or both?\nThe goal here is not to label each system correctly, but rather to sharpen your intuition about why something is considered analytics, AI, or a combination of both. The differences are fairly nuanced. In most real-world systems, the distinction depends less on the mathematical technique and more on how the output is used.\n\nA dashboard showing last quarter’s sales by region, with filters and charts\nThis is best described as analytics. The system summarizes historical data and presents it to a human decision-maker, who interprets the results and decides what action to take. The key thing to note here is that this is not predicting any outcome. That makes this clearly analytics. The way to think about this is to ask “does this model have decision autonomy”? If the answer is no, it is analytics. If it is “maybe” or “yes”, that answer is more complicated.\nA model that predicts the probability a customer will cancel their subscription, displayed to a manager\nThis sits at the boundary but is still primarily analytics. Even though a predictive model is used, the output supports human judgment rather than directly triggering an action.\nThe same churn prediction model automatically sending retention offers to high-risk customers\nThis is best described as AI. The prediction is now embedded in an operational workflow, and the system is actively participating in decision-making and taking action rather than merely informing it.\nA fraud detection system that flags suspicious transactions for human review\nThis example illustrates both analytics and AI. The model performs an AI-like task (pattern recognition and classification), but the final decision remains with a human, creating a hybrid system. These kinds of systems are fairly classical examples of overlap.\nA recommendation engine that personalizes product suggestions in real time\nThis is clearly AI. The system continuously generates predictions and acts on them automatically at scale, often without direct human intervention for each decision.\n\nThe key takeaway is that the same underlying model can be analytics or AI depending on context. What matters is not just whether a model is used, but where it sits in the decision process, how much autonomy it has, and who—or what—ultimately acts on its output."
  },
  {
    "objectID": "textbook_src/week01_main.html#data-models-logic-the-core-of-ai-systems",
    "href": "textbook_src/week01_main.html#data-models-logic-the-core-of-ai-systems",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "3. Data + Models + Logic: The Core of AI Systems",
    "text": "3. Data + Models + Logic: The Core of AI Systems\n\n3.1 Data\nData forms the foundation of all analytics and AI systems. Regardless of how sophisticated a model or decision framework may be, the behavior of the system is fundamentally shaped by the data it receives. Understanding what data represents, how it is generated, and how it enters a system is therefore a prerequisite for understanding how AI systems operate in practice.\nBroadly defined, data consists of recorded observations about the world. These observations may take many forms, including transaction records, sensor readings, text documents, images, user interactions, or system logs. Although these data sources differ in structure and complexity, they share a common role: they capture traces of past events or states that can be analyzed, modeled, and acted upon.\nData serves two critical functions within AI systems. First, it is used to train models. Historical data provides the examples from which models learn patterns, relationships, or representations. The coverage, quality, and diversity of this data directly influence what a model can learn and how well it generalizes beyond its training examples. Second, data is used during system operation, when new observations are provided as input to a trained model in order to generate predictions, classifications, or scores. Errors, shifts, or inconsistencies in either training data or operational data can degrade system performance.\nIt is important to recognize that data is not a complete or neutral representation of reality. Data reflects the processes through which it was collected, including organizational priorities, technical constraints, and human choices. Some phenomena are easier to observe and record than others, some groups or behaviors are overrepresented, and some variables serve only as indirect proxies for what is actually of interest. As a result, data commonly contains noise, omissions, and systematic biases.\nFrom a systems perspective, data does not simply exist—it is acquired and prepared through pipelines. These pipelines involve decisions about what to collect, how frequently to collect it, how it is stored, and how it is cleaned or transformed before use. Choices made at this stage—such as how missing values are handled or how categories are encoded—can have consequences that propagate throughout the system.\nFor these reasons, effective analysis of AI systems begins with careful attention to data. Asking where data comes from, what it represents, and what it leaves out is often the most reliable way to understand system behavior, anticipate limitations, and diagnose failures.\n\n\n3.2 Models\nA model is a formal representation of a relationship between inputs and outputs. In analytics and AI systems, models are used to map observed data to predictions, classifications, scores, or other quantities that support decision-making. While models can take many forms—from simple equations to complex neural networks—their role within a system is conceptually consistent: they provide a structured way to generalize from past observations to new situations.\nModels differ from raw data in an important way. Data records what has already happened, whereas a model encodes an assumption about how the world works. These assumptions may be explicit, as in a linear equation that specifies how inputs combine to produce an output, or implicit, as in a deep learning model that learns internal representations through training. In both cases, the model embodies a hypothesis about underlying patterns in the data.\nIn analytics and AI systems, models are typically created through a training process. During training, historical data is used to adjust the model’s parameters so that its outputs align as closely as possible with observed outcomes. This process allows the model to capture regularities in the data, but it also ties the model’s behavior to the quality and scope of the data it was trained on. A model cannot reliably learn patterns that are absent, rare, or systematically distorted in the training data.\nOnce trained, a model is used to generate outputs for new inputs during system operation. These outputs are often probabilistic rather than deterministic. Instead of producing a single “correct” answer, a model may estimate the likelihood of different outcomes or assign scores that reflect relative confidence. This probabilistic nature is a strength—it allows models to operate under uncertainty—but it also requires careful interpretation and downstream handling.\nIt is also important to recognize that models are not inherently intelligent or autonomous. They do not understand context, intent, or consequences in a human sense. Instead, they apply learned patterns mechanically, based on the structure imposed during training. As a result, models can perform impressively within familiar conditions while behaving unpredictably when those conditions change.\nUnderstanding models as components rather than complete systems helps clarify both their power and their limitations. Models can recognize patterns, make estimates, and scale decisions, but they do so within the boundaries defined by data, training procedures, and design choices. How their outputs are ultimately used depends on the surrounding logic and decision framework, which is addressed next.\n\n\n3.3 Logic\nWhile data and models are often the most visible components of AI systems, logic is what ultimately connects model outputs to real-world actions. Logic defines how predictions, scores, or classifications are interpreted and how they are translated into decisions. Without logic, even the most accurate model remains analytically interesting but operationally incomplete.\nLogic encompasses the rules, thresholds, constraints, and objectives that govern system behavior. These elements specify what should happen when a model produces a particular output. For example, a probability score may be compared against a threshold to determine whether an alert is triggered, a recommendation is shown, or a transaction is blocked. These thresholds are not inherent to the model—they are design choices that reflect priorities, trade-offs, and risk tolerance.\nIn many systems, logic also encodes business or organizational constraints. These may include resource limitations, regulatory requirements, fairness considerations, or cost structures. A model might identify many high-risk cases, but logic determines how many can realistically be acted upon, which cases are prioritized, and which actions are permissible. As a result, logic often mediates between what a model suggests and what an organization can or should do.\nLogic can be implemented in various ways. In some systems, it takes the form of explicit rules written by humans, such as conditional statements or decision trees. In others, logic is embedded within optimization routines or policy frameworks that balance competing objectives. Even when decisions appear automated, logic typically reflects human judgments made earlier about acceptable outcomes and trade-offs.\nImportantly, logic is where accountability often resides. When a system produces an undesirable outcome, the cause may not lie in the data or the model, but in the logic that governed how outputs were used. Poorly chosen thresholds, misaligned incentives, or overly rigid rules can undermine system performance even when model accuracy is high.\nViewing logic as a core component of AI systems highlights a critical insight: models produce outputs, but logic determines actions. Understanding this distinction is essential for evaluating system behavior, diagnosing failures, and designing AI systems that align with intended goals and constraints.\n\n\n3.4 Where systems fail (data, model, logic layer)\nFailures in analytics and AI systems rarely originate from a single cause. Instead, they tend to emerge from breakdowns at one or more layers of the system: data, models, or logic. Understanding these layers—and how they interact—provides a structured way to diagnose why a system behaves unexpectedly or produces poor outcomes.\nFailures at the data layer occur when the information feeding the system is incomplete, biased, noisy, or no longer representative of the environment in which the system operates. This may include missing values, measurement errors, outdated records, or shifts in underlying patterns over time. Because models learn from historical data, weaknesses at this layer often propagate forward, limiting what the system can reasonably achieve regardless of model sophistication.\nFailures at the model layer arise when the model is poorly matched to the task or the data available. This may involve overly simplistic models that fail to capture important relationships, overly complex models that overfit historical patterns, or models trained on data that does not reflect current conditions. Even well-designed models can fail when deployed in contexts that differ meaningfully from those seen during training.\nFailures at the logic layer occur when model outputs are translated into decisions inappropriately. Common issues include poorly chosen thresholds, rigid rules that do not adapt to changing conditions, or decision criteria that prioritize the wrong objectives. In these cases, a model may be producing reasonable outputs, but the surrounding logic causes undesirable actions or missed opportunities.\nImportantly, these layers are interdependent. A system with high-quality data and a strong model can still fail due to flawed decision logic. Likewise, careful logic cannot compensate for fundamentally uninformative or biased data. Effective system design therefore requires attention to all three layers simultaneously rather than focusing narrowly on model performance.\nViewing failures through this layered lens encourages more precise diagnosis and more effective intervention. Rather than asking whether an AI system “works” or “does not work,” it becomes possible to ask where it is breaking down and why. This perspective supports more thoughtful system evaluation and more responsible use of analytics and AI in decision-making contexts."
  },
  {
    "objectID": "textbook_src/week01_main.html#ai-paradigms-overview",
    "href": "textbook_src/week01_main.html#ai-paradigms-overview",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "4. AI Paradigms Overview",
    "text": "4. AI Paradigms Overview\n\n4.1 Symbolic AI\nSymbolic AI represents one of the earliest approaches to artificial intelligence. Rather than learning patterns from data, symbolic systems rely on explicit representations of knowledge and rule-based reasoning to perform tasks. These systems operate by manipulating symbols—such as words, categories, or logical statements—according to predefined rules.\nAt the core of symbolic AI is the idea that intelligent behavior can be produced by encoding expert knowledge directly into a system. This often takes the form of if–then rules, decision trees, logic statements, or structured knowledge bases. For example, a symbolic system might contain rules such as: if a customer is late on payment and has missed multiple deadlines, then flag the account for review. Each rule reflects a human judgment that has been translated into formal logic.\nSymbolic AI systems tend to be transparent and interpretable. Because their reasoning process is explicitly defined, it is usually possible to trace how a particular conclusion was reached. This makes symbolic approaches attractive in domains where explanations, compliance, or auditability are critical. They also perform well in environments where the rules are stable and the problem space is well understood.\nHowever, symbolic AI has notable limitations. Writing and maintaining rules is labor-intensive, and such systems struggle to scale as complexity increases. They also perform poorly in settings characterized by ambiguity, noise, or high variability—such as image recognition or natural language understanding—where it is difficult or impractical to enumerate all relevant rules in advance.\nWhile symbolic AI is no longer the dominant paradigm in many areas, it remains an important conceptual foundation. Many modern systems still rely on symbolic components for constraints, validation, and control, even when learning-based models are used elsewhere. Understanding symbolic AI helps clarify both the strengths of explicit reasoning and the challenges that motivated the development of data-driven approaches addressed in the next sections.\n\n\n4.2 Statistical / Machine Learning\nStatistical and machine learning approaches to AI differ from symbolic systems in a fundamental way: rather than relying on explicitly programmed rules, they learn patterns from data. These approaches use historical observations to infer relationships between inputs and outputs, allowing systems to generalize to new, unseen cases without being told exactly how to respond in every situation.\nAt the heart of machine learning is the idea that regularities in data can be captured through mathematical models whose parameters are estimated from examples. During training, a model is exposed to data and adjusted so that its predictions align with observed outcomes as closely as possible. This process allows the system to adapt to complex patterns that would be difficult to specify manually using rules alone.\nMachine learning methods are often categorized based on the type of feedback available during training. In supervised learning, the model is trained using labeled examples, where the correct output is known in advance. Common applications include classification and regression tasks, such as predicting customer churn or estimating demand. In unsupervised learning, the model works with unlabeled data to identify structure, such as clusters or latent patterns, without predefined outcomes. Both approaches are widely used in analytics and AI systems.\nCompared to symbolic AI, statistical and machine learning systems are generally more flexible and scalable. They perform well in environments with large volumes of data and can adapt to subtle patterns and correlations. However, this flexibility comes with trade-offs. Learned models may be less transparent, and their behavior can be sensitive to the data used for training. As a result, understanding and validating model performance often requires careful evaluation rather than direct inspection of rules.\nImportantly, statistical and machine learning approaches do not eliminate the need for human judgment. Choices about which data to use, which features to include, how to evaluate performance, and how to deploy model outputs remain human decisions. Machine learning shifts the burden of specification from rule-writing to data curation and model design, redefining where expertise is applied within AI systems.\nThis paradigm has become central to modern analytics and AI, forming the basis for many applications encountered in practice. It also provides the foundation for more advanced approaches, such as neural and deep learning, discussed next.\n\n\n4.3 Neural / Deep Learning\nNeural and deep learning approaches extend statistical machine learning by focusing on learning representations directly from data. Rather than relying on hand-crafted features or simple functional forms, these models use layered computational structures—commonly referred to as neural networks—to transform raw inputs into increasingly abstract representations.\nThe key idea behind neural networks is inspired by, but not equivalent to, biological neurons. A neural network is composed of interconnected units that apply weighted combinations of inputs followed by nonlinear transformations. By stacking many such layers, deep learning models can capture complex patterns in high-dimensional data. This layered structure allows them to excel in tasks such as image recognition, speech processing, and natural language understanding, where relationships are difficult to specify explicitly.\nOne defining characteristic of deep learning is its ability to operate on unstructured or semi-structured data, including images, audio, and text. In these domains, traditional statistical models often require extensive feature engineering. Deep learning models, by contrast, can learn relevant representations automatically from large volumes of data, reducing the need for manual specification of features.\nThis capability comes with important trade-offs. Neural and deep learning models are typically data-intensive and computationally demanding. Training them often requires large datasets, specialized hardware, and careful tuning. They also tend to be less interpretable than simpler models, making it more difficult to explain why a particular output was produced. As a result, deployment of deep learning systems often involves additional monitoring, validation, and governance mechanisms.\nDespite these challenges, neural and deep learning approaches have reshaped the AI landscape. Many contemporary systems—including speech recognition, computer vision applications, and large language models—are built on deep learning architectures. Understanding this paradigm helps clarify why modern AI systems can handle tasks that were previously infeasible, as well as why concerns about transparency, robustness, and control remain central to their use.\nNeural and deep learning approaches are rarely used in isolation. In practice, they are often combined with statistical methods and symbolic logic to form integrated systems, a topic addressed in the next section.\n\n\n4.4 Hybrid systems in practice\nIn real-world applications, AI systems rarely rely on a single paradigm. Instead, they are typically hybrid systems that combine symbolic reasoning, statistical or machine learning models, and neural or deep learning components. Each paradigm contributes different strengths, and hybrid designs allow systems to balance performance, interpretability, and control.\nA common pattern in hybrid systems is the use of learning-based models for perception and prediction, paired with symbolic or rule-based logic for decision-making and constraints. For example, a deep learning model may be used to recognize objects in an image or extract meaning from text, while a rule-based layer determines whether the output meets regulatory requirements or triggers a specific action. In this structure, learning handles complexity and variability, while symbolic logic enforces consistency and accountability.\nHybrid systems also help address practical limitations of individual approaches. Machine learning models can adapt to data and capture subtle patterns, but they may behave unpredictably outside familiar conditions. Symbolic logic can impose guardrails, prevent certain actions, or require human review under specified circumstances. Statistical models can provide calibrated probabilities that support decision thresholds and prioritization. Together, these components form systems that are more robust than any single approach alone.\nMany modern AI applications illustrate this hybrid structure. Recommendation systems often combine learned user preference models with business rules and inventory constraints. Fraud detection systems use predictive models to score transactions and rule-based logic to manage alerts and workflows. Large language model applications frequently pair neural models with retrieval systems, validation rules, and structured decision logic to ensure usable and reliable outputs.\nUnderstanding AI systems as hybrids reinforces an important perspective: intelligence in practice is distributed across system components, not concentrated in a single model. Performance, reliability, and responsibility emerge from how data, models, and logic are assembled and governed. This systems-level view provides a foundation for analyzing and designing AI applications that operate effectively within real organizational and societal constraints."
  },
  {
    "objectID": "textbook_src/week01_main.html#data-pipelines-and-decision-frameworks",
    "href": "textbook_src/week01_main.html#data-pipelines-and-decision-frameworks",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "5. Data Pipelines and Decision Frameworks",
    "text": "5. Data Pipelines and Decision Frameworks\n\n5.1 How systems acquire data\nBefore any analysis or modeling can occur, data must be acquired. Data acquisition refers to the processes through which information is collected, recorded, and made available for use within analytics and AI systems. These processes shape not only what data is available, but also how timely, reliable, and representative that data is.\nOne common source of data is operational systems. Transaction databases, customer relationship management systems, enterprise resource planning platforms, and web applications routinely generate records as part of normal business activity. In these cases, data is a byproduct of operations rather than something collected specifically for analysis. While such data is often rich and high-volume, it reflects the structure and incentives of the underlying system, which may limit what can be observed.\nAnother important source is instrumentation and logging. Systems are frequently designed to log events, interactions, or performance metrics explicitly for monitoring and analysis. Examples include website click logs, application usage telemetry, or sensor readings from physical devices. Instrumented data can provide fine-grained insight into behavior over time, but it requires deliberate design decisions about what to record and at what level of detail.\nData may also be acquired through external sources. These include public datasets, third-party data providers, APIs, and partner organizations. External data can enrich internal records by providing additional context, such as demographic information, market indicators, or environmental conditions. However, reliance on external sources introduces dependencies related to data quality, licensing, update frequency, and long-term availability.\nAcross these sources, data can be collected in different modes. Batch acquisition involves gathering data at regular intervals, such as daily or weekly extracts, and is common in reporting and strategic analysis. Streaming or real-time acquisition captures data continuously as events occur and is often used in monitoring, personalization, or fraud detection systems. The choice between batch and streaming acquisition affects system responsiveness, complexity, and infrastructure requirements.\nImportantly, data acquisition is not a passive process. Decisions about what to collect, how to define variables, and how frequently to record observations embed assumptions into the system. These assumptions influence downstream analysis and decision-making, sometimes in subtle ways. Understanding how data enters a system is therefore a critical step in evaluating both the capabilities and the limitations of analytics and AI applications.\n\n\n5.2 From pipeline to decision frameworks\nA data pipeline does not exist in isolation. Its purpose is to support decisions by moving raw data through a sequence of steps that make it usable, interpretable, and actionable. Understanding how pipelines connect to decision frameworks helps clarify how analytics and AI systems translate information into outcomes.\nA typical data pipeline begins with acquisition and continues through stages such as cleaning, transformation, storage, and aggregation. Each stage prepares the data for the next, addressing issues such as missing values, inconsistent formats, or incompatible sources. While these steps are often treated as technical details, they play a central role in shaping what information ultimately reaches models and decision-makers.\nOnce data has been processed, it enters the decision framework. This framework defines what decision is being supported or automated, what objectives are being pursued, and what constraints must be respected. In analytical settings, the output of the pipeline may feed dashboards or reports that inform human judgment. In AI-driven settings, processed data may be passed directly to models whose outputs trigger actions or recommendations.\nDecision frameworks also specify how outputs are evaluated and acted upon. This includes defining thresholds, priorities, costs, and trade-offs. For example, a predictive model may estimate risk, but the decision framework determines what level of risk warrants intervention, how limited resources are allocated, and what actions are permissible. These choices are rarely purely technical; they reflect organizational goals and values.\nThe connection between pipelines and decision frameworks is often iterative rather than linear. As decisions are made and actions are taken, new data is generated and fed back into the pipeline. This feedback loop can be used to monitor performance, update models, or revise decision rules over time. Effective systems are designed with this dynamic interaction in mind rather than treating pipelines as one-time processes.\nViewing pipelines and decision frameworks together reinforces a systems-level perspective. Data pipelines make information available, but decision frameworks determine how that information is used. Both are necessary for analytics and AI systems to function effectively, and weaknesses in either can undermine the overall quality of decisions.\n\n\n5.3 Common failure modes\nEven when individual components appear well designed, analytics and AI systems can fail in predictable ways. Many of these failures arise not from a single mistake, but from misalignments between data pipelines, models, and decision frameworks. Recognizing common failure modes makes it easier to diagnose problems and to design systems that are more resilient.\nOne common failure occurs when data pipelines drift away from decision needs. Data may be collected because it is easy to capture rather than because it is relevant to the decision at hand. Over time, pipelines can accumulate variables and transformations that no longer align with current objectives, leading to analyses that are technically correct but operationally unhelpful.\nAnother frequent issue is stale or delayed data. When pipelines operate on batch schedules that are too slow for the decisions they support, outputs may be outdated by the time they are used. This is especially problematic in environments where conditions change rapidly. In such cases, system performance degrades not because models are inaccurate, but because they are responding to yesterday’s information.\nFailures also occur when feedback loops are ignored or misunderstood. Decisions based on model outputs often influence future data, which in turn affects subsequent model behavior. If these feedback effects are not anticipated, systems can reinforce undesirable patterns, amplify noise, or create misleading signals that appear as genuine trends.\nA further source of failure lies in overconfidence in automation. When decision frameworks rely too heavily on model outputs without sufficient monitoring or human oversight, small errors can scale quickly. Conversely, overly cautious frameworks that ignore model outputs may negate potential benefits. Balancing automation and control is therefore a persistent design challenge.\nFinally, failure can result from organizational misalignment. Even well-designed pipelines and models can produce poor outcomes if incentives, responsibilities, or governance structures are unclear. Decisions about who owns the data, who is accountable for outcomes, and how performance is evaluated play a critical role in system success.\nUnderstanding these common failure modes reinforces an important lesson: effective analytics and AI systems depend on alignment across technical and organizational dimensions. Addressing failures requires looking beyond individual components to the system as a whole."
  },
  {
    "objectID": "textbook_src/week01_main.html#running-your-first-python-script-.py",
    "href": "textbook_src/week01_main.html#running-your-first-python-script-.py",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "6. Running Your First Python Script (.py)",
    "text": "6. Running Your First Python Script (.py)\n\n6.1 What a Python script is\nA Python script is a plain text file containing Python code, typically saved with a .py extension. When a script is run, Python reads the file from top to bottom and executes each instruction in order. Unlike interactive environments, such as notebooks or consoles, a script represents a complete, self-contained program.\nScripts are the most common way Python is used in professional analytics and AI workflows. They are easy to version, easy to share, and behave predictably when run multiple times. Because a script always starts from a clean state, its behavior depends entirely on the code it contains and the environment in which it runs—there is no hidden execution history.\nA Python script can contain many elements, including:\n- variable assignments,\n- calculations,\n- function definitions,\n- conditional logic,\n- and instructions to read data or produce output.\nSome scripts are short and perform a single task, while others may coordinate complex workflows. Regardless of size, the defining characteristic is that the script is executed as a unit.\nMany scripts follow a common structural pattern that improves clarity and reusability. For example, logic is often placed inside functions, with a designated entry point that tells Python where execution should begin. This pattern makes it easier to read the code, reuse components later, and avoid unintended behavior when code is imported into other files.\nConceptually, it is helpful to think of a Python script as answering three questions:\n1. What inputs does this program expect?\n2. What processing does it perform?\n3. What outputs does it produce?\nKeeping these questions in mind encourages clearer program structure and makes debugging easier as scripts become more complex.\n\n\n6.2 Running a script in Visual Studio Code\nVisual Studio Code (VS Code) is a commonly used environment for writing and running Python scripts. It combines a code editor, a terminal, and language support in a single interface, making it well suited for analytics and AI workflows.\nTo run a Python script in VS Code, the file must first be opened in the editor. Scripts are typically saved with a .py extension and stored within a project folder. Once the file is open, execution can be initiated in several ways, but all methods ultimately run the script using the Python interpreter selected for the project.\nOne common approach is to use the integrated terminal. VS Code includes a terminal window that opens within the editor and behaves like a standard system terminal. From this terminal, the script can be executed by navigating to the folder containing the file and running:\n\n\n\n\n\n\nRun the script from the terminal\n\n\n\nFrom the project folder, run:\npython hello_world.py\n\n\nVS Code also provides editor-based run options, such as a Run Python File button or command palette actions. These tools are convenient, but they rely on the same underlying mechanism: calling the Python interpreter on the current file. Regardless of how execution is triggered, the result is the same—the script runs from top to bottom and any output is displayed in the terminal.\nAn important step when running scripts in VS Code is selecting the correct Python interpreter. In projects that use virtual environments, the interpreter should point to the environment created for the project rather than the system-wide Python installation. VS Code allows the interpreter to be selected on a per-project basis, ensuring that the correct packages and versions are used when scripts are executed.\nWhen a script runs successfully, any output produced by print() statements or error messages appears in the terminal. If the script finishes without errors, control returns to the terminal prompt. This clear start-and-finish behavior is a defining feature of script-based workflows and makes it easier to reason about program execution and diagnose problems when issues arise.\n\n\n6.3 Reading error messages (very high level)\nWhen a Python script encounters a problem, it produces an error message. While error messages can look intimidating at first, they are an essential part of working with code and often contain enough information to identify what went wrong. Learning to read them at a high level is an important early skill.\nMost Python error messages include three main components. First, they indicate where the error occurred, usually by showing the file name and line number. This helps narrow attention to a specific part of the script rather than the entire program. Second, they describe what type of error occurred, such as a syntax error, a missing name, or an invalid operation. Third, they may include a brief explanation of the issue.\nAt this stage, it is not necessary to understand every detail of an error message. Instead, focus on identifying the general category of the problem. A syntax error usually means Python could not interpret the structure of the code, often due to a missing parenthesis, quotation mark, or colon. Other errors occur while the script is running and typically indicate that Python understood the code but encountered an unexpected situation, such as using a variable that does not exist.\nError messages are not signals to stop; they are signals to inspect and adjust. Often, the fastest way forward is to read the message carefully, locate the referenced line, and compare it to what was intended. Small changes—such as correcting a spelling mistake or fixing indentation—frequently resolve the issue.\nDeveloping comfort with error messages takes time, but it begins with a simple mindset shift: errors are feedback, not failure. Each message is an opportunity to understand how Python interprets instructions and how small changes in code affect execution.\n\n\n6.4 The edit → run → fix loop\nWriting code is an iterative process. Scripts are rarely written correctly on the first attempt, and effective work with Python depends on developing a steady rhythm of editing, running, and fixing code. This cycle—often referred to as the edit → run → fix loop—is the normal way programs are developed and refined.\nThe process begins by making a small change to the code. This might involve adding a new line, modifying an existing statement, or adjusting a variable value. After making the change, the script is run to observe its behavior. Running the script provides immediate feedback, either in the form of expected output or an error message that signals a problem.\nIf the result is not what was intended, the next step is to fix the issue. This may involve correcting a syntax error, revising a calculation, or clarifying the logic of the program. Importantly, fixes are most effective when changes are made incrementally. Altering many parts of a script at once can make it difficult to identify what caused a problem or whether a fix actually worked.\nThis loop encourages experimentation and learning. By making small, deliberate changes and observing their effects, it becomes easier to understand how individual lines of code contribute to overall behavior. Over time, this process builds intuition about how Python executes instructions and how to approach debugging systematically.\nThe edit → run → fix loop also reinforces a practical habit: run code early and often. Frequent execution reduces the distance between cause and effect, making problems easier to diagnose. As scripts grow more complex, maintaining this iterative rhythm becomes one of the most reliable ways to write correct, understandable, and maintainable code."
  },
  {
    "objectID": "textbook_src/week01_main.html#week-1-code",
    "href": "textbook_src/week01_main.html#week-1-code",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "Week 1 Code:",
    "text": "Week 1 Code:\nThis is just an example, the details in the below code are covered in future chapters.\nName the script: hello_world.py\n# hello_world.py\n# A minimal Python script illustrating basic structure and execution.\n\ndef main():\n    \"\"\"\n    The main function contains the core logic of the script.\n    When the script is run, this function will be executed.\n    \"\"\"\n    print(\"Hello, world!\")\n    print(\"This script is running successfully.\")\n\nif __name__ == \"__main__\":\n    # This conditional ensures that main() runs only\n    # when the script is executed directly, not when imported.\n    main()"
  },
  {
    "objectID": "textbook_src/week01_main.html#hello-python-printing-variables-and-simple-math",
    "href": "textbook_src/week01_main.html#hello-python-printing-variables-and-simple-math",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "7. Hello Python: Printing, Variables, and Simple Math",
    "text": "7. Hello Python: Printing, Variables, and Simple Math\n\n7.1 Printing output\nThe simplest way for a Python program to communicate with you is by printing output to the screen. In a script-based workflow, this output appears in the terminal after the script is run. Printing is therefore the most basic feedback mechanism for understanding what a program is doing.\nIn Python, printing is done using the built-in print() function. A function is an instruction that performs a specific task, and print() is responsible for displaying information. Anything placed inside the parentheses is sent to standard output.\nFor example, printing a short message looks like this:\nprint(\"Hello, world!\")\nWhen this line is executed, Python displays the text exactly as written (without the quotation marks). Text enclosed in quotation marks is called a string, and strings are commonly used for messages, labels, and explanations.\nPrinting is not limited to text. Python can also print numbers and the results of calculations:\nprint(3)\nprint(2 + 5)\nIn these cases, Python evaluates the expression first and then prints the result. This makes print() especially useful for checking intermediate values and understanding how a program is behaving as it runs.\nIt is important to note that print() does not change the program’s logic or store information for later use. Its role is purely communicative—it shows values so they can be read by a person. For this reason, printing is frequently used when learning Python, debugging code, or verifying that a script is producing the expected results.\nAs scripts become more complex, printing remains a simple but powerful tool. By printing values at different points in a program, it becomes possible to observe how data flows through the code and how instructions are executed step by step.\n\n\n7.2 Variables and assignment\nIn Python, a variable is a name that refers to a value. Variables allow programs to store information so it can be used, reused, and modified as the program runs. Rather than working only with raw numbers or text, variables make code more readable and flexible.\nVariables are created using assignment, which is done with a single equals sign (=). Assignment tells Python to take the value on the right-hand side and store it under the name on the left-hand side.\nFor example:\nx = 2\nThis line should be read as “assign the value 2 to the variable named x,” not as a statement of equality. The equals sign in Python does not mean “is equal to” in the mathematical sense; it means “store this value under this name.” This can be confusing for new users, so make sure you understand the difference.\nOnce a variable has been assigned, it can be used anywhere a value could be used. For example:\nprint(x)\nHere, Python looks up the value stored in x and prints it. If the value of x changes later, printing x will reflect the new value.\nVariables are especially useful when working with calculations. Instead of repeating numbers directly, values can be stored once and reused:\nprint(2 + 2)\nprint( 2 + x)\nIn these 2 examples, the result should be the same for each of these chunks of code. Initially it can be hard to understand why you might use variables instead of just the numbers. In more complicated examples, variables may make the code easier to understand, and easier to modify.\nprice = 20\ntax = 1.50\ntotal = price + tax\nprint(total)\nAs the values for price and tax changes, the model (calculation of “total”) still continues to work.\nVariable names are chosen by the programmer and should be descriptive enough to indicate what the value represents. While Python allows many naming styles, good variable names improve clarity and reduce confusion, especially as programs grow longer.\nAt a conceptual level, variables answer a simple question: What information does this program need to remember while it runs? Learning to use variables effectively is a key step toward writing programs that do more than display fixed messages.\n\n\n7.3 Simple math and expressions\nPython can perform basic mathematical operations in a way that closely mirrors standard arithmetic. These operations are written as expressions, which are combinations of values and operators that Python evaluates to produce a result.\nThe most common arithmetic operators include addition, subtraction, multiplication, and division:\nprint(2 + 3)\nprint(10 - 4)\nprint(3 * 5)\nprint(8 / 2)\nIn each case, Python evaluates the expression and then prints the result. These operations behave as expected for most everyday calculations, making Python a natural tool for working with numerical data.\nExpressions become more useful when combined with variables. Instead of working with fixed numbers, variables allow calculations to adapt as values change:\na = 10\nb = 4\nprint(a + b)\nprint(a * b)\nHere, Python replaces each variable with its stored value before performing the calculation. This substitution happens automatically and is a core feature of how expressions work.\nPython follows standard order of operations when evaluating expressions. Multiplication and division are performed before addition and subtraction, unless parentheses are used to make the intended order explicit:\nprint(2 + 3 * 4)\nprint((2 + 3) * 4)\nParentheses make calculations clearer and reduce ambiguity, especially as expressions become more complex. Using them deliberately improves readability and helps prevent unintended results.\nIt is also common to combine expressions and printing in a single line, especially when exploring or verifying calculations:\ntotal = 15 + 7\nprint(\"Total:\", total)\nIn this example, Python first evaluates the expression, assigns the result to a variable, and then prints a message that includes the computed value.\nSimple math and expressions form the computational backbone of most programs. Even advanced analytics and AI systems ultimately rely on large numbers of these basic operations. Developing comfort with expressions at this level makes it easier to understand how more complex calculations are built later on.\n\n\n7.4 A first complete Python program\nAt this point, it is useful to bring together the ideas from the previous sections into a single, complete Python script. A complete program combines printing, variables, and expressions in a way that performs a small but meaningful task from start to finish.\nConsider the following example. This script defines a few values, performs a calculation, and prints the result in a readable way:\n# first_program.py\n\nprice = 20\ntax = 1.50\n\ntotal_cost = price + tax\n\nprint(\"Price:\", price)\nprint(\"Tax:\", tax)\nprint(\"Total cost:\", total_cost)\nThis program follows a simple and common structure. First, values are assigned to variables. Next, those variables are used in an expression to compute a new value. Finally, the results are printed so they can be seen in the terminal when the script is run.\nEach line in this script serves a clear purpose. The variable assignments store information the program needs to remember. The expression combines those values to produce a result. The print statements communicate that result to the user. Together, these elements form a complete workflow: define → compute → display.\nWhen this script is executed, Python runs the file from top to bottom. There is no hidden state and no interaction required during execution. Every time the script is run, it produces the same output given the same starting values. This predictability is one of the key advantages of script-based programming.\nAlthough this program is simple, it illustrates the core building blocks that appear in much larger applications. More advanced programs may read data from files, make decisions using conditional logic, or repeat calculations in loops, but they are still composed of the same fundamental elements introduced here.\nBeing able to read, write, and reason about small complete programs is an important milestone. From this point forward, new concepts will build on these foundations rather than replacing them."
  },
  {
    "objectID": "textbook_src/week01_main.html#terminal-and-file-navigation-basics",
    "href": "textbook_src/week01_main.html#terminal-and-file-navigation-basics",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "8. Terminal and File Navigation Basics",
    "text": "8. Terminal and File Navigation Basics\nThis section explains how Python scripts are located and executed on a computer by introducing the basic ideas behind the terminal, file systems, and paths. The goal is not to master the terminal, it is fairly complicated, and takes some time to really figure out. Instead this section should be seen as the minimally viable tools in the terminal.\n\nOpening the terminal (macOS, Windows, and VS Code)\nBefore working with terminal commands, it is important to know how to open a terminal. The exact steps depend on the operating system and tools being used, but the underlying concept is the same: opening a window where text-based commands can be entered and executed.\nOn macOS, the terminal application is called Terminal. It can be opened in several ways:\n- By using Spotlight search and typing “Terminal”\n- By navigating to Applications → Utilities → Terminal\nOnce opened, the Terminal window provides direct access to the macOS command line.\nOn Windows, the terminal experience may appear under different names depending on configuration. Common options include:\n- Command Prompt\n- Windows PowerShell\n- Windows Terminal (a newer application that supports multiple shells)\nAny of these can be opened by searching from the Start menu. While they may look slightly different, they all allow commands to be typed and executed in the same basic way.\nIn this course, most examples assume the use of Visual Studio Code, which includes an integrated terminal. This terminal runs inside the editor itself and behaves just like a regular system terminal, but with important advantages for programming.\nTo open the integrated terminal in Visual Studio Code: - Use the menu option View → Terminal\n- Or use the keyboard shortcut that opens the terminal panel\nThe integrated terminal starts in the context of the current project folder, which reduces the need to navigate manually through the file system. This makes it especially convenient for running Python scripts and managing project files.\nRegardless of how the terminal is opened, the same ideas apply. Commands are typed, executed, and produce output.\nThe sections that follow explain how to understand how to interact with and use the terminal once opened.\n\n\n8.1 What the terminal is and why it matters\nThe terminal is a text-based interface for interacting directly with a computer’s operating system. Instead of clicking on icons or navigating menus, instructions are typed as commands. The computer executes those commands and returns output in the same window.\nWhile graphical interfaces are designed for ease of use, the terminal is designed for precision and control. It allows you to specify exactly what you want the computer to do, where to do it, and how to do it. This makes the terminal especially important for programming, where small differences in location or configuration matter.\nIn a graphical interface, running a program often involves clicking on a file. In contrast, running a Python script from the terminal requires two pieces of information:\n1. Which Python interpreter to use\n2. Where the script is located\nThe terminal makes both of these explicit.\nWhen you type a command into the terminal, you are issuing an instruction and then waiting for a response. For example, a simple command that asks the computer where you currently are in the file system looks like this:\npwd\nAfter pressing Enter, the terminal responds by printing the current working directory. This directory is the location the terminal is “pointing to,” and it determines which files the computer can see when you issue commands.\nThis idea of location is critical. When you run a Python script using a command such as:\npython first_program.py\nPython looks for the file named first_program.py in the current working directory. If the file is not located there, Python cannot run it, even if the file exists elsewhere on your computer. This is one of the most common sources of confusion for beginners, and it highlights why understanding the terminal matters.\nThe terminal also differs from clicking files in an important way: commands are repeatable and explicit. When you type a command, you can see exactly what was executed. This makes it easier to reproduce results, diagnose errors, and understand what the computer is doing step by step.\nEvery terminal interaction follows the same basic pattern:\n1. You type a command.\n2. The computer executes it.\n3. The terminal displays output or an error message.\n4. Control returns to you.\nFor example, listing the contents of the current directory looks like this:\nls\nThe terminal responds by showing the files and folders in that location. This immediate feedback loop—command followed by output—is central to working effectively with the terminal and will be used throughout the course.\nUnderstanding the terminal is not about memorizing commands. It is about developing a mental model of how the computer interprets instructions, how files are located, and how programs are executed. With that model in place, the terminal becomes a powerful and reliable tool rather than a source of frustration.\n\n\n8.2 Files, folders, and working directories\nComputers organize information using files and folders (also called directories). A file contains data or instructions, such as a Python script, while a folder is a container that holds files and other folders. Every file on a computer exists inside exactly one folder, and folders can be nested inside other folders.\nWhen working in the terminal, the computer always keeps track of a single location called the current working directory. This directory represents “where you are” in the file system at any given moment. All commands you type into the terminal are interpreted relative to this location unless you explicitly say otherwise.\nYou can ask the terminal to show the current working directory using the following command:\npwd\nThe output shows the full path to the folder the terminal is currently using. This location matters because most commands—including those that run Python scripts—operate on files in this directory by default.\nTo see what files and folders exist in the current working directory, you can list its contents:\nls\nThe terminal responds by displaying the names of files and folders in that location. If a Python script does not appear in this list, it means the terminal cannot “see” it from the current directory.\nThis explains why a command such as:\npython script.py\nonly works when the file named script.py is located in the current working directory. When this command is run, Python looks for script.py in the folder the terminal is currently pointing to. If the file is elsewhere, Python cannot run it and will report that the file cannot be found.\nPrograms locate files using the same logic. When a program is executed, it starts in the current working directory and interprets file references relative to that location. If a script refers to another file without specifying a full path, Python assumes that file is located in—or relative to—the directory where the program was run.\nChanging the current working directory changes what files are visible to the terminal and to any programs launched from it. Moving between folders is therefore a fundamental skill when working with scripts and project-based code. In the next section, this idea is extended by introducing paths, which provide precise ways to describe file locations both relative to the current directory and relative to the entire file system.\n\n\n8.3 Relative vs absolute paths\nA path is a description of where a file or folder is located on a computer. Paths allow both humans and programs to refer to specific locations in the file system. When working in the terminal, paths are how you tell the computer which file or folder you mean.\nThere are two main types of paths: absolute paths and relative paths. The difference between them depends on where the path begins.\nAn absolute path describes a location starting from the root of the file system. It specifies the full sequence of folders that must be followed to reach a file, regardless of the current working directory. Because absolute paths always start from the same place, they uniquely identify a file’s location.\nFor example, an absolute path might look like this:\n/Users/username/projects/week1/hello_world.py\nThis path tells the computer exactly where the file lives, no matter where the terminal is currently pointed. Absolute paths are precise, but they can be long, system-specific, and inconvenient to type repeatedly.\nA relative path, by contrast, describes a location starting from the current working directory. Instead of beginning at the root of the file system, a relative path is interpreted based on where you are at the moment the command is run.\nFor example, if the terminal is already inside the week1 folder, the same file could be referred to simply as:\nhello_world.py\nRelative paths are shorter and easier to read, but they only make sense in relation to the current working directory. This is why understanding where you are in the file system is so important when using the terminal.\nIn most projects, relative paths are used far more often than absolute paths. Relative paths make code easier to move between computers and directories without modification. If a project folder is copied or shared, relative paths continue to work as long as the internal structure of the project remains the same.\nTwo special symbols are commonly used in relative paths. The symbol . refers to the current directory, while .. refers to the parent directory, which is the folder that contains the current one. These symbols provide a concise way to navigate up and down the folder hierarchy.\nFor example, moving up one level in the directory structure looks like this:\ncd ..\nUsing these symbols allows paths to express relationships between folders rather than fixed locations. This relational view of file locations is central to working effectively with scripts, projects, and command-line tools.\nUnderstanding paths—especially the distinction between relative and absolute paths—helps explain many common terminal errors and clarifies how programs locate the files they need. With this foundation in place, navigating the file system becomes a logical process rather than a trial-and-error exercise.\n\n\n8.4 Essential navigation commands (conceptual overview)\nWorking in the terminal involves a small set of core commands that allow you to navigate the file system and understand what files are available at any given moment. These commands are used constantly when working with Python scripts, not because they are complex, but because they express intent very directly.\nOne of the most common tasks is moving between folders. This is done by changing the current working directory. Conceptually, this is no different from opening a different folder in a graphical interface, except that it is done by issuing a command rather than clicking.\nFor example, moving into a folder looks like this:\ncd projects\nAfter this command runs, the terminal’s current working directory changes to the projects folder. All subsequent commands are interpreted relative to this new location.\nAnother common task is listing the contents of a folder. This allows you to see which files and subfolders exist in the current directory:\nls\nListing files is often the first step when something does not work as expected. If a file does not appear in the listing, it means the terminal cannot see it from the current location.\nFolders are also created directly from the terminal. Creating folders is especially useful for organizing projects and keeping related files together:\nmkdir week1\nThis command creates a new folder named week1 inside the current working directory. Organizing code into folders helps keep scripts, data, and outputs clearly separated, which becomes increasingly important as projects grow.\nThese navigation commands matter because the terminal always operates in a specific location. Python scripts are run from that location, files are created there by default, and relative paths are resolved based on it. When a command behaves unexpectedly, the cause is often not the command itself, but the directory from which it was run.\nThe goal is not to memorize commands, but to understand their intent:\n- Where am I?\n- What files are here?\n- Where do I want to go next?\nOnce this mental model is in place, the specific command names become easier to remember, and working in the terminal becomes a predictable and logical process rather than a trial-and-error activity.\n\n\n8.5 How the terminal connects to Python execution\nThe terminal plays a central role in running Python scripts because it provides the context in which commands are interpreted. When a Python script is executed, the terminal is responsible for telling Python which file to run and where to find it.\nWhen you type a command such as:\npython first_program.py\nyou are giving Python two pieces of information at once. First, you are specifying that the Python interpreter should be used. Second, you are specifying the name of the file to execute. What is not explicitly stated—but is critically important—is where Python should look for that file.\nBy default, Python looks for the script in the current working directory. That directory is determined entirely by the terminal’s location at the moment the command is run. If the file named first_program.py is located in that directory, Python can execute it. If it is not, Python reports an error indicating that the file cannot be found.\nThis explains why errors such as “file not found” or “no such file or directory” occur so frequently. In many cases, the issue is not that the file does not exist, but that the terminal is pointed at the wrong folder when the command is issued.\nTerminal navigation commands directly affect this outcome. Changing directories changes what files are visible to Python. Listing files allows you to verify whether the script you want to run is actually present in the current location. Together, these actions form a simple but powerful workflow:\n\nNavigate to the folder containing the script.\n\nConfirm the file is present.\n\nRun the script using the Python command.\n\nProject folder structure reinforces this workflow. When scripts are organized into predictable folders, it becomes easier to navigate to the correct location and run code reliably. Relative paths and consistent organization reduce the likelihood of execution errors and make projects easier to understand and maintain.\nThis leads to a useful mental model for running Python scripts:\nlocation → command → execution\nFirst, the terminal’s location determines what files are accessible. Next, the command specifies what action to take. Finally, Python executes the script based on that context. When something goes wrong, tracing the problem through these three steps is often the fastest way to identify and resolve the issue.\nUnderstanding this connection between the terminal and Python execution transforms error messages from obstacles into signals. Rather than guessing, it becomes possible to reason systematically about what the computer is doing and why a particular command succeeds or fails.\n\n\n8.6 Common mistakes and recovery strategies\nMistakes in the terminal are not signs of failure or lack of ability. They are a normal and expected part of learning to work with files, paths, and scripts. Most issues encountered at this stage fall into a small number of common patterns, and each has straightforward recovery strategies.\nOne frequent mistake is running commands from the wrong directory. When the terminal is pointed at a different folder than expected, commands such as running a Python script will fail because the file cannot be found. In these situations, the solution is not to change the command, but to confirm the terminal’s current location and navigate to the correct folder before trying again.\nAnother common issue is confusing file names or extensions. Python scripts must be referenced using their exact file name, including the .py extension. A missing or misspelled character is enough to cause an error. Verifying file names by listing the contents of the directory often resolves this type of problem quickly.\nIt is also easy to forget where files were saved, especially when working across multiple folders or projects. When this happens, the most effective approach is to slow down and retrace steps: identify the project folder, navigate into it, and inspect its contents. Guessing or repeatedly trying variations of a command rarely helps and often increases frustration.\nGetting “unstuck” usually involves a small set of reliable actions:\n- Check the current working directory.\n- List the files in that directory.\n- Confirm the script’s name and location.\n- Re-run the command once the context is correct.\nThese steps are simple, but they are powerful because they restore clarity about what the computer can see and what it is being asked to do.\nPerhaps most importantly, these mistakes are not unique to beginners. Even experienced programmers regularly encounter them, especially when switching projects or environments. What changes with experience is not the absence of mistakes, but the speed and confidence with which they are resolved.\nLearning to work in the terminal is as much about developing patience and a clear mental model as it is about learning commands. With practice, errors become signals rather than obstacles, and recovery becomes a routine part of working effectively with code."
  },
  {
    "objectID": "textbook_src/week01_main.html#virtual-environments-overview",
    "href": "textbook_src/week01_main.html#virtual-environments-overview",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "9. Virtual Environments Overview",
    "text": "9. Virtual Environments Overview\nUp to this point, the focus has been on understanding how the terminal interacts with files and how Python scripts are located and executed. These ideas establish where commands run and which files they act on. The next layer adds an equally important question: which version of Python, and which set of installed tools, should be used when a script runs.\nThis question becomes especially important as projects grow and as additional libraries are introduced. Virtual environments provide a structured way to manage this complexity by controlling the software context in which Python programs execute. The sections that follow introduce virtual environments as a practical solution to a common problem, building directly on the terminal-based workflows already established.\n\n9.1 Why virtual environments exist\nVirtual environments exist to solve a practical problem that arises when working with Python across multiple projects: different projects often require different packages, or different versions of the same package.\nBy default, Python allows packages to be installed globally, meaning they are available to all Python programs on a computer. While this may seem convenient at first, it quickly leads to conflicts. One project may require a newer version of a library, while another depends on an older version. Installing or upgrading a package for one project can unintentionally break another.\nThese conflicts are especially common in analytics and AI work, where projects often rely on rapidly evolving libraries. Changes in package behavior, version compatibility, or dependencies can cause code that once worked correctly to fail without any changes to the code itself.\nVirtual environments address this problem by providing isolation. Each environment contains its own copy of the Python interpreter along with its own set of installed packages. This allows each project to define and control exactly which packages and versions it uses, without affecting other projects or the system-wide Python installation.\nImportantly, virtual environments are not about adding complexity for its own sake. They are a way to reduce uncertainty and prevent accidental breakage. By isolating dependencies, environments make projects more predictable, easier to reproduce, and easier to share with others.\nFrom a workflow perspective, virtual environments support a simple principle: a project should carry its own assumptions about its software dependencies. When those assumptions are explicit and isolated, problems become easier to diagnose and fixes become easier to apply.\nUnderstanding why virtual environments exist provides context for the steps that follow. Rather than treating environment setup as a ritual to memorize, it becomes clear that environments are a practical response to a real and common problem in Python-based work.\n\n\n9.2 What a virtual environment is\nA virtual environment is a self-contained Python setup that exists alongside, but separate from, the system-wide Python installation. It provides a controlled space in which a project can run using a specific Python interpreter and a specific set of installed packages.\nEach virtual environment includes its own Python interpreter and its own package directory. When a program is run inside an environment, Python uses the interpreter and packages associated with that environment rather than those installed globally on the system. This separation is what allows multiple projects with different requirements to coexist on the same computer without interfering with one another.\nVirtual environments are closely tied to two components: the Python interpreter and installed packages. The interpreter determines which version of Python is used, while the installed packages determine which libraries and tools are available to the program. Together, these define the software context in which a script executes.\nWhen a virtual environment is activated, the terminal is instructed to use the environment’s Python interpreter by default. As a result, running Python commands or executing scripts uses the environment’s configuration rather than the system-wide one. Activation does not change Python itself; it changes which Python is selected when commands are run.\nIt is important to understand what activation does not do. Activating a virtual environment does not delete or replace the system Python installation. It does not modify other environments, and it does not affect projects outside the current working context. The system Python remains available and unchanged; the environment simply provides an alternative that is used temporarily.\nThis distinction is essential for developing confidence with environments. Virtual environments do not take control of the computer or permanently alter Python. They provide a scoped, reversible context in which projects can be developed and executed reliably. Once this idea is clear, the mechanics of using environments become much easier to understand and apply.\n\n\n9.3 The course-standard environment workflow (using uv)\nTo manage virtual environments consistently, this course uses a single tool: uv. Rather than introducing multiple environment managers or allowing a mix of tools, one standard workflow is used throughout. This reduces confusion, minimizes setup errors, and ensures that examples behave the same way for everyone.\nThe choice to standardize on one tool is intentional. Environment management is not the core subject of the course; it is supporting infrastructure. Using a single, reliable tool allows attention to remain on analytics, AI concepts, and Python itself rather than on resolving tooling differences.\nAt a high level, the environment workflow consists of three steps:\n\nCreate an environment\nA new virtual environment is created for the project. This environment serves as an isolated space where the project’s Python interpreter and packages will live.\nActivate the environment\nActivating the environment tells the terminal to use the environment’s Python interpreter by default. From this point forward, Python commands and scripts run within the context of the environment rather than the system-wide installation.\nInstall dependencies\nRequired packages are installed into the environment. These packages become available only within that environment, ensuring that the project’s dependencies are isolated and controlled.\n\nThese steps form a repeatable process that is used for every project. While the specific commands will be introduced later, the important idea is the sequence itself. Creating, activating, and installing are distinct actions with different purposes, and each plays a role in establishing a predictable execution context.\nBy emphasizing process rather than command memorization, the workflow remains understandable even as tools evolve. The details of how uv performs each step may change over time, but the underlying structure of environment-based development remains the same. Understanding this structure makes it easier to reason about errors, recover from mistakes, and apply the same approach in future projects beyond this course.\n\n\n9.4 Activating and deactivating environments (conceptual)\nActivating a virtual environment changes how the terminal interprets Python-related commands. Rather than altering Python itself, activation tells the terminal which Python interpreter and which set of packages should be used when commands are executed.\nWhen an environment is activated, the terminal is temporarily configured so that Python commands refer to the environment’s Python interpreter instead of the system-wide one. As a result, any script that is run uses the packages installed in that environment. This is why activation is such an important step in the workflow: it determines the software context in which code executes.\nActivation affects two key things. First, it determines which Python runs. Even if multiple versions of Python exist on a computer, activation ensures that the correct interpreter is used for the current project. Second, it determines which packages are available. Only the packages installed in the active environment can be imported and used by scripts.\nForgetting to activate an environment is a common source of confusion. When this happens, Python may still run, but it may use the system-wide interpreter and packages instead of the project-specific ones. This can lead to errors where code appears correct but fails because required packages are missing or the wrong versions are being used.\nDeactivating an environment simply returns the terminal to its default state. After deactivation, Python commands once again refer to the system-wide Python installation. No files are deleted, and no environments are removed; the change is temporary and reversible.\nUnderstanding activation and deactivation reinforces an important idea: virtual environments are contexts, not permanent changes. They define how Python behaves in a given terminal session, and they can be entered and exited as needed. With this mental model in place, environment-related issues become easier to identify and resolve.\n\n\n9.5 How environments connect to the terminal and Visual Studio Code\nVirtual environments are closely tied to the terminal session in which they are activated. When an environment is activated, the change applies only to that specific terminal window. Other terminal windows remain unaffected unless the environment is activated there as well. This session-specific behavior is intentional and allows different projects to be worked on simultaneously using different environments.\nBecause activation is terminal-specific, it is possible for two terminals on the same computer to behave differently at the same time. One terminal may be using a project’s virtual environment, while another is using the system-wide Python installation. Understanding this distinction helps explain why code may run successfully in one terminal but fail in another.\nVisual Studio Code builds on this behavior by integrating the terminal and the editor. When a project folder is opened in VS Code, the editor scans the folder structure to detect virtual environments. If an environment is found, VS Code can associate that environment with the project and use it when running Python scripts.\nVS Code’s Python tooling uses this association to determine which Python interpreter should be used for execution, linting, and debugging. When the correct environment is selected, running a script from the editor or from the integrated terminal uses the same Python configuration. This alignment reduces confusion and helps ensure consistent behavior across tools.\nAt this point, several concepts come together:\n- The terminal location determines which files are visible.\n- The active environment determines which Python interpreter and packages are used.\n- The execution command tells Python which script to run.\nAll three must align for code to run as expected. If any one of them is incorrect—wrong directory, wrong environment, or wrong command—errors can occur even if the code itself is correct.\nViewing these elements as a coordinated system rather than independent pieces makes troubleshooting much easier. Instead of guessing, it becomes possible to check each part in turn: where the terminal is, which environment is active, and which Python is being used. This systems-level understanding is key to working confidently with Python projects in both the terminal and Visual Studio Code.\n\n\n9.6 Common environment mistakes and recovery strategies\nMistakes involving virtual environments are extremely common and occur at all experience levels. These issues rarely indicate a problem with the code itself. Instead, they usually arise from a mismatch between the environment that was intended and the one that is actually being used.\nOne frequent mistake is installing packages in the wrong environment. This happens when packages are installed while the system-wide Python environment is active instead of the project’s virtual environment. As a result, a script may fail to import a package even though it appears to be installed. The package exists—but not in the environment the script is using.\nAnother common issue is running scripts with the wrong Python interpreter. Because multiple Python interpreters can exist on the same computer, it is possible to run a script using a different interpreter than expected. When this occurs, the script may behave inconsistently across terminals or tools, even though no changes were made to the code.\nIt is also easy to forget which environment is active, especially when switching between projects or terminal windows. Since activation is specific to each terminal session, opening a new terminal often means starting without any environment activated. This can lead to confusion when previously working code suddenly fails.\nRecovering from environment-related issues usually involves a small number of simple checks:\n- Confirm which environment is active in the current terminal.\n- Verify which Python interpreter is being used.\n- Ensure required packages are installed in that environment.\n- Re-activate the intended environment if necessary.\nThese steps are effective because they reestablish clarity about the execution context before any changes are made. Guessing or reinstalling packages repeatedly is rarely helpful without first confirming which environment is actually in use.\nMost importantly, these mistakes are routine and fixable. Even experienced developers encounter them regularly, particularly when working across multiple projects or tools. With practice, identifying environment issues becomes a quick diagnostic step rather than a source of frustration.\nVirtual environments are designed to make work more reliable, not more fragile. Once their role is understood, environment-related problems become signals to check context rather than obstacles to progress."
  },
  {
    "objectID": "textbook_src/week01_main.html#week-1-summary",
    "href": "textbook_src/week01_main.html#week-1-summary",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "Week 1 Summary",
    "text": "Week 1 Summary\nThis chapter established the foundational ideas and workflows that will be used throughout the course. Conceptually, it introduced analytics as a progression from description to prescription, and positioned artificial intelligence as systems that extend analytics by embedding models into decision-making processes. Rather than treating analytics and AI as competing ideas, the chapter emphasized their overlap and the importance of understanding how model outputs are ultimately used.\nA central organizing framework was the decomposition of AI systems into data, models, and logic. This lens provides a practical way to reason about system behavior, diagnose failures, and understand why strong predictive performance alone does not guarantee good decisions. The discussion of AI paradigms—symbolic, statistical, neural, and hybrid—reinforced the idea that real-world systems are assembled from multiple approaches rather than built around a single technique.\nFrom a practical perspective, the chapter introduced the core mechanics required to work with Python in a professional, script-based workflow. You learned how Python scripts execute, how simple programs are constructed using printing, variables, and expressions, and why the edit → run → fix loop is the normal mode of development. These skills form the computational foundation for later analytical work.\nThe chapter also emphasized the importance of execution context. By introducing the terminal, file navigation, paths, and virtual environments, it clarified how Python code is located, executed, and isolated across projects. Rather than treating these tools as technical hurdles, the chapter framed them as mechanisms for control, predictability, and reproducibility.\nBy the end of Week 1, you should have both a conceptual framework for understanding analytics and AI systems and a practical workflow for running Python code reliably. These foundations will be built upon in subsequent chapters, where data analysis, modeling, and AI techniques become more sophisticated—but always within the same mental and operational structure introduced here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Analytics & AI",
    "section": "",
    "text": "Welcome to the online textbook for the course. It is organized to provide a clear, structured pathway through the foundational ideas and practical techniques of analytics and artificial intelligence. Each week’s chapter includes:\n\nAn explaination of the key/core concepts and our objectives for the week.\n\nDemonstrations (where applicable) and worked examples\n\nLinks to downloadable code associated with the topics in the lectures.\n\nPractice sections and applied exercises\n\nShort conceptual notes to reinforce key ideas\n\nThis site is designed to complement the in-class experience while remaining accessible for self-study. All materials can be viewed online and code files can be downloaded from the course repository. The code in github and on Canvas is the same. The data for this course is stored in Canvas."
  },
  {
    "objectID": "index.html#course-textbook",
    "href": "index.html#course-textbook",
    "title": "Foundations of Analytics & AI",
    "section": "",
    "text": "Welcome to the online textbook for the course. It is organized to provide a clear, structured pathway through the foundational ideas and practical techniques of analytics and artificial intelligence. Each week’s chapter includes:\n\nAn explaination of the key/core concepts and our objectives for the week.\n\nDemonstrations (where applicable) and worked examples\n\nLinks to downloadable code associated with the topics in the lectures.\n\nPractice sections and applied exercises\n\nShort conceptual notes to reinforce key ideas\n\nThis site is designed to complement the in-class experience while remaining accessible for self-study. All materials can be viewed online and code files can be downloaded from the course repository. The code in github and on Canvas is the same. The data for this course is stored in Canvas."
  },
  {
    "objectID": "index.html#how-to-use-this-textbook",
    "href": "index.html#how-to-use-this-textbook",
    "title": "Foundations of Analytics & AI",
    "section": "How to Use This Textbook",
    "text": "How to Use This Textbook\nThe material is organized into three types:\n\n1. Weekly Modules\nThese chapters follow the course schedule and lectures. The readings from the text are not identical to the lectures, but they are close. The readings introduce new concepts, connect them to analytics and AI practice, and provide examples to run independently.\nStart here:\n\nWeek 1 – What is Analytics & AI?\n\nWeek 2 – Python Basics\n\nWeek 3 – Data Structures & Pandas\n\nWeek 4 – From Data to Models\n\n\n\n\n\n2. Practical Guides (Task-Oriented Documentation)\nThese pages provide focused instructions for installing tools, configuring environments, and resolving common issues.\n\nHow to Install Python\n\nInstalling the Course requirements.txt\n\nUsing Google Colab\n\nThese guides remain relevant throughout the semester and may be updated as tools evolve."
  },
  {
    "objectID": "index.html#course-learning-philosophy",
    "href": "index.html#course-learning-philosophy",
    "title": "Foundations of Analytics & AI",
    "section": "Course Learning Philosophy",
    "text": "Course Learning Philosophy\nThis textbook is built around two related objectives:\n\nAI Literacy: developing conceptual understanding of modern analytics and AI systems. How they work, where they succeed, and where they sometimes fail.\n\nJob-Relevant Skills: Translating these concepts into practical, executable workflows using Python and contemporary tools.\n\nEach week reinforces both trajectories: one conceptual, one applied."
  },
  {
    "objectID": "index.html#accessing-code-and-data",
    "href": "index.html#accessing-code-and-data",
    "title": "Foundations of Analytics & AI",
    "section": "Accessing Code and Data",
    "text": "Accessing Code and Data\nAll example code for the course is maintained in the main GitHub repository:\n\nCode folder: code/ (available in GitHub and Canvas) Data folder: data/ (available in Canvas only)\n\nYou may download the entire repository as a .zip file or clone it directly.\nLinks to specific scripts and notebooks appear at the end of each week’s chapter."
  },
  {
    "objectID": "index.html#need-help",
    "href": "index.html#need-help",
    "title": "Foundations of Analytics & AI",
    "section": "Need Help?",
    "text": "Need Help?\nIf you encounter technical issues with installation, environments, or package setup, consult the Guides section using the navigation bar above. These pages provide step-by-step instructions tailored for this course."
  },
  {
    "objectID": "textbook_src/week02_main.html",
    "href": "textbook_src/week02_main.html",
    "title": "Week 2 – Python Basics",
    "section": "",
    "text": "This chapter will introduce Python fundamentals needed for the course.\n\n\n\nVariables and data types\n\nControl flow (if/else, loops)\n\nBasic functions\n\n\n\n\n(links will be added later)"
  },
  {
    "objectID": "textbook_src/week02_main.html#week-2-overview",
    "href": "textbook_src/week02_main.html#week-2-overview",
    "title": "Week 2 – Python Basics",
    "section": "",
    "text": "This chapter will introduce Python fundamentals needed for the course.\n\n\n\nVariables and data types\n\nControl flow (if/else, loops)\n\nBasic functions\n\n\n\n\n(links will be added later)"
  },
  {
    "objectID": "textbook_src/week04_main.html",
    "href": "textbook_src/week04_main.html",
    "title": "Week 4 – From Data to Models",
    "section": "",
    "text": "This chapter will introduce the foundational ideas behind building predictive models.\n\n\n\nTrain/test split\n\nLinear models\n\nModel evaluation\n\n\n\n\n(links will be added later)"
  },
  {
    "objectID": "textbook_src/week04_main.html#week-4-overview",
    "href": "textbook_src/week04_main.html#week-4-overview",
    "title": "Week 4 – From Data to Models",
    "section": "",
    "text": "This chapter will introduce the foundational ideas behind building predictive models.\n\n\n\nTrain/test split\n\nLinear models\n\nModel evaluation\n\n\n\n\n(links will be added later)"
  },
  {
    "objectID": "guides/install_python.html",
    "href": "guides/install_python.html",
    "title": "Setting Up Python",
    "section": "",
    "text": "This document will walk you through the steps to:\n\nInstall Python 3\n\nInstall Visual Studio Code (VS Code)\n\nConfigure VS Code to run .py files\n\nVerify that everything is working correctly\n\nYou only need to complete this setup once on your computer.\nAfter that, you will be able to run all course examples and labs.\n\n\n\n\n\n\nTip\n\n\n\nIf you get stuck at any point:\nInstalling this should be straightforward. LLMs can be a great way to problem solve an install. Use UFs Navigator Chat to help you. There is an optional video on this in the course files. Still stuck? Reach out to a TA and let them help you out. Use the course guidelines for how to ask good questions to request help."
  },
  {
    "objectID": "guides/install_python.html#on-windows",
    "href": "guides/install_python.html#on-windows",
    "title": "Setting Up Python",
    "section": "On Windows?",
    "text": "On Windows?\n\nOpen the Start Menu.\n\nType cmd and press Enter to open the Command Prompt.\n\nIn the black window, type:\n\npy --version\nor\npython --version\n\nPress Enter.\n\n\nIf you see something like Python 3.10.11, Python 3 is already installed.\n\nIf you see an error or a much older version (like Python 2.x), you should install a current version.\nMost MAC’s will have Python installed, but this is for your system, you want to install it again. We don’t want to be making changes to the factory installed Python."
  },
  {
    "objectID": "guides/install_python.html#on-macos",
    "href": "guides/install_python.html#on-macos",
    "title": "Setting Up Python",
    "section": "on macOS?",
    "text": "on macOS?\n\nOpen Terminal (Spotlight → type Terminal).\n\nType:\n\npython3 --version\n\nPress Enter.\n\n\nIf you see Python 3.x.x, you have Python 3.\n\nFor the most part, you should install a new version, even if you see this."
  },
  {
    "objectID": "guides/install_python.html#on-linux",
    "href": "guides/install_python.html#on-linux",
    "title": "Setting Up Python",
    "section": "on Linux?",
    "text": "on Linux?\n\nOpen a terminal.\n\nType:\n\npython3 --version\n\nPress Enter.\n\nIf Python 3 is not installed, follow the installation steps below."
  },
  {
    "objectID": "guides/install_python.html#windows-installation",
    "href": "guides/install_python.html#windows-installation",
    "title": "Setting Up Python",
    "section": "Windows Installation",
    "text": "Windows Installation\n\nClick Download Python 3.x.x for Windows.\n\nRun the installer (python-3.x.x.exe) after it downloads.\n\nOn the first screen:\n\nCheck the box that says:\n“Add Python 3.x to PATH”\n\nThen click “Install Now”.\n\nWait for the installation to finish.\n\nClick “Close” when done.\n\n\nVerify Python on Windows\nOpen Command Prompt again and type:\npy --version\nor\npython --version\nYou should now see Python 3.x.x.\nIf you don’t, restart your computer and try again."
  },
  {
    "objectID": "guides/install_python.html#macos-installation",
    "href": "guides/install_python.html#macos-installation",
    "title": "Setting Up Python",
    "section": "macOS Installation",
    "text": "macOS Installation\n\nOn https://www.python.org/downloads/, choose “Download Python 3.x.x for macOS”.\n\nOpen the downloaded .pkg file.\n\nFollow the installer prompts and accept the defaults.\n\n\nVerify Python on macOS\nOpen Terminal and type:\npython3 --version\nYou should see Python 3.x.x.\nIf you don’t, log out and back in, or restart your machine."
  },
  {
    "objectID": "guides/install_python.html#linux-installation-brief",
    "href": "guides/install_python.html#linux-installation-brief",
    "title": "Setting Up Python",
    "section": "Linux Installation (brief)",
    "text": "Linux Installation (brief)\nOn many Linux distributions, Python 3 is already installed.\nIf not, you can install it with your package manager. For example:\n\nUbuntu / Debian:\nsudo apt-get update\nsudo apt-get install python3 python3-pip\nFedora:\nsudo dnf install python3 python3-pip\n\nAfter installation, check:\npython3 --version"
  },
  {
    "objectID": "guides/install_python.html#download-and-install-vs-code",
    "href": "guides/install_python.html#download-and-install-vs-code",
    "title": "Setting Up Python",
    "section": "Download and Install VS Code",
    "text": "Download and Install VS Code\n\nGo to: https://code.visualstudio.com/\n\nClick Download for your operating system (Windows, macOS, or Linux).\n\nRun the installer and accept the defaults."
  },
  {
    "objectID": "guides/install_python.html#install-the-python-extension-in-vs-code",
    "href": "guides/install_python.html#install-the-python-extension-in-vs-code",
    "title": "Setting Up Python",
    "section": "Install the Python Extension in VS Code",
    "text": "Install the Python Extension in VS Code\n\nOpen VS Code.\n\nOn the left sidebar, click the Extensions icon (four squares).\n\nIn the search bar, type: Python.\n\nFind the extension published by Microsoft (it should be at the top).\n\nClick Install.\n\nVS Code will now understand and support Python files."
  },
  {
    "objectID": "guides/install_python.html#running-the-script-vs-code-integrated-terminal",
    "href": "guides/install_python.html#running-the-script-vs-code-integrated-terminal",
    "title": "Setting Up Python",
    "section": "Running the Script (VS Code Integrated Terminal)",
    "text": "Running the Script (VS Code Integrated Terminal)\n\nIn VS Code, open the Terminal:\n\nView → Terminal, or\n\nPress Ctrl+` (Windows/Linux)\n\nPress Ctrl+Shift+` (macOS, depending on keybindings)\n\nMake sure the terminal is using the correct folder. You should see a path that ends with week01.\nType one of the following commands and press Enter:\n\nOn Windows:\npy test_setup.py\nor\npython test_setup.py\nOn macOS/Linux:\npython3 test_setup.py\n\nYou should see:\n\nPython is working, awesome!\nIf you see that message, your environment is ready."
  },
  {
    "objectID": "guides/install_python.html#python-is-not-recognized-on-windows",
    "href": "guides/install_python.html#python-is-not-recognized-on-windows",
    "title": "Setting Up Python",
    "section": "“Python is not recognized” on Windows",
    "text": "“Python is not recognized” on Windows\nIf you see:\n'python' is not recognized as an internal or external command\nor\n'py' is not recognized...\nTry:\n\nRestarting your computer.\n\nRunning the installer again and making sure “Add Python to PATH” is checked."
  },
  {
    "objectID": "guides/install_python.html#scripts-not-finding-data-files",
    "href": "guides/install_python.html#scripts-not-finding-data-files",
    "title": "Setting Up Python",
    "section": "Scripts not finding data files",
    "text": "Scripts not finding data files\nMake sure:\n\nYour .py script and your data files are in the same folder, or\n\nYour script uses the correct relative path, like:\n\nimport pandas as pd\n\ndf = pd.read_csv(\"data/housing_prices.csv\")\nIf you move files around, update the paths accordingly."
  }
]