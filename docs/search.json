[
  {
    "objectID": "guides/using_colab.html",
    "href": "guides/using_colab.html",
    "title": "Using Google Colab for This Course",
    "section": "",
    "text": "Google Colab is a free, cloud-based environment for running Python notebooks.\nYou can use it from any computer without installing Python locally. It is especially helpful if:\n\nYour computer cannot install packages reliably\n\nYou are troubleshooting your Python installation\n\nYou want quick access to a working Python environment\n\nThis guide explains how to open Colab, upload or access notebooks, install packages, work with files, and download your work."
  },
  {
    "objectID": "guides/using_colab.html#temporary-upload",
    "href": "guides/using_colab.html#temporary-upload",
    "title": "Using Google Colab for This Course",
    "section": "Temporary Upload",
    "text": "Temporary Upload\nfrom google.colab import files\nuploaded = files.upload()"
  },
  {
    "objectID": "guides/using_colab.html#mount-google-drive",
    "href": "guides/using_colab.html#mount-google-drive",
    "title": "Using Google Colab for This Course",
    "section": "Mount Google Drive",
    "text": "Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\nYour Drive will appear at:\n/content/drive/MyDrive/"
  },
  {
    "objectID": "guides/install_requirements.html",
    "href": "guides/install_requirements.html",
    "title": "Installing the Course Requirements and Checking Your Environment",
    "section": "",
    "text": "This guide will walk you through:\n\nDownloading the course requirements file\n\nUsing the terminal / command line to navigate to your course folder\n\nInstalling the required Python packages with pip install -r requirements.txt\n\nRunning a small Python script to confirm that everything is set up correctly\n\nBy the end of this guide, your Python environment should be ready for the rest of the semester."
  },
  {
    "objectID": "guides/install_requirements.html#download-the-files",
    "href": "guides/install_requirements.html#download-the-files",
    "title": "Installing the Course Requirements and Checking Your Environment",
    "section": "1.1 — Download the files",
    "text": "1.1 — Download the files\n\nGo to the location where your instructor has shared the files.\n\nDownload requirements.txt.\n\nDownload check_environment.py."
  },
  {
    "objectID": "guides/install_requirements.html#save-them-to-your-week-1-folder",
    "href": "guides/install_requirements.html#save-them-to-your-week-1-folder",
    "title": "Installing the Course Requirements and Checking Your Environment",
    "section": "1.2 — Save them to your Week 1 folder",
    "text": "1.2 — Save them to your Week 1 folder\nSave both files in:\nAI_Course/week01/\nSo your folder structure looks like:\nAI_Course/\n  week01/\n    requirements.txt\n    check_environment.py\n  week02/\n  ..."
  },
  {
    "objectID": "guides/install_requirements.html#windows-vs-code-recommended",
    "href": "guides/install_requirements.html#windows-vs-code-recommended",
    "title": "Installing the Course Requirements and Checking Your Environment",
    "section": "Windows (VS Code recommended)",
    "text": "Windows (VS Code recommended)\n\nOpen VS Code.\n\nClick File → Open Folder… → select AI_Course/week01/.\n\nGo to View → Terminal."
  },
  {
    "objectID": "guides/install_requirements.html#macos-linux",
    "href": "guides/install_requirements.html#macos-linux",
    "title": "Installing the Course Requirements and Checking Your Environment",
    "section": "macOS / Linux",
    "text": "macOS / Linux\n\nOpen Terminal.\n\nUse cd to navigate to your Week 1 folder (see next step)."
  },
  {
    "objectID": "textbook_src/week03_main.html",
    "href": "textbook_src/week03_main.html",
    "title": "Week 3 – Working with Data: Pandas, Files, and APIs",
    "section": "",
    "text": "What a dataset is and how structured data is organized.\n\nHow Pandas represents tabular data using DataFrames.\n\nHow to load, inspect, clean, and transform datasets.\n\nHow to summarize data using descriptive statistics.\n\nHow JSON structures data and why it matters for AI systems.\n\nHow to retrieve data from an API using Python.\n\n\n\n\n\nPython scripts that load and inspect real datasets.\n\nCleaned and transformed DataFrames.\n\nSummary statistics computed with Pandas.\n\nData retrieved from a public API and parsed from JSON.\n\nA complete end-to-end data workflow in the mini-lab.\n\n\n\n\nBy the end of this week, you should be able to:\n\nExplain what a dataset is and how structure enables analysis.\n\nDescribe how tabular data supports analytics and AI workflows.\n\nExplain the role of data cleaning in model reliability.\n\nDescribe how APIs and JSON support modern data pipelines.\n\n\n\n\nBy the end of this week, you should be able to:\n\nLoad data from CSV files using Pandas.\n\nInspect datasets for structure and quality.\n\nHandle missing values and clean columns.\n\nFilter and select data using Pandas.\n\nCompute descriptive statistics.\n\nFetch and parse JSON data from an API."
  },
  {
    "objectID": "textbook_src/week03_main.html#week-3-overview",
    "href": "textbook_src/week03_main.html#week-3-overview",
    "title": "Week 3 – Working with Data: Pandas, Files, and APIs",
    "section": "",
    "text": "What a dataset is and how structured data is organized.\n\nHow Pandas represents tabular data using DataFrames.\n\nHow to load, inspect, clean, and transform datasets.\n\nHow to summarize data using descriptive statistics.\n\nHow JSON structures data and why it matters for AI systems.\n\nHow to retrieve data from an API using Python.\n\n\n\n\n\nPython scripts that load and inspect real datasets.\n\nCleaned and transformed DataFrames.\n\nSummary statistics computed with Pandas.\n\nData retrieved from a public API and parsed from JSON.\n\nA complete end-to-end data workflow in the mini-lab.\n\n\n\n\nBy the end of this week, you should be able to:\n\nExplain what a dataset is and how structure enables analysis.\n\nDescribe how tabular data supports analytics and AI workflows.\n\nExplain the role of data cleaning in model reliability.\n\nDescribe how APIs and JSON support modern data pipelines.\n\n\n\n\nBy the end of this week, you should be able to:\n\nLoad data from CSV files using Pandas.\n\nInspect datasets for structure and quality.\n\nHandle missing values and clean columns.\n\nFilter and select data using Pandas.\n\nCompute descriptive statistics.\n\nFetch and parse JSON data from an API."
  },
  {
    "objectID": "textbook_src/week03_main.html#data-concepts-what-is-a-dataset",
    "href": "textbook_src/week03_main.html#data-concepts-what-is-a-dataset",
    "title": "Week 3 – Working with Data: Pandas, Files, and APIs",
    "section": "1. Data Concepts: What Is a Dataset?",
    "text": "1. Data Concepts: What Is a Dataset?\nBefore working with tools like Pandas or loading files into Python, it is important to clarify what we mean by a dataset. In analytics and AI, datasets are not just collections of numbers or text—they are structured representations of observations about the world. How data is organized determines what kinds of questions can be asked and what kinds of analysis are possible.\nThis section introduces datasets as the foundational input to analytics and AI systems.\n\n\n1.1 What a dataset represents\nA dataset is a structured collection of observations. Each observation represents a single instance, case, or entity, and each observation is described using a consistent set of attributes.\nIn most analytical contexts, datasets are organized in a tabular form:\n- Rows represent individual observations or records.\n- Columns represent variables or attributes measured for each observation.\nFor example, a dataset of students might contain one row per student and columns for attributes such as major, exam score, or graduation year. A dataset of transactions might contain one row per transaction and columns describing amount, date, or location.\nThis structure allows datasets to be treated as inputs to analytics and AI systems. Models, summaries, and visualizations all assume that data is organized in a consistent way, where each row means the same thing and each column has a defined interpretation.\nConceptually, a dataset answers the question:\nWhat observations do we have, and what do we know about each one?\n\n\n\n1.2 Schema and structure\nA dataset is more than just values arranged in rows and columns. It also has a schema, which defines the structure and meaning of the data.\nA schema specifies:\n- what columns exist,\n- what each column represents,\n- and what type of data each column contains.\nFor example, a column might represent numeric values, categorical labels, dates, or text. These distinctions matter because different operations are valid for different types of data. Numeric columns can be averaged or summed, while text columns cannot. Boolean columns encode yes/no logic, while categorical columns group observations into meaningful categories.\nStructure is what allows computers to process data reliably. When the structure is clear and consistent, programs can apply the same operations across all rows without ambiguity. When structure is unclear or inconsistent, errors become more likely and results become harder to interpret.\nIn analytics and AI workflows, much of the effort is spent not on modeling itself, but on ensuring that data conforms to an expected schema. Understanding schema early helps explain why later steps—such as cleaning, renaming columns, or converting data types—are necessary parts of the process rather than optional refinements.\n\n\n\n1.3 Structured vs unstructured data\nNot all data is organized in neat tables. It is useful to distinguish between structured and unstructured data.\nStructured data follows a consistent format, with clearly defined rows, columns, and data types. Examples include spreadsheets, CSV files, and database tables. This kind of data is well suited for tools like Pandas, which are designed to operate on tabular structures.\nUnstructured data, by contrast, does not naturally fit into a fixed table. Examples include free-form text, images, audio recordings, and video. While these data types are extremely important in modern AI systems, they require different representations and tools before they can be analyzed in the same way as structured data.\nMany AI workflows begin by transforming unstructured data into structured form. For example, text may be converted into counts, embeddings, or labels; images may be converted into feature vectors. Once data is structured, it can be stored in datasets and processed using familiar analytical tools.\nPandas focuses on structured data because structure enables computation. Tables provide a shared language between humans and machines, allowing data to be summarized, filtered, compared, and modeled systematically.\nUnderstanding the distinction between structured and unstructured data clarifies why datasets are so central to analytics and AI. They are the bridge between raw observations and computational reasoning."
  },
  {
    "objectID": "textbook_src/week03_main.html#getting-started-with-pandas",
    "href": "textbook_src/week03_main.html#getting-started-with-pandas",
    "title": "Week 3 – Working with Data: Pandas, Files, and APIs",
    "section": "2. Getting Started with Pandas",
    "text": "2. Getting Started with Pandas\nAs datasets grow in size and complexity, basic Python data structures such as lists and dictionaries begin to show their limitations. While these structures are essential building blocks, they are not designed to efficiently represent or manipulate large, tabular datasets. Pandas was created to fill this gap.\nThis section introduces Pandas as a foundational tool for working with structured data and explains the abstractions it provides for analytics and AI workflows.\n\n\n2.1 What Pandas is and why it exists\nLists and dictionaries are flexible and powerful, but they are not well suited for representing tables of data. Lists organize values by position, and dictionaries organize values by keys, but neither naturally represents a dataset with many rows and many columns where operations need to be applied consistently across variables.\nFor example, storing each column of a dataset as a separate list quickly becomes difficult to manage. Ensuring that all lists stay aligned, handling missing values, and performing column-wise operations requires substantial manual effort and careful bookkeeping.\nPandas is a data analysis library designed specifically to address these challenges. It provides data structures and functions that make it easier to load, inspect, clean, transform, and summarize structured data. Rather than working with individual values or small collections, Pandas allows programs to operate directly on entire datasets.\nAt the center of Pandas is the DataFrame, which represents a dataset as a table with labeled columns and indexed rows. This abstraction closely mirrors how analysts and decision-makers think about data, making code more readable and reducing the cognitive gap between analysis intent and implementation.\nConceptually, Pandas exists to answer the question:\nHow can I work with datasets as datasets, rather than as scattered collections of Python objects?\n\n\n\n2.2 DataFrames and Series\nA DataFrame is Pandas’ primary data structure. It represents data in a two-dimensional, tabular form, with rows and columns. Each column has a name (the column label), and each row has an index that identifies it.\nWithin a DataFrame, each column is represented as a Series. A Series is a one-dimensional array of values with an associated index. While DataFrames represent entire datasets, Series represent individual variables within those datasets.\nThe distinction is important: - A DataFrame represents the whole table. - A Series represents a single column from that table.\nThis structure allows Pandas to apply operations across columns, across rows, or to individual variables in a consistent way. For example, summary statistics can be computed column by column, and filters can be applied row by row.\nThe following example illustrates the creation of a simple DataFrame with two columns:\nimport pandas as pd\ndf = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\nIn this example:\n- The DataFrame has two columns, labeled a and b.\n- Each column is a Series containing numeric values.\n- Each row represents a single observation.\nAlthough this example is small, the same structure scales to datasets with thousands or millions of rows. The consistent organization of rows and columns is what enables Pandas to support efficient data manipulation and analysis.\nUnderstanding DataFrames and Series establishes the mental model needed for the rest of the week. Nearly all subsequent operations—loading files, inspecting data, cleaning values, filtering rows, and computing summaries—operate on these two abstractions."
  },
  {
    "objectID": "textbook_src/week03_main.html#files-and-paths-in-data-workflows",
    "href": "textbook_src/week03_main.html#files-and-paths-in-data-workflows",
    "title": "Week 3 – Working with Data: Pandas, Files, and APIs",
    "section": "3. Files and Paths in Data Workflows",
    "text": "3. Files and Paths in Data Workflows\nAs you begin working with real datasets, managing how and where data is stored becomes a critical part of the workflow. Data used in analytics and AI is almost always stored in external files rather than embedded directly in code. Understanding how Python locates and accesses these files is essential for building reliable and reproducible data workflows.\nThis section focuses on why file paths matter, how they connect scripts to data, and how to diagnose common file access problems.\n\n\n3.1 Why file paths matter for data\nIn most data workflows, Python scripts and data files are separate. A script contains instructions for what to do, while data files contain the information to be processed. File paths are what connect the two.\nA file path is a description of where a file lives on a computer. When a script loads a dataset, it uses a file path to tell Python where to find that file. If the path is incorrect, the script cannot access the data, regardless of whether the file exists somewhere else on the system.\nThis separation between code and data is intentional. It allows:\n- the same code to be reused with different datasets,\n- data to be updated without changing code,\n- projects to be organized into clear directory structures.\nCorrectly managing file paths is therefore not just a technical detail; it is part of designing a clean and maintainable data workflow.\n\n\n\n3.2 Relative vs absolute paths (revisited)\nAs discussed earlier in the course, file paths can be either absolute or relative.\nAn absolute path specifies the full location of a file starting from the root of the file system. Absolute paths are precise, but they are tied to a specific machine and directory layout. This makes them fragile when projects are moved or shared.\nA relative path specifies a file’s location relative to the current working directory of the script. Relative paths are preferred in most projects because they make code portable. As long as the internal project structure remains the same, relative paths continue to work across machines and environments.\nIn data workflows, relative paths are especially important because datasets are often stored in project subfolders such as data/. A typical pattern is to keep data files separate from scripts, but within the same project directory.\ndata_path = \"data/sample.csv\"\nThis example assumes that the script is being run from the project’s root directory and that the dataset is located in a folder named data. Using relative paths in this way allows the entire project to be moved or shared without modifying file references.\nThese ideas directly connect back to the file path concepts introduced earlier in the course. The same reasoning about working directories and relative locations applies when loading data with Pandas.\n\n\n\n3.3 Common file path errors\nFile path issues are among the most common sources of errors in data workflows. These errors are predictable and usually easy to diagnose once you know what to look for.\nOne frequent error is file not found. This occurs when Python cannot locate the file at the specified path. Common causes include misspelled file names, incorrect extensions, or incorrect assumptions about where the file is stored.\nAnother common issue is running a script from the wrong working directory. Relative paths are interpreted based on the directory from which the script is executed, not the location of the script file itself. If the working directory is not what you expect, relative paths may fail even if the file exists.\nWhen debugging file access issues, it is often helpful to inspect the current working directory and list the files Python can see:\nimport os\nprint(os.getcwd())\nprint(os.listdir())\nThese checks help confirm whether the script is looking in the correct place and whether the expected files are present.\nA useful debugging strategy is to temporarily use an absolute path to confirm that the file can be loaded at all. Once the issue is resolved, the path can be converted back to a relative one to restore portability.\nUnderstanding and resolving file path errors reinforces an important lesson: many data-loading problems are not caused by Pandas or Python itself, but by mismatches between assumptions about directory structure and the actual execution context."
  },
  {
    "objectID": "textbook_src/week03_main.html#loading-data-from-csv-files",
    "href": "textbook_src/week03_main.html#loading-data-from-csv-files",
    "title": "Week 3 – Working with Data: Pandas, Files, and APIs",
    "section": "4. Loading Data from CSV Files",
    "text": "4. Loading Data from CSV Files\nOne of the most common ways datasets are stored and shared is through CSV files. CSV files are simple, flexible, and widely supported, which makes them a standard format in analytics workflows. This section explains what CSV files represent and how to load them into Python using Pandas.\n\n\n4.1 CSV files as datasets\nA CSV file (Comma-Separated Values) represents a dataset in a plain-text format. Each line in the file corresponds to a row in the dataset, and values within a row are separated by commas. Typically, the first row contains column names that describe the variables in the dataset.\nConceptually, a CSV file maps directly to the idea of a tabular dataset:\n\neach row represents an observation,\n\neach column represents a variable,\n\nand each cell contains a single value.\n\nBecause CSV files are plain text, they are easy to create, inspect, and share. They can be opened in spreadsheet software, text editors, and programming environments without requiring specialized tools.\nHowever, CSV files also have limitations. They do not explicitly store data types, constraints, or relationships between columns. Everything in a CSV file is initially read as text, and structure must be inferred by the software that loads it. This is why inspection and cleaning steps are so important after loading data.\nDespite these limitations, CSV files remain a foundational format for analytics because they strike a balance between simplicity and usefulness.\n\n\n\n4.2 Reading CSV files with Pandas\nPandas provides a dedicated function for loading CSV files into a DataFrame: read_csv. This function reads the contents of a CSV file and constructs a DataFrame where:\n- rows correspond to records,\n- columns correspond to variables,\n- and column labels are inferred from the header row.\ndf = pd.read_csv(\"data/sample.csv\")\nIn this example, the CSV file located at data/sample.csv is read into a DataFrame named df. From this point forward, the dataset can be manipulated using Pandas operations rather than low-level file handling.\nAssigning the result of read_csv to a variable is essential. The DataFrame becomes the central object through which all subsequent inspection, cleaning, transformation, and analysis steps are performed.\nAlthough read_csv has many optional parameters, the default behavior is sufficient for many well-formed datasets. Additional options can be introduced later as data complexity increases.\n\n\n\n4.3 Verifying successful data loading\nAfter loading a dataset, it is important to verify that the data was read correctly. This step helps catch issues early, before errors propagate through later analysis.\nTwo simple checks are especially useful. First, examining the shape of the DataFrame confirms the number of rows and columns:\ndf.shape\nThe shape provides a quick sanity check. If the number of rows or columns is unexpected, it may indicate a problem with the file path, the delimiter, or the structure of the CSV file.\nSecond, inspecting the column names helps verify that variables were read correctly:\ndf.columns\nThis allows you to confirm that column labels match expectations and that no unexpected formatting issues occurred.\nAt this stage, the goal is not to deeply analyze the data, but to establish confidence that the dataset is present, structured, and ready for further inspection. Verifying successful data loading is a small step that prevents much larger problems later in the workflow."
  },
  {
    "objectID": "textbook_src/week03_main.html#data-cleaning-and-exploration",
    "href": "textbook_src/week03_main.html#data-cleaning-and-exploration",
    "title": "Week 3 – Working with Data: Pandas, Files, and APIs",
    "section": "5. Data Cleaning and Exploration",
    "text": "5. Data Cleaning and Exploration\nOnce a dataset has been loaded into a DataFrame, the next step is to understand what you are working with. Data cleaning and exploration begin not by changing anything, but by inspecting structure, contents, and basic properties of the data.\nThis section introduces a small set of inspection tools that provide high-value information early in the workflow. These tools help establish expectations, reveal potential problems, and guide subsequent cleaning and transformation steps.\n\n\n5.1 Inspecting data structure\nThe first task after loading a dataset is to examine its overall structure. This involves looking at both the data itself and the metadata Pandas has inferred about it.\nOne common starting point is to view the first few rows of the dataset. This provides a quick sense of what each column represents and how values are formatted.\ndf.head()\nThe output of head() shows the first rows of the DataFrame, including column names and sample values. This makes it easier to spot obvious issues such as unexpected column names, misaligned values, or formatting problems.\nAnother essential inspection step is examining the data types and completeness of each column.\ndf.info()\nThe info() method provides a summary of the DataFrame, including:\n- the number of rows,\n- the names of columns,\n- the data type inferred for each column,\n- and the count of non-missing values.\nThis information is critical for understanding how Pandas interprets the dataset. For example, a column intended to represent numbers may be interpreted as text, or a column may contain fewer non-null values than expected.\nAt this stage, the goal is not to fix problems, but to identify them. Inspection establishes a baseline understanding of the dataset before any modifications are made.\n\n\n\n5.2 Descriptive summaries\nAfter inspecting structure and types, it is useful to examine summary statistics. Descriptive summaries condense large amounts of data into a small number of informative metrics.\nPandas provides the describe() method for this purpose.\ndf.describe()\nFor numeric columns, describe() typically reports: - count, - mean, - standard deviation, - minimum and maximum values, - and key percentiles.\nThese summaries help reveal the distribution and scale of the data. Extremely large or small values, unexpected ranges, or missing observations can often be identified at this stage.\nBy default, describe() focuses on numeric data. Non-numeric columns, such as strings or categorical labels, require different inspection strategies, which will be introduced later. This distinction reinforces an important idea: different types of data require different forms of analysis.\nDescriptive summaries do not provide answers on their own, but they guide reasoning. They help determine whether values look reasonable, whether further cleaning is required, and which variables may be relevant for analysis.\nTogether, inspection and descriptive summaries form the foundation of data cleaning. Before transforming or modeling data, it is essential to know what the data contains, how it is structured, and where potential issues may lie."
  },
  {
    "objectID": "textbook_src/week03_main.html#handling-missing-values",
    "href": "textbook_src/week03_main.html#handling-missing-values",
    "title": "Week 3 – Working with Data: Pandas, Files, and APIs",
    "section": "6. Handling Missing Values",
    "text": "6. Handling Missing Values\nMissing data is a common and unavoidable feature of real-world datasets. Values may be absent for many reasons, and how missing data is handled can significantly influence analytical results and model behavior. This section introduces missing values as a concept, shows how to detect them, and outlines simple, practical strategies for dealing with them.\nThe emphasis here is not on finding a single “correct” solution, but on understanding the tradeoffs involved in different approaches.\n\n\n6.1 What missing data represents\nA missing value indicates that a data point is absent where a value is expected. In Pandas, missing values are typically represented using special markers that indicate the absence of data rather than a meaningful value.\nMissing data can occur for many reasons:\n- information was not collected,\n- a measurement failed or was skipped,\n- data was lost during transfer or processing,\n- a value was not applicable in a particular context.\nImportantly, missing values are not the same as zero, empty strings, or false values. They represent unknown or unavailable information, and treating them as ordinary values can lead to incorrect conclusions.\nMissing data has important implications for analytics and AI systems. Many statistical operations and models assume complete data, and missing values can cause calculations to fail or produce misleading results. For example, averages may be skewed, relationships may appear weaker or stronger than they truly are, and models may learn patterns based on incomplete information.\nUnderstanding what missing data represents is the first step toward deciding how to handle it responsibly.\n\n\n\n6.2 Detecting missing values\nBefore missing data can be addressed, it must be identified. Pandas provides tools to detect and summarize missing values across a dataset.\nA common approach is to check which values are missing and count how many missing values appear in each column.\ndf.isna().sum()\nThe isna() method returns a DataFrame of boolean values indicating whether each entry is missing. When combined with sum(), it produces a count of missing values for each column.\nThis summary helps answer key questions:\n- Which columns contain missing values?\n- How many values are missing in each column?\n- Are missing values concentrated in specific variables or spread throughout the dataset?\nDetecting missing values early allows informed decisions about whether data cleaning is required and which variables may need special attention.\n\n\n\n6.3 Simple strategies for handling missing data\nOnce missing values have been identified, several basic strategies can be used to handle them. Each approach has advantages and disadvantages, and the appropriate choice depends on the context and goals of the analysis.\nOne simple strategy is dropping rows or columns that contain missing values. This approach is straightforward, but it can result in the loss of potentially valuable data, especially if missing values are common.\nAnother approach is filling missing values with a substitute value. For numeric data, this might involve using a constant, an average, or another summary statistic. For categorical data, a placeholder value may be used. Filling allows the dataset to remain complete, but it introduces assumptions about what the missing values should represent.\nBoth strategies involve tradeoffs:\n- Dropping data reduces sample size but avoids introducing assumptions.\n- Filling data preserves sample size but may distort distributions or relationships.\nAt this stage, the goal is not to apply advanced imputation techniques, but to develop an awareness of how missing data affects analysis and why handling it requires deliberate choice. Simple strategies provide a starting point and help illustrate the consequences of different decisions.\nHandling missing values is an essential step in preparing data for analysis and modeling. Thoughtful treatment of missing data improves the reliability and interpretability of results and lays the groundwork for more advanced techniques later in the course."
  },
  {
    "objectID": "textbook_src/week03_main.html#renaming-and-dropping-columns",
    "href": "textbook_src/week03_main.html#renaming-and-dropping-columns",
    "title": "Week 3 – Working with Data: Pandas, Files, and APIs",
    "section": "7. Renaming and Dropping Columns",
    "text": "7. Renaming and Dropping Columns\nAfter inspecting a dataset and addressing missing values, a common next step is to clean up the columns themselves. Column names and column selection play a central role in how readable, interpretable, and usable a dataset is. This section focuses on improving dataset clarity by renaming columns and removing those that are unnecessary.\n\n\n7.1 Why column names matter\nColumn names are not just labels; they are part of the dataset’s schema. They communicate what each variable represents and how it should be interpreted. Clear, consistent column names make data easier to understand, easier to analyze, and less error-prone to work with.\nPoorly chosen column names can introduce confusion. Names may be too vague, too long, inconsistently formatted, or reflect internal system conventions rather than analytical meaning. For example, column names inherited from raw data sources may include abbreviations, spaces, or special characters that make code harder to read and write.\nImproving column names serves several purposes:\n- It increases readability for humans.\n- It reduces the likelihood of mistakes when referencing columns in code.\n- It clarifies the intended meaning of each variable.\nBecause column names are used repeatedly throughout an analysis, treating them as part of the schema—and cleaning them early—pays dividends later in the workflow.\n\n\n\n7.2 Renaming columns\nRenaming columns is a common data-cleaning task. Pandas allows columns to be renamed by providing a mapping from old names to new names. This approach supports incremental cleanup, where only problematic columns are renamed rather than rewriting the entire schema at once.\ndf.rename(columns={\"old\": \"new\"}, inplace=True)\nIn this example, the column originally named \"old\" is renamed to \"new\". Other columns remain unchanged. This targeted approach makes it easier to track changes and reduces the risk of unintended consequences.\nRenaming columns is often used to:\n- replace cryptic or abbreviated names with descriptive ones,\n- standardize capitalization or naming conventions,\n- remove spaces or special characters,\n- align column names with analytical concepts rather than source-system terminology.\nPerforming renaming early in the analysis ensures that subsequent code is easier to read and that variable references are consistent throughout the project.\n\n\n\n7.3 Dropping columns\nNot all columns in a dataset are useful for every analysis. Some columns may be redundant, irrelevant, or simply not needed for the current task. Dropping columns reduces dataset complexity and helps focus attention on the variables that matter.\ndf.drop(columns=[\"unused\"], inplace=True)\nIn this example, the column named \"unused\" is removed from the DataFrame. Dropping unnecessary columns can: - reduce memory usage,\n- simplify inspection and analysis,\n- make code easier to understand,\n- and reduce the risk of accidentally using irrelevant variables.\nDeciding which columns to drop is a substantive analytical decision. Removing data too aggressively can eliminate useful information, while keeping too many columns can obscure important patterns. The goal is to strike a balance between completeness and clarity.\nRenaming and dropping columns are small operations individually, but together they play a crucial role in shaping a dataset that is well-structured, interpretable, and ready for further transformation and analysis."
  },
  {
    "objectID": "textbook_src/week03_main.html#data-transformation-filtering-and-selecting-data",
    "href": "textbook_src/week03_main.html#data-transformation-filtering-and-selecting-data",
    "title": "Week 3 – Working with Data: Pandas, Files, and APIs",
    "section": "8. Data Transformation: Filtering and Selecting Data",
    "text": "8. Data Transformation: Filtering and Selecting Data\nOnce a dataset has been loaded, inspected, and cleaned, the next step is often to focus on the parts of the data that matter for a specific question. Data transformation involves selecting relevant variables, filtering observations, and reshaping datasets to support analysis.\nThis section introduces basic selection and filtering techniques in Pandas, emphasizing how these operations help turn raw datasets into analytically useful subsets.\n\n\n8.1 Selecting columns\nSelecting columns allows you to focus on a subset of variables within a dataset. Rather than working with every column at once, column selection narrows attention to the variables that are relevant for a particular analysis.\nColumn selection matters for several reasons:\n- It improves readability by reducing clutter.\n- It makes code more explicit about which variables are being used.\n- It reduces the chance of accidentally incorporating irrelevant data.\nIn Pandas, selecting columns produces a new DataFrame or Series that contains only the specified variables. This operation does not change the original dataset unless explicitly assigned back to it.\nConceptually, selecting columns answers the question:\nWhich variables from this dataset are relevant right now?\nFocusing on relevant columns is an important analytical habit. It encourages intentional use of data rather than treating all available variables as equally important.\n\n\n\n8.2 Filtering rows with conditions\nFiltering rows allows you to select observations that meet specific criteria. Instead of analyzing all rows in a dataset, filtering narrows the dataset to those records that satisfy a condition.\nIn Pandas, row filtering is typically done using boolean masks. A boolean mask is a sequence of True and False values that indicates whether each row meets a condition.\nfiltered = df[df[\"score\"] &gt; 80]\nIn this example, the condition df[\"score\"] &gt; 80 produces a boolean mask. Pandas uses this mask to keep only the rows where the condition is true.\nFiltering rows is a powerful way to explore subsets of data, such as:\n- high-performing observations,\n- records from a specific category,\n- or cases that meet defined thresholds.\nConceptually, filtering answers the question:\nWhich observations should be included in this analysis?\nBecause filtering is based on conditions, it directly connects to the conditional logic introduced earlier in the course.\n\n\n\n8.3 Using .loc and .iloc\nPandas provides two explicit indexing tools for selecting data: .loc and .iloc. These tools clarify whether selection is based on labels or positions.\n\n.loc is used for label-based selection. It selects rows and columns using index labels and column names.\n.iloc is used for position-based selection. It selects rows and columns using integer positions, similar to list indexing.\n\nUnderstanding the distinction between .loc and .iloc is important because it makes code more precise and easier to read. When selection is label-based, .loc communicates intent clearly. When selection depends on position, .iloc makes that dependence explicit.\nChoosing between .loc and .iloc depends on context:\n- Use .loc when working with meaningful labels.\n- Use .iloc when selection depends on row or column position.\nAlthough basic filtering and selection can be done without these methods, .loc and .iloc become increasingly important as datasets grow more complex and analyses become more detailed."
  },
  {
    "objectID": "textbook_src/week03_main.html#descriptive-statistics-with-pandas",
    "href": "textbook_src/week03_main.html#descriptive-statistics-with-pandas",
    "title": "Week 3 – Working with Data: Pandas, Files, and APIs",
    "section": "9. Descriptive Statistics with Pandas",
    "text": "9. Descriptive Statistics with Pandas\nAfter data has been cleaned, filtered, and transformed, the next step is often to summarize what the data contains. Descriptive statistics provide a way to move from raw rows and columns to interpretable information that supports reasoning and decision-making.\nThis section introduces descriptive statistics as sense-making tools rather than final answers. The goal is to understand what the data looks like, not to explain why patterns exist or to make predictions.\n\n\n9.1 Why summarization matters\nRaw datasets can be large and difficult to interpret directly. Even after filtering and cleaning, looking at individual rows rarely provides a clear picture of overall patterns or tendencies.\nSummarization condenses many observations into a small number of meaningful quantities. These summaries help answer questions such as:\n- What is typical in this dataset?\n- How much variation exists?\n- Are values generally large or small?\n- Are there obvious extremes or anomalies?\nDescriptive statistics are often the first step in turning data into insight. They provide context and grounding before more advanced analysis is attempted. In analytics and AI workflows, descriptive summaries help analysts understand what the model will see and what assumptions may be reasonable.\nAt a conceptual level, summarization answers the question:\nWhat does this dataset look like as a whole?\n\n\n\n9.2 Common summary statistics\nPandas provides convenient methods for computing common descriptive statistics. These statistics describe central tendency, spread, and range.\nSome of the most frequently used summary statistics include:\n- count, the number of non-missing values,\n- mean, the average value,\n- median, the middle value,\n- minimum and maximum, which describe the range of values.\nFor example, computing the average of a numeric column can be done directly:\ndf[\"score\"].mean()\nThese operations aggregate information across all rows in a column, producing a single value that summarizes the data. Similar methods exist for other statistics, and many can be applied column by column across an entire DataFrame.\nSummary statistics are especially useful for identifying potential issues: - unexpected ranges,\n- unusually large or small values,\n- or discrepancies between measures such as mean and median.\nWhile these statistics are simple, they play an essential role in exploratory analysis. They help determine whether further cleaning is needed and guide decisions about what analyses are appropriate.\n\n\n\n9.3 Group-level summaries (conceptual preview)\nIn many datasets, observations belong to meaningful groups or categories. For example, data may be grouped by region, category, or time period. Summarizing data across the entire dataset can obscure important differences between groups.\nGroup-level summaries address this by computing descriptive statistics within categories rather than across all observations at once. This allows comparisons such as:\n- average scores by group,\n- counts by category,\n- or ranges within subpopulations.\nAlthough the mechanics of grouping data will be introduced later, it is important to recognize the conceptual role of group-level summaries. They allow analysts to move from “overall” descriptions to more nuanced views that reveal structure within the data.\nGroup-level summaries prepare the ground for deeper analysis, including comparisons, modeling, and evaluation. They represent a natural progression from understanding individual variables to understanding relationships between variables and categories.\nAt this stage, the key idea is simple: how data is grouped affects what summaries reveal. Being intentional about grouping is as important as choosing which statistics to compute."
  },
  {
    "objectID": "textbook_src/week03_main.html#json-and-apis",
    "href": "textbook_src/week03_main.html#json-and-apis",
    "title": "Week 3 – Working with Data: Pandas, Files, and APIs",
    "section": "10. JSON and APIs",
    "text": "10. JSON and APIs\nAs analytics and AI systems increasingly rely on data from external services, it becomes important to understand formats beyond CSV and tables. One of the most common formats used to exchange data between systems is JSON. This section introduces JSON as a structured data format and explains how it differs from the tabular data used earlier in the week.\n\n\n10.1 What JSON is\nJSON (JavaScript Object Notation) is a text-based format used to represent structured data. Despite its name, JSON is language-agnostic and is widely used across programming languages and platforms.\nJSON represents data using key–value pairs, much like Python dictionaries. Values associated with keys can themselves be other objects, lists, or simple values such as numbers, strings, or booleans. This allows JSON to represent complex, nested structures.\nConceptually, JSON encodes data as a hierarchy rather than a table. Instead of rows and columns, JSON organizes information into objects that contain other objects or lists. This makes it well suited for representing entities with varying attributes or nested relationships.\nJSON is common in AI systems because it is:\n- human-readable,\n- easy for machines to parse,\n- flexible in structure,\n- and well suited for transmitting data over networks.\nAPIs frequently return data in JSON format because it allows systems to exchange structured information without requiring a fixed schema in advance. This flexibility is valuable in environments where data evolves over time or where different consumers may need different parts of the data.\n\n\n\n10.2 JSON vs tabular data\nAlthough JSON and tabular data both represent structured information, they differ in how that structure is expressed.\nTabular data is flat. Each row represents an observation, and each column represents a variable. This structure is ideal for many forms of analysis, especially when observations are uniform and relationships are simple.\nJSON data, by contrast, is hierarchical. Objects can contain nested objects or lists, and not every object must have the same keys. This allows JSON to represent more complex relationships, but it also makes direct analysis more challenging.\nMapping JSON to tables often requires flattening the structure. Nested objects may be expanded into columns, and lists may be transformed into multiple rows. This transformation step is common in data pipelines that ingest API data and prepare it for analysis with tools like Pandas.\nUnderstanding the differences between JSON and tabular data clarifies why different tools are used at different stages of analytics and AI workflows. JSON is well suited for data exchange and representation of complex entities, while tabular data is well suited for computation, summarization, and modeling.\nAt a high level, JSON answers the question:\nHow can complex, structured information be exchanged between systems?\nTabular data answers a different question:\nHow can structured information be analyzed efficiently?"
  },
  {
    "objectID": "textbook_src/week03_main.html#calling-an-api-with-python",
    "href": "textbook_src/week03_main.html#calling-an-api-with-python",
    "title": "Week 3 – Working with Data: Pandas, Files, and APIs",
    "section": "11. Calling an API with Python",
    "text": "11. Calling an API with Python\nMany modern datasets are not stored as static files. Instead, they are accessed dynamically through APIs. APIs allow programs to request data from external systems on demand, making them a central component of contemporary analytics and AI pipelines.\nThis section introduces APIs conceptually and demonstrates how Python can be used to retrieve and inspect data returned in JSON format.\n\n\n11.1 What an API is\nAn API (Application Programming Interface) is a structured way for one system to request information or services from another system. In data workflows, APIs are most often used as data access mechanisms.\nRather than downloading a file manually, a program sends a request to an API endpoint and receives a response containing data. This interaction follows a predictable pattern:\n- the client sends a request,\n- the server processes it,\n- the server returns a response.\nThe response typically includes both the requested data and metadata about the request itself, such as whether it was successful.\nAPIs are widely used because they:\n- allow real-time or near-real-time access to data,\n- enable controlled and authenticated access,\n- support integration across systems and platforms.\nIn analytics and AI contexts, APIs are commonly used to retrieve data from web services, cloud platforms, and internal systems. Understanding APIs therefore expands the range of data sources that Python programs can interact with.\n\n\n\n11.2 Making a request with Python\nPython provides several libraries for working with APIs. One of the most commonly used is the requests library, which simplifies sending HTTP requests and handling responses.\nThe basic workflow for making an API request involves:\n1. importing the requests library,\n2. sending a request to a URL,\n3. storing the response for further inspection.\nimport requests\nresponse = requests.get(\"https://api.example.com/data\")\nIn this example, a GET request is sent to the specified URL. The result is stored in a variable named response. At this stage, no assumptions are made about the content of the response; it is treated as an object that contains information returned by the server.\nHandling responses responsibly involves checking whether the request succeeded and understanding what kind of data was returned. Although error handling and authentication are not covered here, it is important to recognize that API requests can fail for many reasons, including network issues, invalid endpoints, or access restrictions.\nThe key idea is that APIs allow programs to retrieve data programmatically rather than manually.\n\n\n\n11.3 Parsing JSON responses\nMost APIs return data in JSON format. Once a response has been received, the next step is to convert that JSON data into Python objects that can be inspected and manipulated.\nThe requests library provides a method for this purpose:\ndata = response.json()\nCalling json() on the response parses the JSON text and converts it into native Python data structures, typically dictionaries and lists. This transformation allows the data to be explored using familiar Python tools.\nAt this point, inspection becomes important. API responses often contain nested structures, metadata, or multiple layers of information. Examining the structure of the returned object helps determine which parts of the data are relevant and how they might be transformed into tabular form.\nParsing JSON responses reinforces a recurring theme from this week: data often arrives in one structure and must be transformed into another before analysis can occur. APIs provide access to rich data sources, but turning that data into usable datasets requires careful inspection and preparation."
  },
  {
    "objectID": "textbook_src/week03_main.html#mini-lab-end-to-end-data-cleaning-and-summary",
    "href": "textbook_src/week03_main.html#mini-lab-end-to-end-data-cleaning-and-summary",
    "title": "Week 3 – Working with Data: Pandas, Files, and APIs",
    "section": "12. Mini-Lab: End-to-End Data Cleaning and Summary",
    "text": "12. Mini-Lab: End-to-End Data Cleaning and Summary\nThis mini-lab brings together the concepts and skills developed throughout Week 3 into a single, coherent workflow. Rather than introducing new techniques, the goal is to practice moving from raw data to interpretable summaries using a structured and deliberate process.\nThe emphasis is on reasoning about data at each step: what the data looks like, what issues it contains, what choices are made during cleaning, and how those choices affect the final results.\n\n\n12.1 Importing a dataset\nThe workflow begins by importing a dataset from a CSV file into a Pandas DataFrame. This step establishes the dataset as the central object for the remainder of the analysis.\nimport pandas as pd\ndf = pd.read_csv(\"data/sample.csv\")\nAt this point, the focus is simply on getting the data into the environment successfully. No assumptions are made yet about data quality or suitability for analysis. Importing the dataset creates the foundation on which all subsequent steps depend.\n\n\n\n12.2 Inspecting and cleaning data\nOnce the dataset is loaded, the next step is to inspect its structure and identify potential issues. Inspection helps determine whether the dataset aligns with expectations and where cleaning may be required.\ndf.head()\ndf.info()\nThese inspection steps reveal column names, data types, and the presence of missing values. Based on this information, cleaning decisions can be made.\nHandling missing values is often a necessary part of preparing data for analysis. Depending on the context, missing values may be dropped or filled. Similarly, column names may be renamed for clarity, and columns that are not relevant to the analysis may be removed.\ndf = df.dropna()\ndf.rename(columns={\"old_name\": \"new_name\"}, inplace=True)\ndf.drop(columns=[\"unused_column\"], inplace=True)\nThe goal of cleaning is not perfection, but fitness for purpose. Each cleaning step reflects a choice about what information is important and how it should be represented.\n\n\n\n12.3 Filtering and summarizing\nWith a cleaned dataset, the next step is to focus on the subset of data relevant to the analytical question. This often involves filtering rows based on conditions and selecting specific columns for analysis.\nfiltered = df[df[\"score\"] &gt; 80]\nFiltering narrows the dataset to observations that meet defined criteria. From there, descriptive statistics can be computed to summarize the filtered data.\nfiltered[\"score\"].mean()\nfiltered.describe()\nThese summaries condense the dataset into interpretable metrics that describe central tendency, variation, and range. The results provide a high-level view of the data after cleaning and filtering.\n\n\n\n12.4 Interpreting results\nThe final step of the workflow is interpretation. Descriptive statistics do not speak for themselves; they must be interpreted in light of how the data was collected, cleaned, and filtered.\nKey questions to consider include:\n- What do the summaries reveal about the dataset?\n- How did cleaning decisions affect the results?\n- Are there patterns or anomalies that warrant further investigation?\n- What limitations remain due to missing data or assumptions made during cleaning?\nInterpreting results connects technical steps back to analytical reasoning. It reinforces the idea that analytics is not just about computation, but about making informed judgments based on data quality, structure, and context.\nThis mini-lab demonstrates a complete, end-to-end data workflow: importing data, inspecting structure, cleaning issues, transforming subsets, summarizing results, and interpreting what those results mean. This pattern will reappear throughout the course as datasets become larger and analyses become more sophisticated."
  },
  {
    "objectID": "textbook_src/week03_main.html#week-3-summary",
    "href": "textbook_src/week03_main.html#week-3-summary",
    "title": "Week 3 – Working with Data: Pandas, Files, and APIs",
    "section": "Week 3 Summary",
    "text": "Week 3 Summary"
  },
  {
    "objectID": "textbook_src/week03_main.html#week-3-summary-1",
    "href": "textbook_src/week03_main.html#week-3-summary-1",
    "title": "Week 3 – Working with Data: Pandas, Files, and APIs",
    "section": "Week 3 Summary",
    "text": "Week 3 Summary\nThis chapter focused on working with data as an object of analysis, rather than as isolated values or small collections. The emphasis shifted from writing Python logic to understanding how real-world data is structured, accessed, cleaned, transformed, and summarized in preparation for analytics and AI tasks.\nConceptually, the chapter introduced datasets as structured collections of observations governed by a schema. Understanding rows as records and columns as variables provided a foundation for reasoning about data quality, consistency, and meaning. The distinction between structured and unstructured data clarified why tools like Pandas are central to analytics workflows and how unstructured data is often transformed before analysis.\nPandas was introduced as a way to treat datasets as first-class objects through DataFrames and Series. This abstraction made it possible to load data from external files, inspect structure and types, handle missing values, and clean schemas in a systematic way. Rather than viewing data cleaning as a peripheral task, the chapter emphasized it as a core part of responsible analysis.\nThe chapter also highlighted how data location and access shape workflows. File paths, relative directories, and APIs were treated as integral components of data pipelines rather than technical afterthoughts. Understanding how data is retrieved—whether from CSV files or through APIs returning JSON—expanded the range of data sources that Python programs can work with.\nThrough filtering, selection, and descriptive statistics, the chapter demonstrated how raw datasets are transformed into interpretable summaries. These summaries do not provide final answers, but they support sense-making, guide further analysis, and surface potential data quality issues. The mini-lab reinforced this end-to-end workflow by integrating import, inspection, cleaning, transformation, and interpretation into a single analytical process.\nBy the end of Week 3, you should be able to move confidently from raw data to cleaned, summarized information, while understanding how each step affects the results. These skills form the foundation for the next stage of the course, where datasets are no longer just described, but used to support modeling, prediction, and decision-making."
  },
  {
    "objectID": "textbook_src/week01_main.html",
    "href": "textbook_src/week01_main.html",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "",
    "text": "What “analytics” means, and the four common types: descriptive, diagnostic, predictive, and prescriptive.\nWhat “AI” means in practice—and why it often involves judgment, uncertainty, and automation.\nHow analytics and AI overlap, and how they differ based on how results are used in decision-making.\nA simple way to break down AI systems into data, models, and logic.\nThe major AI approaches (symbolic, machine learning, deep learning) and what each is good at.\nHow data gets into real systems, and why pipelines matter for decision-making.\n\n\n\n\n\nA working course project folder with a .venv created via uv\nSeveral runnable Python scripts (.py) you can execute from the terminal\n\n\n\n\nBy the end of this week, you should be able to:\n\nExplain what analytics is and how it differs from (and overlaps with) artificial intelligence.\nDescribe AI systems using the Data + Models + Logic framework.\nIdentify the major AI paradigms (symbolic, statistical, neural) and explain how they differ conceptually.\nExplain how data pipelines support decision-making in modern analytics and AI systems.\n\n\n\n\nBy the end of this week, you should be able to:\n\nRun a Python script (.py) from the terminal.\nNavigate folders and files using basic shell commands.\nUnderstand the role of virtual environments in Python workflows.\nCreate and activate a virtual environment using uv.\nVerify that your local Python environment is correctly configured for the course."
  },
  {
    "objectID": "textbook_src/week01_main.html#week-1-overview",
    "href": "textbook_src/week01_main.html#week-1-overview",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "",
    "text": "What “analytics” means, and the four common types: descriptive, diagnostic, predictive, and prescriptive.\nWhat “AI” means in practice—and why it often involves judgment, uncertainty, and automation.\nHow analytics and AI overlap, and how they differ based on how results are used in decision-making.\nA simple way to break down AI systems into data, models, and logic.\nThe major AI approaches (symbolic, machine learning, deep learning) and what each is good at.\nHow data gets into real systems, and why pipelines matter for decision-making.\n\n\n\n\n\nA working course project folder with a .venv created via uv\nSeveral runnable Python scripts (.py) you can execute from the terminal\n\n\n\n\nBy the end of this week, you should be able to:\n\nExplain what analytics is and how it differs from (and overlaps with) artificial intelligence.\nDescribe AI systems using the Data + Models + Logic framework.\nIdentify the major AI paradigms (symbolic, statistical, neural) and explain how they differ conceptually.\nExplain how data pipelines support decision-making in modern analytics and AI systems.\n\n\n\n\nBy the end of this week, you should be able to:\n\nRun a Python script (.py) from the terminal.\nNavigate folders and files using basic shell commands.\nUnderstand the role of virtual environments in Python workflows.\nCreate and activate a virtual environment using uv.\nVerify that your local Python environment is correctly configured for the course."
  },
  {
    "objectID": "textbook_src/week01_main.html#welcome-to-the-course-and-how-it-works",
    "href": "textbook_src/week01_main.html#welcome-to-the-course-and-how-it-works",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "1. Welcome to the Course and How It Works",
    "text": "1. Welcome to the Course and How It Works\nThis section is not intended to replace the syllabus. It is a brief outline of some of the course philosophy.\n\n1.1 Course outcomes and the two-track approach (AI literacy + job skills)\nThis course is designed around a simple but intentional idea: understanding AI is not the same as being able to work with it. Many courses emphasize one at the expense of the other—either focusing heavily on conceptual discussions with little hands-on skill development, or emphasizing tools and code without building a durable mental model of what those tools are actually doing.\nThis course explicitly pursues two parallel outcomes, which we will refer to throughout the semester as AI literacy and job-ready skills.\nAI literacy refers to your ability to reason about analytics and AI systems. This includes understanding what problems these systems are designed to solve, how they are structured, where they tend to fail, and how their outputs should (and should not) be used in decision-making. AI literacy is not about memorizing algorithms or mathematical formulas. Instead, it is about developing a clear conceptual framework that allows you to ask good questions, interpret results critically, and communicate effectively with both technical and non-technical stakeholders.\nIn parallel, job-ready skills focus on your ability to work with analytics and AI tools in practice. This includes writing and running Python code, managing files and environments, and following workflows that resemble how analytics work is actually performed in organizational settings. These skills are intentionally practical: the goal is not to turn you into a software engineer, but to ensure that you can confidently execute, modify, and troubleshoot analytical work rather than treating code as a black box.\nImportantly, these two tracks are not separate parts of the course—they are interwoven each week. Conceptual material will often appear before coding so that you understand why a tool or technique exists before learning how to use it. Conversely, hands-on work will frequently reinforce conceptual ideas by making them concrete. When something breaks, behaves unexpectedly, or produces a surprising result, that moment is not a failure—it is an opportunity to deepen both your literacy and your skill.\nBy the end of the course, success will not be defined by how many lines of code you can write or how many definitions you can recite. Instead, success means that you can explain what an analytics or AI system is doing, execute it reliably, and evaluate its output with informed judgment. That combination—understanding plus execution—is the core outcome this course is designed to deliver.\n\n\n1.2 What you will build each week\nEach week in this course follows a consistent production pattern. Rather than treating readings, code, and assignments as separate activities, you will build a small, coherent set of artifacts that work together. This structure is intentional: it mirrors how analytics and AI work is typically organized in practice, where documentation, code, and reasoning evolve together.\nFirst, you will work through a weekly textbook chapter, delivered as an HTML document. These chapters serve as both your primary learning resource and a long-term reference. They introduce concepts, explain why techniques exist, and provide annotated examples. You are encouraged to revisit these chapters throughout the semester, especially when later topics build on earlier ideas.\nAlongside most but not all chapters, you will create and run one or more Python scripts (.py). These scripts are not isolated exercises, even if at times they feel that way; they are the hands-on implementation of the ideas discussed in the chapter and/or the lecture. All code in the textbook is written so that it can be copied directly into a script file and executed in your Python environment. Over time, these scripts are intended to create a small personal codebase that reflects your growing capability using these tools.\nBy the end of each week, you should be able to look at what you have built and answer two questions confidently:\n(1) Do I understand what this code is doing and why it exists?\n(2) Can I run it, modify it, and explain its output?\nIf the answer to both is yes, you are doing great.\n\n\n1.3 How to succeed in this course\nSuccess in this course is less about prior experience and more about how you approach the work. Analytics and AI involve working with systems that are complex, imperfect, and sometimes frustrating. Learning to make progress in that environment is a skill in itself, and this course is designed to help you develop it.\nFirst, treat confusion as a normal part of the process. You will encounter unfamiliar terms, error messages that do not immediately make sense, and code that does not work the first time you run it. This is not a sign that you are “bad at Python” or “not technical enough.” It is simply how learning in this space works. Progress will come from narrowing the problem, reading error messages carefully, and making small, deliberate changes rather than trying to fix everything at once. The struggle is what will cement these skills for you.\nSecond, focus on understanding before optimization. You are not expected to write elegant or highly efficient code early in the course, it is not the point of the course at all. Instead we focus on prioritizing clarity: writing code you can read, explain, and reason about. If you can explain what each line is doing and why it is there, you are on the right track—even if the solution feels simple or verbose. This sometimes means the course code is not written in what would be considered “textbook Python”, and as your skills progress in this area during and after the course, you will probably grow out of the patterns and approaches used here. That is normal.\nThird, develop a consistent workflow. Each week, you should expect to read the chapter, run the provided code, modify it, and verify that it behaves as expected. Running code, managing files deliberately, and using your programming environment consistently will save you significant time and frustration later in the semester. Small habits—such as running scripts frequently, saving your work often, and keeping your project folders organized—compound quickly.\nFinally, use available resources strategically. When you get stuck, start by re-reading the relevant section of the chapter and carefully examining any error messages. If you consult external resources or AI tools, do so with intention: use them to clarify concepts or suggest possible fixes, but always make sure you understand the solution before moving on. Being able to explain why a fix works is more important than finding one quickly."
  },
  {
    "objectID": "textbook_src/week01_main.html#what-is-analytics-and-what-is-ai",
    "href": "textbook_src/week01_main.html#what-is-analytics-and-what-is-ai",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "2. What Is Analytics and What Is AI?",
    "text": "2. What Is Analytics and What Is AI?\n\n2.1 Analytics: descriptive → diagnostic → predictive → prescriptive\nAnalytics is best understood not as a single technique, but as a progression of questions organizations ask about their data. These questions range from understanding what has already happened to deciding what action should be taken next. A useful way to organize this progression is through four common categories: descriptive, diagnostic, predictive, and prescriptive analytics.\nDescriptive analytics answers the question: What happened?\nThis is the most familiar form of analytics and focuses on summarizing historical data. Examples include reports, dashboards, averages, totals, and trends over time. Descriptive analytics does not attempt to explain why something occurred or what will happen next—it provides a clear picture of past outcomes. In practice, this might look like a weekly sales report, website traffic summary, or inventory count.\nDiagnostic analytics asks: Why did it happen?\nOnce an outcome is observed, the next step is often to understand its cause. Diagnostic analytics explores relationships, comparisons, and breakdowns in the data to identify contributing factors. This might involve segmenting customers, comparing performance across regions, or examining changes before and after a specific event. While still focused on historical data, diagnostic analytics moves beyond description toward explanation.\nPredictive analytics shifts the focus to the future by asking: What is likely to happen next?\nHere, statistical models and machine learning techniques are often used to estimate future outcomes based on patterns in past data. Examples include forecasting demand, predicting customer churn, or estimating the probability that an event will occur. Predictive analytics does not guarantee what will happen—it produces probabilistic estimates that support informed planning and risk management.\nPrescriptive analytics addresses the final question: What should we do about it?\nPrescriptive analytics builds on predictions by incorporating goals, constraints, and trade-offs to recommend actions. This may involve optimization models, business rules, or simulation. For example, a system might recommend how much inventory to reorder, which customers to target with a promotion, or how to allocate limited resources. At this stage, analytics becomes tightly connected to decision-making rather than analysis alone.\nIt is important to recognize that these categories are not mutually exclusive and are often combined in real systems. A single workflow might summarize past performance (descriptive), identify a problem area (diagnostic), estimate future risk (predictive), and recommend an action (prescriptive). Understanding this progression provides a foundation for seeing where AI fits—and where it extends beyond traditional analytics—later in the course.\n\n\n2.2 AI: systems that perform tasks requiring human-like judgment\nArtificial intelligence (AI) refers to a class of systems designed to perform tasks that would normally require human judgment, interpretation, or decision-making. Unlike traditional analytics, which primarily focuses on summarizing data or supporting decisions, AI systems are often embedded directly into processes where they make or influence decisions in real time.\nA defining feature of AI systems is that they operate in environments where rules are incomplete, uncertainty is present, or inputs are too complex to handle with simple, hand-coded logic. Examples include recognizing objects in images, understanding natural language, detecting fraudulent transactions, or recommending products to users. In each case, the system must evaluate patterns, weigh evidence, and produce an output that resembles what a human might do in a similar situation.\nAI systems typically rely on models trained from data rather than explicit instructions for every possible scenario. Instead of being told exactly how to respond in each case, the system learns statistical relationships or representations from historical examples. When presented with new inputs, it uses those learned patterns to generate predictions, classifications, or actions. This learning-based approach allows AI systems to scale to complex tasks, but it also introduces uncertainty and the possibility of error.\nAnother important characteristic of AI is that its outputs are often probabilistic rather than definitive. An AI system may estimate the likelihood that an email is spam, that a customer will stop using a service, or that an image contains a particular object. These estimates are then combined with thresholds, business rules, or human oversight to determine what action is taken. As a result, AI should be understood as a component within a broader decision system rather than as an autonomous replacement for human judgment.\nFinally, it is useful to distinguish between narrow AI and broader notions of intelligence. The systems discussed in this course are narrow by design: they are built to perform specific tasks under specific conditions, often very well, but they do not possess general understanding or awareness. Recognizing both the strengths and limitations of these systems is essential for using them responsibly and effectively in organizational settings.\nIn the next sections, we will compare analytics and AI more directly, highlighting where they overlap, where they differ, and how they are often combined in modern decision-making systems.\n\n\n2.3 Analytics vs AI: overlap and differences\nAnalytics and AI are closely related, but they are not interchangeable. In practice, many systems labeled as “AI” rely heavily on analytical techniques, and many analytical workflows now incorporate AI-based models. Understanding how these two domains overlap and differ is essential for making sense of how modern decision systems are designed and deployed.\nAt a high level, analytics is primarily concerned with supporting human decision-making. It focuses on extracting insight from data, identifying patterns, and presenting information in ways that help people understand what is happening and decide what to do. Even in advanced forms such as predictive or prescriptive analytics, the output is often intended to inform a human decision-maker, who retains responsibility for interpreting the results and acting on them.\nAI, by contrast, is often designed to participate directly in the decision process. AI systems may classify, recommend, prioritize, or trigger actions with minimal human involvement, especially in high-volume or time-sensitive contexts. While humans still define objectives, constraints, and oversight mechanisms, AI systems are frequently embedded into operational workflows where their outputs have immediate consequences.\nThere is, however, a significant area of overlap. Both analytics and AI: - rely on data as their primary input, - use models to represent relationships or patterns, - and produce outputs that influence decisions.\nMany predictive analytics techniques—such as regression or classification models—are also foundational components of AI systems. The difference often lies not in the mathematics, but in how the results are used. A churn prediction model displayed on a dashboard for managers is typically considered analytics; the same model automatically triggering retention offers may be considered AI.\nAnother key distinction is the role of interpretability and automation. Analytics tools often emphasize transparency, explainability, and exploration, allowing users to drill down into results and ask follow-up questions. AI systems, especially those based on complex models, may prioritize performance and scalability over interpretability, requiring additional governance and monitoring to ensure appropriate use.\nRather than viewing analytics and AI as competing approaches, it is more accurate to see them as points along a continuum. Many real-world systems combine analytical reporting, predictive modeling, and AI-driven automation into a single pipeline. Recognizing where a system sits along this continuum helps clarify expectations, risks, and responsibilities associated with its use.\nThis distinction will be especially important as we move deeper into topics such as machine learning and generative AI, where the same underlying techniques can support very different organizational roles depending on how they are deployed.\n\n\n2.4 Prediction as a component, not the whole system\nMany discussions of AI—especially those focused on machine learning—treat prediction as the central task. While prediction is a critical capability, it is important to recognize that prediction alone does not make a complete AI system. In practice, prediction is just one component within a larger structure that connects data, models, and decisions.\nA prediction answers a narrow question such as “What is the likelihood that this event will occur?” or “Which category does this input most likely belong to?” For example, a model might predict the probability that a customer will churn, that a transaction is fraudulent, or that a document belongs to a particular topic. These outputs are useful, but on their own they do not specify what action should be taken.\nWhat turns a prediction into something operational is the surrounding decision logic. Organizations must decide how to interpret a predicted probability, what threshold to apply, what constraints exist, and what costs are associated with different actions. A churn prediction of 70%, for instance, does not automatically imply the same response as a churn prediction of 40%—and even the same prediction may lead to different actions depending on business priorities or resource availability.\nThis distinction highlights why AI systems should be understood as socio-technical systems, not just models. Data pipelines determine what information is available to the model. Models generate predictions based on learned patterns. Decision frameworks translate those predictions into actions, often with human oversight and governance layered on top. Focusing exclusively on prediction risks ignoring these equally important components.\nRecognizing prediction as a component rather than the whole system also helps clarify common misunderstandings about AI capability. High predictive accuracy does not guarantee good decisions, ethical outcomes, or organizational value. Poorly designed thresholds, misaligned incentives, or unexamined assumptions can undermine even the most accurate model.\nThroughout this course, we will return to this idea repeatedly: models produce predictions, but systems produce decisions. Developing the ability to evaluate and design the full system—not just the model—will be a central goal as we move from foundational concepts into more advanced analytics and AI techniques.\nQuick check: Analytics, AI, or both?\nThe goal here is not to label each system correctly, but rather to sharpen your intuition about why something is considered analytics, AI, or a combination of both. The differences are fairly nuanced. In most real-world systems, the distinction depends less on the mathematical technique and more on how the output is used.\n\nA dashboard showing last quarter’s sales by region, with filters and charts\nThis is best described as analytics. The system summarizes historical data and presents it to a human decision-maker, who interprets the results and decides what action to take. The key thing to note here is that this is not predicting any outcome. That makes this clearly analytics. The way to think about this is to ask “does this model have decision autonomy”? If the answer is no, it is analytics. If it is “maybe” or “yes”, that answer is more complicated.\nA model that predicts the probability a customer will cancel their subscription, displayed to a manager\nThis sits at the boundary but is still primarily analytics. Even though a predictive model is used, the output supports human judgment rather than directly triggering an action.\nThe same churn prediction model automatically sending retention offers to high-risk customers\nThis is best described as AI. The prediction is now embedded in an operational workflow, and the system is actively participating in decision-making and taking action rather than merely informing it.\nA fraud detection system that flags suspicious transactions for human review\nThis example illustrates both analytics and AI. The model performs an AI-like task (pattern recognition and classification), but the final decision remains with a human, creating a hybrid system. These kinds of systems are fairly classical examples of overlap.\nA recommendation engine that personalizes product suggestions in real time\nThis is clearly AI. The system continuously generates predictions and acts on them automatically at scale, often without direct human intervention for each decision.\n\nThe key takeaway is that the same underlying model can be analytics or AI depending on context. What matters is not just whether a model is used, but where it sits in the decision process, how much autonomy it has, and who—or what—ultimately acts on its output."
  },
  {
    "objectID": "textbook_src/week01_main.html#data-models-logic-the-core-of-ai-systems",
    "href": "textbook_src/week01_main.html#data-models-logic-the-core-of-ai-systems",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "3. Data + Models + Logic: The Core of AI Systems",
    "text": "3. Data + Models + Logic: The Core of AI Systems\n\n3.1 Data\nData forms the foundation of all analytics and AI systems. Regardless of how sophisticated a model or decision framework may be, the behavior of the system is fundamentally shaped by the data it receives. Understanding what data represents, how it is generated, and how it enters a system is therefore a prerequisite for understanding how AI systems operate in practice.\nBroadly defined, data consists of recorded observations about the world. These observations may take many forms, including transaction records, sensor readings, text documents, images, user interactions, or system logs. Although these data sources differ in structure and complexity, they share a common role: they capture traces of past events or states that can be analyzed, modeled, and acted upon.\nData serves two critical functions within AI systems. First, it is used to train models. Historical data provides the examples from which models learn patterns, relationships, or representations. The coverage, quality, and diversity of this data directly influence what a model can learn and how well it generalizes beyond its training examples. Second, data is used during system operation, when new observations are provided as input to a trained model in order to generate predictions, classifications, or scores. Errors, shifts, or inconsistencies in either training data or operational data can degrade system performance.\nIt is important to recognize that data is not a complete or neutral representation of reality. Data reflects the processes through which it was collected, including organizational priorities, technical constraints, and human choices. Some phenomena are easier to observe and record than others, some groups or behaviors are overrepresented, and some variables serve only as indirect proxies for what is actually of interest. As a result, data commonly contains noise, omissions, and systematic biases.\nFrom a systems perspective, data does not simply exist—it is acquired and prepared through pipelines. These pipelines involve decisions about what to collect, how frequently to collect it, how it is stored, and how it is cleaned or transformed before use. Choices made at this stage—such as how missing values are handled or how categories are encoded—can have consequences that propagate throughout the system.\nFor these reasons, effective analysis of AI systems begins with careful attention to data. Asking where data comes from, what it represents, and what it leaves out is often the most reliable way to understand system behavior, anticipate limitations, and diagnose failures.\n\n\n3.2 Models\nA model is a formal representation of a relationship between inputs and outputs. In analytics and AI systems, models are used to map observed data to predictions, classifications, scores, or other quantities that support decision-making. While models can take many forms—from simple equations to complex neural networks—their role within a system is conceptually consistent: they provide a structured way to generalize from past observations to new situations.\nModels differ from raw data in an important way. Data records what has already happened, whereas a model encodes an assumption about how the world works. These assumptions may be explicit, as in a linear equation that specifies how inputs combine to produce an output, or implicit, as in a deep learning model that learns internal representations through training. In both cases, the model embodies a hypothesis about underlying patterns in the data.\nIn analytics and AI systems, models are typically created through a training process. During training, historical data is used to adjust the model’s parameters so that its outputs align as closely as possible with observed outcomes. This process allows the model to capture regularities in the data, but it also ties the model’s behavior to the quality and scope of the data it was trained on. A model cannot reliably learn patterns that are absent, rare, or systematically distorted in the training data.\nOnce trained, a model is used to generate outputs for new inputs during system operation. These outputs are often probabilistic rather than deterministic. Instead of producing a single “correct” answer, a model may estimate the likelihood of different outcomes or assign scores that reflect relative confidence. This probabilistic nature is a strength—it allows models to operate under uncertainty—but it also requires careful interpretation and downstream handling.\nIt is also important to recognize that models are not inherently intelligent or autonomous. They do not understand context, intent, or consequences in a human sense. Instead, they apply learned patterns mechanically, based on the structure imposed during training. As a result, models can perform impressively within familiar conditions while behaving unpredictably when those conditions change.\nUnderstanding models as components rather than complete systems helps clarify both their power and their limitations. Models can recognize patterns, make estimates, and scale decisions, but they do so within the boundaries defined by data, training procedures, and design choices. How their outputs are ultimately used depends on the surrounding logic and decision framework, which is addressed next.\n\n\n3.3 Logic\nWhile data and models are often the most visible components of AI systems, logic is what ultimately connects model outputs to real-world actions. Logic defines how predictions, scores, or classifications are interpreted and how they are translated into decisions. Without logic, even the most accurate model remains analytically interesting but operationally incomplete.\nLogic encompasses the rules, thresholds, constraints, and objectives that govern system behavior. These elements specify what should happen when a model produces a particular output. For example, a probability score may be compared against a threshold to determine whether an alert is triggered, a recommendation is shown, or a transaction is blocked. These thresholds are not inherent to the model—they are design choices that reflect priorities, trade-offs, and risk tolerance.\nIn many systems, logic also encodes business or organizational constraints. These may include resource limitations, regulatory requirements, fairness considerations, or cost structures. A model might identify many high-risk cases, but logic determines how many can realistically be acted upon, which cases are prioritized, and which actions are permissible. As a result, logic often mediates between what a model suggests and what an organization can or should do.\nLogic can be implemented in various ways. In some systems, it takes the form of explicit rules written by humans, such as conditional statements or decision trees. In others, logic is embedded within optimization routines or policy frameworks that balance competing objectives. Even when decisions appear automated, logic typically reflects human judgments made earlier about acceptable outcomes and trade-offs.\nImportantly, logic is where accountability often resides. When a system produces an undesirable outcome, the cause may not lie in the data or the model, but in the logic that governed how outputs were used. Poorly chosen thresholds, misaligned incentives, or overly rigid rules can undermine system performance even when model accuracy is high.\nViewing logic as a core component of AI systems highlights a critical insight: models produce outputs, but logic determines actions. Understanding this distinction is essential for evaluating system behavior, diagnosing failures, and designing AI systems that align with intended goals and constraints.\n\n\n3.4 Where systems fail (data, model, logic layer)\nFailures in analytics and AI systems rarely originate from a single cause. Instead, they tend to emerge from breakdowns at one or more layers of the system: data, models, or logic. Understanding these layers—and how they interact—provides a structured way to diagnose why a system behaves unexpectedly or produces poor outcomes.\nFailures at the data layer occur when the information feeding the system is incomplete, biased, noisy, or no longer representative of the environment in which the system operates. This may include missing values, measurement errors, outdated records, or shifts in underlying patterns over time. Because models learn from historical data, weaknesses at this layer often propagate forward, limiting what the system can reasonably achieve regardless of model sophistication.\nFailures at the model layer arise when the model is poorly matched to the task or the data available. This may involve overly simplistic models that fail to capture important relationships, overly complex models that overfit historical patterns, or models trained on data that does not reflect current conditions. Even well-designed models can fail when deployed in contexts that differ meaningfully from those seen during training.\nFailures at the logic layer occur when model outputs are translated into decisions inappropriately. Common issues include poorly chosen thresholds, rigid rules that do not adapt to changing conditions, or decision criteria that prioritize the wrong objectives. In these cases, a model may be producing reasonable outputs, but the surrounding logic causes undesirable actions or missed opportunities.\nImportantly, these layers are interdependent. A system with high-quality data and a strong model can still fail due to flawed decision logic. Likewise, careful logic cannot compensate for fundamentally uninformative or biased data. Effective system design therefore requires attention to all three layers simultaneously rather than focusing narrowly on model performance.\nViewing failures through this layered lens encourages more precise diagnosis and more effective intervention. Rather than asking whether an AI system “works” or “does not work,” it becomes possible to ask where it is breaking down and why. This perspective supports more thoughtful system evaluation and more responsible use of analytics and AI in decision-making contexts."
  },
  {
    "objectID": "textbook_src/week01_main.html#ai-paradigms-overview",
    "href": "textbook_src/week01_main.html#ai-paradigms-overview",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "4. AI Paradigms Overview",
    "text": "4. AI Paradigms Overview\n\n4.1 Symbolic AI\nSymbolic AI represents one of the earliest approaches to artificial intelligence. Rather than learning patterns from data, symbolic systems rely on explicit representations of knowledge and rule-based reasoning to perform tasks. These systems operate by manipulating symbols—such as words, categories, or logical statements—according to predefined rules.\nAt the core of symbolic AI is the idea that intelligent behavior can be produced by encoding expert knowledge directly into a system. This often takes the form of if–then rules, decision trees, logic statements, or structured knowledge bases. For example, a symbolic system might contain rules such as: if a customer is late on payment and has missed multiple deadlines, then flag the account for review. Each rule reflects a human judgment that has been translated into formal logic.\nSymbolic AI systems tend to be transparent and interpretable. Because their reasoning process is explicitly defined, it is usually possible to trace how a particular conclusion was reached. This makes symbolic approaches attractive in domains where explanations, compliance, or auditability are critical. They also perform well in environments where the rules are stable and the problem space is well understood.\nHowever, symbolic AI has notable limitations. Writing and maintaining rules is labor-intensive, and such systems struggle to scale as complexity increases. They also perform poorly in settings characterized by ambiguity, noise, or high variability—such as image recognition or natural language understanding—where it is difficult or impractical to enumerate all relevant rules in advance.\nWhile symbolic AI is no longer the dominant paradigm in many areas, it remains an important conceptual foundation. Many modern systems still rely on symbolic components for constraints, validation, and control, even when learning-based models are used elsewhere. Understanding symbolic AI helps clarify both the strengths of explicit reasoning and the challenges that motivated the development of data-driven approaches addressed in the next sections.\n\n\n4.2 Statistical / Machine Learning\nStatistical and machine learning approaches to AI differ from symbolic systems in a fundamental way: rather than relying on explicitly programmed rules, they learn patterns from data. These approaches use historical observations to infer relationships between inputs and outputs, allowing systems to generalize to new, unseen cases without being told exactly how to respond in every situation.\nAt the heart of machine learning is the idea that regularities in data can be captured through mathematical models whose parameters are estimated from examples. During training, a model is exposed to data and adjusted so that its predictions align with observed outcomes as closely as possible. This process allows the system to adapt to complex patterns that would be difficult to specify manually using rules alone.\nMachine learning methods are often categorized based on the type of feedback available during training. In supervised learning, the model is trained using labeled examples, where the correct output is known in advance. Common applications include classification and regression tasks, such as predicting customer churn or estimating demand. In unsupervised learning, the model works with unlabeled data to identify structure, such as clusters or latent patterns, without predefined outcomes. Both approaches are widely used in analytics and AI systems.\nCompared to symbolic AI, statistical and machine learning systems are generally more flexible and scalable. They perform well in environments with large volumes of data and can adapt to subtle patterns and correlations. However, this flexibility comes with trade-offs. Learned models may be less transparent, and their behavior can be sensitive to the data used for training. As a result, understanding and validating model performance often requires careful evaluation rather than direct inspection of rules.\nImportantly, statistical and machine learning approaches do not eliminate the need for human judgment. Choices about which data to use, which features to include, how to evaluate performance, and how to deploy model outputs remain human decisions. Machine learning shifts the burden of specification from rule-writing to data curation and model design, redefining where expertise is applied within AI systems.\nThis paradigm has become central to modern analytics and AI, forming the basis for many applications encountered in practice. It also provides the foundation for more advanced approaches, such as neural and deep learning, discussed next.\n\n\n4.3 Neural / Deep Learning\nNeural and deep learning approaches extend statistical machine learning by focusing on learning representations directly from data. Rather than relying on hand-crafted features or simple functional forms, these models use layered computational structures—commonly referred to as neural networks—to transform raw inputs into increasingly abstract representations.\nThe key idea behind neural networks is inspired by, but not equivalent to, biological neurons. A neural network is composed of interconnected units that apply weighted combinations of inputs followed by nonlinear transformations. By stacking many such layers, deep learning models can capture complex patterns in high-dimensional data. This layered structure allows them to excel in tasks such as image recognition, speech processing, and natural language understanding, where relationships are difficult to specify explicitly.\nOne defining characteristic of deep learning is its ability to operate on unstructured or semi-structured data, including images, audio, and text. In these domains, traditional statistical models often require extensive feature engineering. Deep learning models, by contrast, can learn relevant representations automatically from large volumes of data, reducing the need for manual specification of features.\nThis capability comes with important trade-offs. Neural and deep learning models are typically data-intensive and computationally demanding. Training them often requires large datasets, specialized hardware, and careful tuning. They also tend to be less interpretable than simpler models, making it more difficult to explain why a particular output was produced. As a result, deployment of deep learning systems often involves additional monitoring, validation, and governance mechanisms.\nDespite these challenges, neural and deep learning approaches have reshaped the AI landscape. Many contemporary systems—including speech recognition, computer vision applications, and large language models—are built on deep learning architectures. Understanding this paradigm helps clarify why modern AI systems can handle tasks that were previously infeasible, as well as why concerns about transparency, robustness, and control remain central to their use.\nNeural and deep learning approaches are rarely used in isolation. In practice, they are often combined with statistical methods and symbolic logic to form integrated systems, a topic addressed in the next section.\n\n\n4.4 Hybrid systems in practice\nIn real-world applications, AI systems rarely rely on a single paradigm. Instead, they are typically hybrid systems that combine symbolic reasoning, statistical or machine learning models, and neural or deep learning components. Each paradigm contributes different strengths, and hybrid designs allow systems to balance performance, interpretability, and control.\nA common pattern in hybrid systems is the use of learning-based models for perception and prediction, paired with symbolic or rule-based logic for decision-making and constraints. For example, a deep learning model may be used to recognize objects in an image or extract meaning from text, while a rule-based layer determines whether the output meets regulatory requirements or triggers a specific action. In this structure, learning handles complexity and variability, while symbolic logic enforces consistency and accountability.\nHybrid systems also help address practical limitations of individual approaches. Machine learning models can adapt to data and capture subtle patterns, but they may behave unpredictably outside familiar conditions. Symbolic logic can impose guardrails, prevent certain actions, or require human review under specified circumstances. Statistical models can provide calibrated probabilities that support decision thresholds and prioritization. Together, these components form systems that are more robust than any single approach alone.\nMany modern AI applications illustrate this hybrid structure. Recommendation systems often combine learned user preference models with business rules and inventory constraints. Fraud detection systems use predictive models to score transactions and rule-based logic to manage alerts and workflows. Large language model applications frequently pair neural models with retrieval systems, validation rules, and structured decision logic to ensure usable and reliable outputs.\nUnderstanding AI systems as hybrids reinforces an important perspective: intelligence in practice is distributed across system components, not concentrated in a single model. Performance, reliability, and responsibility emerge from how data, models, and logic are assembled and governed. This systems-level view provides a foundation for analyzing and designing AI applications that operate effectively within real organizational and societal constraints."
  },
  {
    "objectID": "textbook_src/week01_main.html#data-pipelines-and-decision-frameworks",
    "href": "textbook_src/week01_main.html#data-pipelines-and-decision-frameworks",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "5. Data Pipelines and Decision Frameworks",
    "text": "5. Data Pipelines and Decision Frameworks\n\n5.1 How systems acquire data\nBefore any analysis or modeling can occur, data must be acquired. Data acquisition refers to the processes through which information is collected, recorded, and made available for use within analytics and AI systems. These processes shape not only what data is available, but also how timely, reliable, and representative that data is.\nOne common source of data is operational systems. Transaction databases, customer relationship management systems, enterprise resource planning platforms, and web applications routinely generate records as part of normal business activity. In these cases, data is a byproduct of operations rather than something collected specifically for analysis. While such data is often rich and high-volume, it reflects the structure and incentives of the underlying system, which may limit what can be observed.\nAnother important source is instrumentation and logging. Systems are frequently designed to log events, interactions, or performance metrics explicitly for monitoring and analysis. Examples include website click logs, application usage telemetry, or sensor readings from physical devices. Instrumented data can provide fine-grained insight into behavior over time, but it requires deliberate design decisions about what to record and at what level of detail.\nData may also be acquired through external sources. These include public datasets, third-party data providers, APIs, and partner organizations. External data can enrich internal records by providing additional context, such as demographic information, market indicators, or environmental conditions. However, reliance on external sources introduces dependencies related to data quality, licensing, update frequency, and long-term availability.\nAcross these sources, data can be collected in different modes. Batch acquisition involves gathering data at regular intervals, such as daily or weekly extracts, and is common in reporting and strategic analysis. Streaming or real-time acquisition captures data continuously as events occur and is often used in monitoring, personalization, or fraud detection systems. The choice between batch and streaming acquisition affects system responsiveness, complexity, and infrastructure requirements.\nImportantly, data acquisition is not a passive process. Decisions about what to collect, how to define variables, and how frequently to record observations embed assumptions into the system. These assumptions influence downstream analysis and decision-making, sometimes in subtle ways. Understanding how data enters a system is therefore a critical step in evaluating both the capabilities and the limitations of analytics and AI applications.\n\n\n5.2 From pipeline to decision frameworks\nA data pipeline does not exist in isolation. Its purpose is to support decisions by moving raw data through a sequence of steps that make it usable, interpretable, and actionable. Understanding how pipelines connect to decision frameworks helps clarify how analytics and AI systems translate information into outcomes.\nA typical data pipeline begins with acquisition and continues through stages such as cleaning, transformation, storage, and aggregation. Each stage prepares the data for the next, addressing issues such as missing values, inconsistent formats, or incompatible sources. While these steps are often treated as technical details, they play a central role in shaping what information ultimately reaches models and decision-makers.\nOnce data has been processed, it enters the decision framework. This framework defines what decision is being supported or automated, what objectives are being pursued, and what constraints must be respected. In analytical settings, the output of the pipeline may feed dashboards or reports that inform human judgment. In AI-driven settings, processed data may be passed directly to models whose outputs trigger actions or recommendations.\nDecision frameworks also specify how outputs are evaluated and acted upon. This includes defining thresholds, priorities, costs, and trade-offs. For example, a predictive model may estimate risk, but the decision framework determines what level of risk warrants intervention, how limited resources are allocated, and what actions are permissible. These choices are rarely purely technical; they reflect organizational goals and values.\nThe connection between pipelines and decision frameworks is often iterative rather than linear. As decisions are made and actions are taken, new data is generated and fed back into the pipeline. This feedback loop can be used to monitor performance, update models, or revise decision rules over time. Effective systems are designed with this dynamic interaction in mind rather than treating pipelines as one-time processes.\nViewing pipelines and decision frameworks together reinforces a systems-level perspective. Data pipelines make information available, but decision frameworks determine how that information is used. Both are necessary for analytics and AI systems to function effectively, and weaknesses in either can undermine the overall quality of decisions.\n\n\n5.3 Common failure modes\nEven when individual components appear well designed, analytics and AI systems can fail in predictable ways. Many of these failures arise not from a single mistake, but from misalignments between data pipelines, models, and decision frameworks. Recognizing common failure modes makes it easier to diagnose problems and to design systems that are more resilient.\nOne common failure occurs when data pipelines drift away from decision needs. Data may be collected because it is easy to capture rather than because it is relevant to the decision at hand. Over time, pipelines can accumulate variables and transformations that no longer align with current objectives, leading to analyses that are technically correct but operationally unhelpful.\nAnother frequent issue is stale or delayed data. When pipelines operate on batch schedules that are too slow for the decisions they support, outputs may be outdated by the time they are used. This is especially problematic in environments where conditions change rapidly. In such cases, system performance degrades not because models are inaccurate, but because they are responding to yesterday’s information.\nFailures also occur when feedback loops are ignored or misunderstood. Decisions based on model outputs often influence future data, which in turn affects subsequent model behavior. If these feedback effects are not anticipated, systems can reinforce undesirable patterns, amplify noise, or create misleading signals that appear as genuine trends.\nA further source of failure lies in overconfidence in automation. When decision frameworks rely too heavily on model outputs without sufficient monitoring or human oversight, small errors can scale quickly. Conversely, overly cautious frameworks that ignore model outputs may negate potential benefits. Balancing automation and control is therefore a persistent design challenge.\nFinally, failure can result from organizational misalignment. Even well-designed pipelines and models can produce poor outcomes if incentives, responsibilities, or governance structures are unclear. Decisions about who owns the data, who is accountable for outcomes, and how performance is evaluated play a critical role in system success.\nUnderstanding these common failure modes reinforces an important lesson: effective analytics and AI systems depend on alignment across technical and organizational dimensions. Addressing failures requires looking beyond individual components to the system as a whole."
  },
  {
    "objectID": "textbook_src/week01_main.html#running-your-first-python-script-.py",
    "href": "textbook_src/week01_main.html#running-your-first-python-script-.py",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "6. Running Your First Python Script (.py)",
    "text": "6. Running Your First Python Script (.py)\n\n6.1 What a Python script is\nA Python script is a plain text file containing Python code, typically saved with a .py extension. When a script is run, Python reads the file from top to bottom and executes each instruction in order. Unlike interactive environments, such as notebooks or consoles, a script represents a complete, self-contained program.\nScripts are the most common way Python is used in professional analytics and AI workflows. They are easy to version, easy to share, and behave predictably when run multiple times. Because a script always starts from a clean state, its behavior depends entirely on the code it contains and the environment in which it runs—there is no hidden execution history.\nA Python script can contain many elements, including:\n- variable assignments,\n- calculations,\n- function definitions,\n- conditional logic,\n- and instructions to read data or produce output.\nSome scripts are short and perform a single task, while others may coordinate complex workflows. Regardless of size, the defining characteristic is that the script is executed as a unit.\nMany scripts follow a common structural pattern that improves clarity and reusability. For example, logic is often placed inside functions, with a designated entry point that tells Python where execution should begin. This pattern makes it easier to read the code, reuse components later, and avoid unintended behavior when code is imported into other files.\nConceptually, it is helpful to think of a Python script as answering three questions:\n1. What inputs does this program expect?\n2. What processing does it perform?\n3. What outputs does it produce?\nKeeping these questions in mind encourages clearer program structure and makes debugging easier as scripts become more complex.\n\n\n6.2 Running a script in Visual Studio Code\nVisual Studio Code (VS Code) is a commonly used environment for writing and running Python scripts. It combines a code editor, a terminal, and language support in a single interface, making it well suited for analytics and AI workflows.\nTo run a Python script in VS Code, the file must first be opened in the editor. Scripts are typically saved with a .py extension and stored within a project folder. Once the file is open, execution can be initiated in several ways, but all methods ultimately run the script using the Python interpreter selected for the project.\nOne common approach is to use the integrated terminal. VS Code includes a terminal window that opens within the editor and behaves like a standard system terminal. From this terminal, the script can be executed by navigating to the folder containing the file and running:\n\n\n\n\n\n\nRun the script from the terminal\n\n\n\nFrom the project folder, run:\npython hello_world.py\n\n\nVS Code also provides editor-based run options, such as a Run Python File button or command palette actions. These tools are convenient, but they rely on the same underlying mechanism: calling the Python interpreter on the current file. Regardless of how execution is triggered, the result is the same—the script runs from top to bottom and any output is displayed in the terminal.\nAn important step when running scripts in VS Code is selecting the correct Python interpreter. In projects that use virtual environments, the interpreter should point to the environment created for the project rather than the system-wide Python installation. VS Code allows the interpreter to be selected on a per-project basis, ensuring that the correct packages and versions are used when scripts are executed.\nWhen a script runs successfully, any output produced by print() statements or error messages appears in the terminal. If the script finishes without errors, control returns to the terminal prompt. This clear start-and-finish behavior is a defining feature of script-based workflows and makes it easier to reason about program execution and diagnose problems when issues arise.\n\n\n6.3 Reading error messages (very high level)\nWhen a Python script encounters a problem, it produces an error message. While error messages can look intimidating at first, they are an essential part of working with code and often contain enough information to identify what went wrong. Learning to read them at a high level is an important early skill.\nMost Python error messages include three main components. First, they indicate where the error occurred, usually by showing the file name and line number. This helps narrow attention to a specific part of the script rather than the entire program. Second, they describe what type of error occurred, such as a syntax error, a missing name, or an invalid operation. Third, they may include a brief explanation of the issue.\nAt this stage, it is not necessary to understand every detail of an error message. Instead, focus on identifying the general category of the problem. A syntax error usually means Python could not interpret the structure of the code, often due to a missing parenthesis, quotation mark, or colon. Other errors occur while the script is running and typically indicate that Python understood the code but encountered an unexpected situation, such as using a variable that does not exist.\nError messages are not signals to stop; they are signals to inspect and adjust. Often, the fastest way forward is to read the message carefully, locate the referenced line, and compare it to what was intended. Small changes—such as correcting a spelling mistake or fixing indentation—frequently resolve the issue.\nDeveloping comfort with error messages takes time, but it begins with a simple mindset shift: errors are feedback, not failure. Each message is an opportunity to understand how Python interprets instructions and how small changes in code affect execution.\n\n\n6.4 The edit → run → fix loop\nWriting code is an iterative process. Scripts are rarely written correctly on the first attempt, and effective work with Python depends on developing a steady rhythm of editing, running, and fixing code. This cycle—often referred to as the edit → run → fix loop—is the normal way programs are developed and refined.\nThe process begins by making a small change to the code. This might involve adding a new line, modifying an existing statement, or adjusting a variable value. After making the change, the script is run to observe its behavior. Running the script provides immediate feedback, either in the form of expected output or an error message that signals a problem.\nIf the result is not what was intended, the next step is to fix the issue. This may involve correcting a syntax error, revising a calculation, or clarifying the logic of the program. Importantly, fixes are most effective when changes are made incrementally. Altering many parts of a script at once can make it difficult to identify what caused a problem or whether a fix actually worked.\nThis loop encourages experimentation and learning. By making small, deliberate changes and observing their effects, it becomes easier to understand how individual lines of code contribute to overall behavior. Over time, this process builds intuition about how Python executes instructions and how to approach debugging systematically.\nThe edit → run → fix loop also reinforces a practical habit: run code early and often. Frequent execution reduces the distance between cause and effect, making problems easier to diagnose. As scripts grow more complex, maintaining this iterative rhythm becomes one of the most reliable ways to write correct, understandable, and maintainable code."
  },
  {
    "objectID": "textbook_src/week01_main.html#week-1-code",
    "href": "textbook_src/week01_main.html#week-1-code",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "Week 1 Code:",
    "text": "Week 1 Code:\nThis is just an example, the details in the below code are covered in future chapters.\nName the script: hello_world.py\n# hello_world.py\n# A minimal Python script illustrating basic structure and execution.\n\ndef main():\n    \"\"\"\n    The main function contains the core logic of the script.\n    When the script is run, this function will be executed.\n    \"\"\"\n    print(\"Hello, world!\")\n    print(\"This script is running successfully.\")\n\nif __name__ == \"__main__\":\n    # This conditional ensures that main() runs only\n    # when the script is executed directly, not when imported.\n    main()"
  },
  {
    "objectID": "textbook_src/week01_main.html#hello-python-printing-variables-and-simple-math",
    "href": "textbook_src/week01_main.html#hello-python-printing-variables-and-simple-math",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "7. Hello Python: Printing, Variables, and Simple Math",
    "text": "7. Hello Python: Printing, Variables, and Simple Math\n\n7.1 Printing output\nThe simplest way for a Python program to communicate with you is by printing output to the screen. In a script-based workflow, this output appears in the terminal after the script is run. Printing is therefore the most basic feedback mechanism for understanding what a program is doing.\nIn Python, printing is done using the built-in print() function. A function is an instruction that performs a specific task, and print() is responsible for displaying information. Anything placed inside the parentheses is sent to standard output.\nFor example, printing a short message looks like this:\nprint(\"Hello, world!\")\nWhen this line is executed, Python displays the text exactly as written (without the quotation marks). Text enclosed in quotation marks is called a string, and strings are commonly used for messages, labels, and explanations.\nPrinting is not limited to text. Python can also print numbers and the results of calculations:\nprint(3)\nprint(2 + 5)\nIn these cases, Python evaluates the expression first and then prints the result. This makes print() especially useful for checking intermediate values and understanding how a program is behaving as it runs.\nIt is important to note that print() does not change the program’s logic or store information for later use. Its role is purely communicative—it shows values so they can be read by a person. For this reason, printing is frequently used when learning Python, debugging code, or verifying that a script is producing the expected results.\nAs scripts become more complex, printing remains a simple but powerful tool. By printing values at different points in a program, it becomes possible to observe how data flows through the code and how instructions are executed step by step.\n\n\n7.2 Variables and assignment\nIn Python, a variable is a name that refers to a value. Variables allow programs to store information so it can be used, reused, and modified as the program runs. Rather than working only with raw numbers or text, variables make code more readable and flexible.\nVariables are created using assignment, which is done with a single equals sign (=). Assignment tells Python to take the value on the right-hand side and store it under the name on the left-hand side.\nFor example:\nx = 2\nThis line should be read as “assign the value 2 to the variable named x,” not as a statement of equality. The equals sign in Python does not mean “is equal to” in the mathematical sense; it means “store this value under this name.” This can be confusing for new users, so make sure you understand the difference.\nOnce a variable has been assigned, it can be used anywhere a value could be used. For example:\nprint(x)\nHere, Python looks up the value stored in x and prints it. If the value of x changes later, printing x will reflect the new value.\nVariables are especially useful when working with calculations. Instead of repeating numbers directly, values can be stored once and reused:\nprint(2 + 2)\nprint(2 + x)\nIn the second line, x is substituted with its stored value before the addition happens.\nIn these 2 examples, the result should be the same for each of these chunks of code. Initially it can be hard to understand why you might use variables instead of just the numbers. In more complicated examples, variables may make the code easier to understand, and easier to modify.\nprice = 20\ntax = 1.50\ntotal = price + tax\nprint(total)\nAs the values for price and tax changes, the model (calculation of “total”) still continues to work.\nVariable names are chosen by the programmer and should be descriptive enough to indicate what the value represents. While Python allows many naming styles, good variable names improve clarity and reduce confusion, especially as programs grow longer.\nAt a conceptual level, variables answer a simple question: What information does this program need to remember while it runs? Learning to use variables effectively is a key step toward writing programs that do more than display fixed messages.\n\n\n7.3 Simple math and expressions\nPython can perform basic mathematical operations in a way that closely mirrors standard arithmetic. These operations are written as expressions, which are combinations of values and operators that Python evaluates to produce a result.\nThe most common arithmetic operators include addition, subtraction, multiplication, and division:\nprint(2 + 3)\nprint(10 - 4)\nprint(3 * 5)\nprint(8 / 2)\nIn each case, Python evaluates the expression and then prints the result. These operations behave as expected for most everyday calculations, making Python a natural tool for working with numerical data.\nExpressions become more useful when combined with variables. Instead of working with fixed numbers, variables allow calculations to adapt as values change:\na = 10\nb = 4\nprint(a + b)\nprint(a * b)\nHere, Python replaces each variable with its stored value before performing the calculation. This substitution happens automatically and is a core feature of how expressions work.\nPython follows standard order of operations when evaluating expressions. Multiplication and division are performed before addition and subtraction, unless parentheses are used to make the intended order explicit:\nprint(2 + 3 * 4)\nprint((2 + 3) * 4)\nParentheses make calculations clearer and reduce ambiguity, especially as expressions become more complex. Using them deliberately improves readability and helps prevent unintended results.\nIt is also common to combine expressions and printing in a single line, especially when exploring or verifying calculations:\ntotal = 15 + 7\nprint(\"Total:\", total)\nIn this example, Python first evaluates the expression, assigns the result to a variable, and then prints a message that includes the computed value.\nSimple math and expressions form the computational backbone of most programs. Even advanced analytics and AI systems ultimately rely on large numbers of these basic operations. Developing comfort with expressions at this level makes it easier to understand how more complex calculations are built later on.\n\n\n7.4 A first complete Python program\nAt this point, it is useful to bring together the ideas from the previous sections into a single, complete Python script. A complete program combines printing, variables, and expressions in a way that performs a small but meaningful task from start to finish.\nConsider the following example. This script defines a few values, performs a calculation, and prints the result in a readable way:\n# first_program.py\n\nprice = 20\ntax = 1.50\n\ntotal_cost = price + tax\n\nprint(\"Price:\", price)\nprint(\"Tax:\", tax)\nprint(\"Total cost:\", total_cost)\nThis program follows a simple and common structure. First, values are assigned to variables. Next, those variables are used in an expression to compute a new value. Finally, the results are printed so they can be seen in the terminal when the script is run.\nEach line in this script serves a clear purpose. The variable assignments store information the program needs to remember. The expression combines those values to produce a result. The print statements communicate that result to the user. Together, these elements form a complete workflow: define → compute → display.\nWhen this script is executed, Python runs the file from top to bottom. There is no hidden state and no interaction required during execution. Every time the script is run, it produces the same output given the same starting values. This predictability is one of the key advantages of script-based programming.\nAlthough this program is simple, it illustrates the core building blocks that appear in much larger applications. More advanced programs may read data from files, make decisions using conditional logic, or repeat calculations in loops, but they are still composed of the same fundamental elements introduced here.\nBeing able to read, write, and reason about small complete programs is an important milestone. From this point forward, new concepts will build on these foundations rather than replacing them."
  },
  {
    "objectID": "textbook_src/week01_main.html#terminal-and-file-navigation-basics",
    "href": "textbook_src/week01_main.html#terminal-and-file-navigation-basics",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "8. Terminal and File Navigation Basics",
    "text": "8. Terminal and File Navigation Basics\nThis section explains how Python scripts are located and executed on a computer by introducing the basic ideas behind the terminal, file systems, and paths. The goal is not to master the terminal, it is fairly complicated, and takes some time to really figure out. Instead this section should be seen as the minimally viable tools in the terminal.\n\nOpening the terminal (macOS, Windows, and VS Code)\nBefore working with terminal commands, it is important to know how to open a terminal. The exact steps depend on the operating system and tools being used, but the underlying concept is the same: opening a window where text-based commands can be entered and executed.\nOn macOS, the terminal application is called Terminal. It can be opened in several ways:\n- By using Spotlight search and typing “Terminal”\n- By navigating to Applications → Utilities → Terminal\nOnce opened, the Terminal window provides direct access to the macOS command line.\nOn Windows, the terminal experience may appear under different names depending on configuration. Common options include:\n- Command Prompt\n- Windows PowerShell\n- Windows Terminal (a newer application that supports multiple shells)\nAny of these can be opened by searching from the Start menu. While they may look slightly different, they all allow commands to be typed and executed in the same basic way.\nIn this course, most examples assume the use of Visual Studio Code, which includes an integrated terminal. This terminal runs inside the editor itself and behaves just like a regular system terminal, but with important advantages for programming.\nTo open the integrated terminal in Visual Studio Code: - Use the menu option View → Terminal\n- Or use the keyboard shortcut that opens the terminal panel\nThe integrated terminal starts in the context of the current project folder, which reduces the need to navigate manually through the file system. This makes it especially convenient for running Python scripts and managing project files.\nRegardless of how the terminal is opened, the same ideas apply. Commands are typed, executed, and produce output.\nThe sections that follow explain how to understand how to interact with and use the terminal once opened.\n\n\n8.1 What the terminal is and why it matters\nThe terminal is a text-based interface for interacting directly with a computer’s operating system. Instead of clicking on icons or navigating menus, instructions are typed as commands. The computer executes those commands and returns output in the same window.\nWhile graphical interfaces are designed for ease of use, the terminal is designed for precision and control. It allows you to specify exactly what you want the computer to do, where to do it, and how to do it. This makes the terminal especially important for programming, where small differences in location or configuration matter.\nIn a graphical interface, running a program often involves clicking on a file. In contrast, running a Python script from the terminal requires two pieces of information:\n1. Which Python interpreter to use\n2. Where the script is located\nThe terminal makes both of these explicit.\nWhen you type a command into the terminal, you are issuing an instruction and then waiting for a response. For example, a simple command that asks the computer where you currently are in the file system looks like this:\npwd\nAfter pressing Enter, the terminal responds by printing the current working directory. This directory is the location the terminal is “pointing to,” and it determines which files the computer can see when you issue commands.\nThis idea of location is critical. When you run a Python script using a command such as:\npython first_program.py\nPython looks for the file named first_program.py in the current working directory. If the file is not located there, Python cannot run it, even if the file exists elsewhere on your computer. This is one of the most common sources of confusion for beginners, and it highlights why understanding the terminal matters.\nThe terminal also differs from clicking files in an important way: commands are repeatable and explicit. When you type a command, you can see exactly what was executed. This makes it easier to reproduce results, diagnose errors, and understand what the computer is doing step by step.\nEvery terminal interaction follows the same basic pattern:\n1. You type a command.\n2. The computer executes it.\n3. The terminal displays output or an error message.\n4. Control returns to you.\nFor example, listing the contents of the current directory looks like this:\nls\nThe terminal responds by showing the files and folders in that location. This immediate feedback loop—command followed by output—is central to working effectively with the terminal and will be used throughout the course.\nUnderstanding the terminal is not about memorizing commands. It is about developing a mental model of how the computer interprets instructions, how files are located, and how programs are executed. With that model in place, the terminal becomes a powerful and reliable tool rather than a source of frustration.\n\n\n8.2 Files, folders, and working directories\nComputers organize information using files and folders (also called directories). A file contains data or instructions, such as a Python script, while a folder is a container that holds files and other folders. Every file on a computer exists inside exactly one folder, and folders can be nested inside other folders.\nWhen working in the terminal, the computer always keeps track of a single location called the current working directory. This directory represents “where you are” in the file system at any given moment. All commands you type into the terminal are interpreted relative to this location unless you explicitly say otherwise.\nYou can ask the terminal to show the current working directory using the following command:\npwd\nThe output shows the full path to the folder the terminal is currently using. This location matters because most commands—including those that run Python scripts—operate on files in this directory by default.\nTo see what files and folders exist in the current working directory, you can list its contents:\nls\nThe terminal responds by displaying the names of files and folders in that location. If a Python script does not appear in this list, it means the terminal cannot “see” it from the current directory.\nThis explains why a command such as:\npython script.py\nonly works when the file named script.py is located in the current working directory. When this command is run, Python looks for script.py in the folder the terminal is currently pointing to. If the file is elsewhere, Python cannot run it and will report that the file cannot be found.\nPrograms locate files using the same logic. When a program is executed, it starts in the current working directory and interprets file references relative to that location. If a script refers to another file without specifying a full path, Python assumes that file is located in—or relative to—the directory where the program was run.\nChanging the current working directory changes what files are visible to the terminal and to any programs launched from it. Moving between folders is therefore a fundamental skill when working with scripts and project-based code. In the next section, this idea is extended by introducing paths, which provide precise ways to describe file locations both relative to the current directory and relative to the entire file system.\n\n\n8.3 Relative vs absolute paths\nA path is a description of where a file or folder is located on a computer. Paths allow both humans and programs to refer to specific locations in the file system. When working in the terminal, paths are how you tell the computer which file or folder you mean.\nThere are two main types of paths: absolute paths and relative paths. The difference between them depends on where the path begins.\nAn absolute path describes a location starting from the root of the file system. It specifies the full sequence of folders that must be followed to reach a file, regardless of the current working directory. Because absolute paths always start from the same place, they uniquely identify a file’s location.\nFor example, an absolute path might look like this:\n/Users/username/projects/week1/hello_world.py\nThis path tells the computer exactly where the file lives, no matter where the terminal is currently pointed. Absolute paths are precise, but they can be long, system-specific, and inconvenient to type repeatedly.\nA relative path, by contrast, describes a location starting from the current working directory. Instead of beginning at the root of the file system, a relative path is interpreted based on where you are at the moment the command is run.\nFor example, if the terminal is already inside the week1 folder, the same file could be referred to simply as:\nhello_world.py\nRelative paths are shorter and easier to read, but they only make sense in relation to the current working directory. This is why understanding where you are in the file system is so important when using the terminal.\nIn most projects, relative paths are used far more often than absolute paths. Relative paths make code easier to move between computers and directories without modification. If a project folder is copied or shared, relative paths continue to work as long as the internal structure of the project remains the same.\nTwo special symbols are commonly used in relative paths. The symbol . refers to the current directory, while .. refers to the parent directory, which is the folder that contains the current one. These symbols provide a concise way to navigate up and down the folder hierarchy.\nFor example, moving up one level in the directory structure looks like this:\ncd ..\nUsing these symbols allows paths to express relationships between folders rather than fixed locations. This relational view of file locations is central to working effectively with scripts, projects, and command-line tools.\nUnderstanding paths—especially the distinction between relative and absolute paths—helps explain many common terminal errors and clarifies how programs locate the files they need. With this foundation in place, navigating the file system becomes a logical process rather than a trial-and-error exercise.\n\n\n8.4 Essential navigation commands (conceptual overview)\nWorking in the terminal involves a small set of core commands that allow you to navigate the file system and understand what files are available at any given moment. These commands are used constantly when working with Python scripts, not because they are complex, but because they express intent very directly.\nOne of the most common tasks is moving between folders. This is done by changing the current working directory. Conceptually, this is no different from opening a different folder in a graphical interface, except that it is done by issuing a command rather than clicking.\nFor example, moving into a folder looks like this:\ncd projects\nAfter this command runs, the terminal’s current working directory changes to the projects folder. All subsequent commands are interpreted relative to this new location.\nAnother common task is listing the contents of a folder. This allows you to see which files and subfolders exist in the current directory:\nls\nListing files is often the first step when something does not work as expected. If a file does not appear in the listing, it means the terminal cannot see it from the current location.\nFolders are also created directly from the terminal. Creating folders is especially useful for organizing projects and keeping related files together:\nmkdir week1\nThis command creates a new folder named week1 inside the current working directory. Organizing code into folders helps keep scripts, data, and outputs clearly separated, which becomes increasingly important as projects grow.\nThese navigation commands matter because the terminal always operates in a specific location. Python scripts are run from that location, files are created there by default, and relative paths are resolved based on it. When a command behaves unexpectedly, the cause is often not the command itself, but the directory from which it was run.\nThe goal is not to memorize commands, but to understand their intent:\n- Where am I?\n- What files are here?\n- Where do I want to go next?\nOnce this mental model is in place, the specific command names become easier to remember, and working in the terminal becomes a predictable and logical process rather than a trial-and-error activity.\n\n\n8.5 How the terminal connects to Python execution\nThe terminal plays a central role in running Python scripts because it provides the context in which commands are interpreted. When a Python script is executed, the terminal is responsible for telling Python which file to run and where to find it.\nWhen you type a command such as:\npython first_program.py\nyou are giving Python two pieces of information at once. First, you are specifying that the Python interpreter should be used. Second, you are specifying the name of the file to execute. What is not explicitly stated—but is critically important—is where Python should look for that file.\nBy default, Python looks for the script in the current working directory. That directory is determined entirely by the terminal’s location at the moment the command is run. If the file named first_program.py is located in that directory, Python can execute it. If it is not, Python reports an error indicating that the file cannot be found.\nThis explains why errors such as “file not found” or “no such file or directory” occur so frequently. In many cases, the issue is not that the file does not exist, but that the terminal is pointed at the wrong folder when the command is issued.\nTerminal navigation commands directly affect this outcome. Changing directories changes what files are visible to Python. Listing files allows you to verify whether the script you want to run is actually present in the current location. Together, these actions form a simple but powerful workflow:\n\nNavigate to the folder containing the script.\n\nConfirm the file is present.\n\nRun the script using the Python command.\n\nProject folder structure reinforces this workflow. When scripts are organized into predictable folders, it becomes easier to navigate to the correct location and run code reliably. Relative paths and consistent organization reduce the likelihood of execution errors and make projects easier to understand and maintain.\nThis leads to a useful mental model for running Python scripts:\nlocation → command → execution\nFirst, the terminal’s location determines what files are accessible. Next, the command specifies what action to take. Finally, Python executes the script based on that context. When something goes wrong, tracing the problem through these three steps is often the fastest way to identify and resolve the issue.\nUnderstanding this connection between the terminal and Python execution transforms error messages from obstacles into signals. Rather than guessing, it becomes possible to reason systematically about what the computer is doing and why a particular command succeeds or fails.\n\n\n8.6 Common mistakes and recovery strategies\nMistakes in the terminal are not signs of failure or lack of ability. They are a normal and expected part of learning to work with files, paths, and scripts. Most issues encountered at this stage fall into a small number of common patterns, and each has straightforward recovery strategies.\nOne frequent mistake is running commands from the wrong directory. When the terminal is pointed at a different folder than expected, commands such as running a Python script will fail because the file cannot be found. In these situations, the solution is not to change the command, but to confirm the terminal’s current location and navigate to the correct folder before trying again.\nAnother common issue is confusing file names or extensions. Python scripts must be referenced using their exact file name, including the .py extension. A missing or misspelled character is enough to cause an error. Verifying file names by listing the contents of the directory often resolves this type of problem quickly.\nIt is also easy to forget where files were saved, especially when working across multiple folders or projects. When this happens, the most effective approach is to slow down and retrace steps: identify the project folder, navigate into it, and inspect its contents. Guessing or repeatedly trying variations of a command rarely helps and often increases frustration.\nGetting “unstuck” usually involves a small set of reliable actions:\n- Check the current working directory.\n- List the files in that directory.\n- Confirm the script’s name and location.\n- Re-run the command once the context is correct.\nThese steps are simple, but they are powerful because they restore clarity about what the computer can see and what it is being asked to do.\nPerhaps most importantly, these mistakes are not unique to beginners. Even experienced programmers regularly encounter them, especially when switching projects or environments. What changes with experience is not the absence of mistakes, but the speed and confidence with which they are resolved.\nLearning to work in the terminal is as much about developing patience and a clear mental model as it is about learning commands. With practice, errors become signals rather than obstacles, and recovery becomes a routine part of working effectively with code."
  },
  {
    "objectID": "textbook_src/week01_main.html#virtual-environments-overview",
    "href": "textbook_src/week01_main.html#virtual-environments-overview",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "9. Virtual Environments Overview",
    "text": "9. Virtual Environments Overview\nUp to this point, the focus has been on understanding how the terminal interacts with files and how Python scripts are located and executed. These ideas establish where commands run and which files they act on. The next layer adds an equally important question: which version of Python, and which set of installed tools, should be used when a script runs.\nThis question becomes especially important as projects grow and as additional libraries are introduced. Virtual environments provide a structured way to manage this complexity by controlling the software context in which Python programs execute. The sections that follow introduce virtual environments as a practical solution to a common problem, building directly on the terminal-based workflows already established.\n\n9.1 Why virtual environments exist\nVirtual environments exist to solve a practical problem that arises when working with Python across multiple projects: different projects often require different packages, or different versions of the same package.\nBy default, Python allows packages to be installed globally, meaning they are available to all Python programs on a computer. While this may seem convenient at first, it quickly leads to conflicts. One project may require a newer version of a library, while another depends on an older version. Installing or upgrading a package for one project can unintentionally break another.\nThese conflicts are especially common in analytics and AI work, where projects often rely on rapidly evolving libraries. Changes in package behavior, version compatibility, or dependencies can cause code that once worked correctly to fail without any changes to the code itself.\nVirtual environments address this problem by providing isolation. Each environment contains its own copy of the Python interpreter along with its own set of installed packages. This allows each project to define and control exactly which packages and versions it uses, without affecting other projects or the system-wide Python installation.\nImportantly, virtual environments are not about adding complexity for its own sake. They are a way to reduce uncertainty and prevent accidental breakage. By isolating dependencies, environments make projects more predictable, easier to reproduce, and easier to share with others.\nFrom a workflow perspective, virtual environments support a simple principle: a project should carry its own assumptions about its software dependencies. When those assumptions are explicit and isolated, problems become easier to diagnose and fixes become easier to apply.\nUnderstanding why virtual environments exist provides context for the steps that follow. Rather than treating environment setup as a ritual to memorize, it becomes clear that environments are a practical response to a real and common problem in Python-based work.\n\n\n9.2 What a virtual environment is\nA virtual environment is a self-contained Python setup that exists alongside, but separate from, the system-wide Python installation. It provides a controlled space in which a project can run using a specific Python interpreter and a specific set of installed packages.\nEach virtual environment includes its own Python interpreter and its own package directory. When a program is run inside an environment, Python uses the interpreter and packages associated with that environment rather than those installed globally on the system. This separation is what allows multiple projects with different requirements to coexist on the same computer without interfering with one another.\nVirtual environments are closely tied to two components: the Python interpreter and installed packages. The interpreter determines which version of Python is used, while the installed packages determine which libraries and tools are available to the program. Together, these define the software context in which a script executes.\nWhen a virtual environment is activated, the terminal is instructed to use the environment’s Python interpreter by default. As a result, running Python commands or executing scripts uses the environment’s configuration rather than the system-wide one. Activation does not change Python itself; it changes which Python is selected when commands are run.\nIt is important to understand what activation does not do. Activating a virtual environment does not delete or replace the system Python installation. It does not modify other environments, and it does not affect projects outside the current working context. The system Python remains available and unchanged; the environment simply provides an alternative that is used temporarily.\nThis distinction is essential for developing confidence with environments. Virtual environments do not take control of the computer or permanently alter Python. They provide a scoped, reversible context in which projects can be developed and executed reliably. Once this idea is clear, the mechanics of using environments become much easier to understand and apply.\n\n\n9.3 The course-standard environment workflow (using uv)\nTo manage virtual environments consistently, this course uses a single tool: uv. Rather than introducing multiple environment managers or allowing a mix of tools, one standard workflow is used throughout. This reduces confusion, minimizes setup errors, and ensures that examples behave the same way for everyone.\nThe choice to standardize on one tool is intentional. Environment management is not the core subject of the course; it is supporting infrastructure. Using a single, reliable tool allows attention to remain on analytics, AI concepts, and Python itself rather than on resolving tooling differences.\nAt a high level, the environment workflow consists of three steps:\n\nCreate an environment\nA new virtual environment is created for the project. This environment serves as an isolated space where the project’s Python interpreter and packages will live.\nActivate the environment\nActivating the environment tells the terminal to use the environment’s Python interpreter by default. From this point forward, Python commands and scripts run within the context of the environment rather than the system-wide installation.\nInstall dependencies\nRequired packages are installed into the environment. These packages become available only within that environment, ensuring that the project’s dependencies are isolated and controlled.\n\nThese steps form a repeatable process that is used for every project. While the specific commands will be introduced later, the important idea is the sequence itself. Creating, activating, and installing are distinct actions with different purposes, and each plays a role in establishing a predictable execution context.\nBy emphasizing process rather than command memorization, the workflow remains understandable even as tools evolve. The details of how uv performs each step may change over time, but the underlying structure of environment-based development remains the same. Understanding this structure makes it easier to reason about errors, recover from mistakes, and apply the same approach in future projects beyond this course.\n\n\n9.4 Activating and deactivating environments (conceptual)\nActivating a virtual environment changes how the terminal interprets Python-related commands. Rather than altering Python itself, activation tells the terminal which Python interpreter and which set of packages should be used when commands are executed.\nWhen an environment is activated, the terminal is temporarily configured so that Python commands refer to the environment’s Python interpreter instead of the system-wide one. As a result, any script that is run uses the packages installed in that environment. This is why activation is such an important step in the workflow: it determines the software context in which code executes.\nActivation affects two key things. First, it determines which Python runs. Even if multiple versions of Python exist on a computer, activation ensures that the correct interpreter is used for the current project. Second, it determines which packages are available. Only the packages installed in the active environment can be imported and used by scripts.\nForgetting to activate an environment is a common source of confusion. When this happens, Python may still run, but it may use the system-wide interpreter and packages instead of the project-specific ones. This can lead to errors where code appears correct but fails because required packages are missing or the wrong versions are being used.\nDeactivating an environment simply returns the terminal to its default state. After deactivation, Python commands once again refer to the system-wide Python installation. No files are deleted, and no environments are removed; the change is temporary and reversible.\nUnderstanding activation and deactivation reinforces an important idea: virtual environments are contexts, not permanent changes. They define how Python behaves in a given terminal session, and they can be entered and exited as needed. With this mental model in place, environment-related issues become easier to identify and resolve.\n\n\n9.5 How environments connect to the terminal and Visual Studio Code\nVirtual environments are closely tied to the terminal session in which they are activated. When an environment is activated, the change applies only to that specific terminal window. Other terminal windows remain unaffected unless the environment is activated there as well. This session-specific behavior is intentional and allows different projects to be worked on simultaneously using different environments.\nBecause activation is terminal-specific, it is possible for two terminals on the same computer to behave differently at the same time. One terminal may be using a project’s virtual environment, while another is using the system-wide Python installation. Understanding this distinction helps explain why code may run successfully in one terminal but fail in another.\nVisual Studio Code builds on this behavior by integrating the terminal and the editor. When a project folder is opened in VS Code, the editor scans the folder structure to detect virtual environments. If an environment is found, VS Code can associate that environment with the project and use it when running Python scripts.\nVS Code’s Python tooling uses this association to determine which Python interpreter should be used for execution, linting, and debugging. When the correct environment is selected, running a script from the editor or from the integrated terminal uses the same Python configuration. This alignment reduces confusion and helps ensure consistent behavior across tools.\nAt this point, several concepts come together:\n- The terminal location determines which files are visible.\n- The active environment determines which Python interpreter and packages are used.\n- The execution command tells Python which script to run.\nAll three must align for code to run as expected. If any one of them is incorrect—wrong directory, wrong environment, or wrong command—errors can occur even if the code itself is correct.\nViewing these elements as a coordinated system rather than independent pieces makes troubleshooting much easier. Instead of guessing, it becomes possible to check each part in turn: where the terminal is, which environment is active, and which Python is being used. This systems-level understanding is key to working confidently with Python projects in both the terminal and Visual Studio Code.\n\n\n9.6 Common environment mistakes and recovery strategies\nMistakes involving virtual environments are extremely common and occur at all experience levels. These issues rarely indicate a problem with the code itself. Instead, they usually arise from a mismatch between the environment that was intended and the one that is actually being used.\nOne frequent mistake is installing packages in the wrong environment. This happens when packages are installed while the system-wide Python environment is active instead of the project’s virtual environment. As a result, a script may fail to import a package even though it appears to be installed. The package exists—but not in the environment the script is using.\nAnother common issue is running scripts with the wrong Python interpreter. Because multiple Python interpreters can exist on the same computer, it is possible to run a script using a different interpreter than expected. When this occurs, the script may behave inconsistently across terminals or tools, even though no changes were made to the code.\nIt is also easy to forget which environment is active, especially when switching between projects or terminal windows. Since activation is specific to each terminal session, opening a new terminal often means starting without any environment activated. This can lead to confusion when previously working code suddenly fails.\nRecovering from environment-related issues usually involves a small number of simple checks:\n- Confirm which environment is active in the current terminal.\n- Verify which Python interpreter is being used.\n- Ensure required packages are installed in that environment.\n- Re-activate the intended environment if necessary.\nThese steps are effective because they reestablish clarity about the execution context before any changes are made. Guessing or reinstalling packages repeatedly is rarely helpful without first confirming which environment is actually in use.\nMost importantly, these mistakes are routine and fixable. Even experienced developers encounter them regularly, particularly when working across multiple projects or tools. With practice, identifying environment issues becomes a quick diagnostic step rather than a source of frustration.\nVirtual environments are designed to make work more reliable, not more fragile. Once their role is understood, environment-related problems become signals to check context rather than obstacles to progress."
  },
  {
    "objectID": "textbook_src/week01_main.html#week-1-summary",
    "href": "textbook_src/week01_main.html#week-1-summary",
    "title": "Week 1 – What is Analytics & AI?",
    "section": "Week 1 Summary",
    "text": "Week 1 Summary\nThis chapter established the foundational ideas and workflows that will be used throughout the course. Conceptually, it introduced analytics as a progression from description to prescription, and positioned artificial intelligence as systems that extend analytics by embedding models into decision-making processes. Rather than treating analytics and AI as competing ideas, the chapter emphasized their overlap and the importance of understanding how model outputs are ultimately used.\nA central organizing framework was the decomposition of AI systems into data, models, and logic. This lens provides a practical way to reason about system behavior, diagnose failures, and understand why strong predictive performance alone does not guarantee good decisions. The discussion of AI paradigms—symbolic, statistical, neural, and hybrid—reinforced the idea that real-world systems are assembled from multiple approaches rather than built around a single technique.\nFrom a practical perspective, the chapter introduced the core mechanics required to work with Python in a professional, script-based workflow. You learned how Python scripts execute, how simple programs are constructed using printing, variables, and expressions, and why the edit → run → fix loop is the normal mode of development. These skills form the computational foundation for later analytical work.\nThe chapter also emphasized the importance of execution context. By introducing the terminal, file navigation, paths, and virtual environments, it clarified how Python code is located, executed, and isolated across projects. Rather than treating these tools as technical hurdles, the chapter framed them as mechanisms for control, predictability, and reproducibility.\nBy the end of Week 1, you should have both a conceptual framework for understanding analytics and AI systems and a practical workflow for running Python code reliably. These foundations will be built upon in subsequent chapters, where data analysis, modeling, and AI techniques become more sophisticated—but always within the same mental and operational structure introduced here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Analytics & AI",
    "section": "",
    "text": "Welcome to the online textbook for the course. It is organized to provide a clear, structured pathway through the foundational ideas and practical techniques of analytics and artificial intelligence. Each week’s chapter includes:\n\nAn explaination of the key/core concepts and our objectives for the week.\n\nDemonstrations (where applicable) and worked examples\n\nLinks to downloadable code associated with the topics in the lectures.\n\nPractice sections and applied exercises\n\nShort conceptual notes to reinforce key ideas\n\nThis site is designed to complement the in-class experience while remaining accessible for self-study. All materials can be viewed online and code files can be downloaded from the course repository. The code in github and on Canvas is the same. The data for this course is stored in Canvas."
  },
  {
    "objectID": "index.html#course-textbook",
    "href": "index.html#course-textbook",
    "title": "Foundations of Analytics & AI",
    "section": "",
    "text": "Welcome to the online textbook for the course. It is organized to provide a clear, structured pathway through the foundational ideas and practical techniques of analytics and artificial intelligence. Each week’s chapter includes:\n\nAn explaination of the key/core concepts and our objectives for the week.\n\nDemonstrations (where applicable) and worked examples\n\nLinks to downloadable code associated with the topics in the lectures.\n\nPractice sections and applied exercises\n\nShort conceptual notes to reinforce key ideas\n\nThis site is designed to complement the in-class experience while remaining accessible for self-study. All materials can be viewed online and code files can be downloaded from the course repository. The code in github and on Canvas is the same. The data for this course is stored in Canvas."
  },
  {
    "objectID": "index.html#how-to-use-this-textbook",
    "href": "index.html#how-to-use-this-textbook",
    "title": "Foundations of Analytics & AI",
    "section": "How to Use This Textbook",
    "text": "How to Use This Textbook\nThe material is organized into three types:\n\n1. Weekly Modules\nThese chapters follow the course schedule and lectures. The readings from the text are not identical to the lectures, but they are close. The readings introduce new concepts, connect them to analytics and AI practice, and provide examples to run independently.\nStart here:\n\nWeek 1 – What is Analytics & AI?\n\nWeek 2 – Python Basics\n\nWeek 3 – Data Structures & Pandas\n\nWeek 4 – From Data to Models\n\n\n\n\n\n2. Practical Guides (Task-Oriented Documentation)\nThese pages provide focused instructions for installing tools, configuring environments, and resolving common issues.\n\nHow to Install Python\n\nInstalling the Course requirements.txt\n\nUsing Google Colab\n\nThese guides remain relevant throughout the semester and may be updated as tools evolve."
  },
  {
    "objectID": "index.html#course-learning-philosophy",
    "href": "index.html#course-learning-philosophy",
    "title": "Foundations of Analytics & AI",
    "section": "Course Learning Philosophy",
    "text": "Course Learning Philosophy\nThis textbook is built around two related objectives:\n\nAI Literacy: developing conceptual understanding of modern analytics and AI systems. How they work, where they succeed, and where they sometimes fail.\n\nJob-Relevant Skills: Translating these concepts into practical, executable workflows using Python and contemporary tools.\n\nEach week reinforces both trajectories: one conceptual, one applied."
  },
  {
    "objectID": "index.html#accessing-code-and-data",
    "href": "index.html#accessing-code-and-data",
    "title": "Foundations of Analytics & AI",
    "section": "Accessing Code and Data",
    "text": "Accessing Code and Data\nAll example code for the course is maintained in the main GitHub repository:\n\nCode folder: code/ (available in GitHub and Canvas) Data folder: data/ (available in Canvas only)\n\nYou may download the entire repository as a .zip file or clone it directly.\nLinks to specific scripts and notebooks appear at the end of each week’s chapter."
  },
  {
    "objectID": "index.html#need-help",
    "href": "index.html#need-help",
    "title": "Foundations of Analytics & AI",
    "section": "Need Help?",
    "text": "Need Help?\nIf you encounter technical issues with installation, environments, or package setup, consult the Guides section using the navigation bar above. These pages provide step-by-step instructions tailored for this course."
  },
  {
    "objectID": "textbook_src/week02_main.html",
    "href": "textbook_src/week02_main.html",
    "title": "Week 2 – Python Foundations: Data, Control, and Functions",
    "section": "",
    "text": "How Python represents different kinds of data and why types matter.\nHow to work with text and structured collections of values.\nHow programs make decisions and repeat actions using control flow.\nHow functions help organize and reuse code.\nHow to read Python error messages and debug common problems.\n\n\n\n\n\nMultiple Python scripts demonstrating data types, data structures, and control flow.\nFunctions that encapsulate reusable logic.\nDebugged versions of scripts that initially contain errors.\n\n\n\n\nBy the end of this week, you should be able to:\n- Explain how data representation affects program behavior.\n- Recognize how structured data enables more complex reasoning in programs.\n- Describe how control flow supports conditional and repeated decision-making.\n- Explain why modular design (functions) matters for scalable systems.\n\n\n\nBy the end of this week, you should be able to:\n- Work with core Python data types: numbers, strings, and booleans.\n- Manipulate lists and dictionaries to store and retrieve data.\n- Write conditional logic using if/else statements.\n- Use for and while loops to iterate over data.\n- Define and call functions to organize code. - Interpret Python error messages and apply basic debugging strategies."
  },
  {
    "objectID": "textbook_src/week02_main.html#week-2-overview",
    "href": "textbook_src/week02_main.html#week-2-overview",
    "title": "Week 2 – Python Foundations: Data, Control, and Functions",
    "section": "",
    "text": "How Python represents different kinds of data and why types matter.\nHow to work with text and structured collections of values.\nHow programs make decisions and repeat actions using control flow.\nHow functions help organize and reuse code.\nHow to read Python error messages and debug common problems.\n\n\n\n\n\nMultiple Python scripts demonstrating data types, data structures, and control flow.\nFunctions that encapsulate reusable logic.\nDebugged versions of scripts that initially contain errors.\n\n\n\n\nBy the end of this week, you should be able to:\n- Explain how data representation affects program behavior.\n- Recognize how structured data enables more complex reasoning in programs.\n- Describe how control flow supports conditional and repeated decision-making.\n- Explain why modular design (functions) matters for scalable systems.\n\n\n\nBy the end of this week, you should be able to:\n- Work with core Python data types: numbers, strings, and booleans.\n- Manipulate lists and dictionaries to store and retrieve data.\n- Write conditional logic using if/else statements.\n- Use for and while loops to iterate over data.\n- Define and call functions to organize code. - Interpret Python error messages and apply basic debugging strategies."
  },
  {
    "objectID": "textbook_src/week02_main.html#python-basics-variables-and-data-types",
    "href": "textbook_src/week02_main.html#python-basics-variables-and-data-types",
    "title": "Week 2 – Python Foundations: Data, Control, and Functions",
    "section": "1. Python Basics: Variables and Data Types",
    "text": "1. Python Basics: Variables and Data Types\nThis section revisits variables and introduces Python’s core data types. The goal is to move from thinking of variables as simple containers to understanding how data type influences program behavior, interpretation, and outcomes.\n\n1.1 Variables revisited\nA variable in Python is a name that refers to a value. When a variable is created, Python associates that name with an object stored in memory. This association can change over time as the program runs.\nOne important idea to reinforce is that variables are not fixed containers. They are labels that can be reassigned to different values:\nx = 5\nx = 10\nprint(x)\nAfter this code runs, the value associated with x is 10, not 5. The original value is no longer referenced by x. This ability to reassign variables is what allows programs to update state, respond to inputs, and evolve during execution.\nVariables also help make programs easier to understand. Using meaningful variable names communicates intent and reduces the need to mentally track raw values. As programs become more complex, this clarity becomes increasingly important.\nAt this stage, the key takeaway is that variables: - bind names to values, - can be reassigned, - and allow programs to remember and update information as they run.\n\n\n\n1.2 Numbers and numeric behavior\nPython supports several numeric data types, the most common being integers (whole numbers) and floating-point numbers (numbers with decimal points). While they may look similar, they behave differently in certain operations.\nFor example:\na = 10\nb = 3\nprint(a / b)\nEven though both a and b are integers, the result of division is a floating-point number. Python automatically determines the appropriate type based on the operation being performed.\nThis automatic behavior is convenient, but it also means that numeric results may not always be what you expect at first glance. Floating-point numbers, in particular, can introduce small rounding effects due to how they are represented internally.\nAt this level, the most important ideas are:\n- Python distinguishes between integers and floats,\n- arithmetic operations can change types,\n- and numeric behavior is governed by both the values and the operation.\nUnderstanding these basics helps prevent confusion later when numeric results appear slightly different than anticipated.\n\n\n\n1.3 Strings as data\nA string is a sequence of characters used to represent text. Strings are created by enclosing characters in quotation marks:\ntext = \"analytics\"\nStrings behave differently from numbers. Although they may look similar when printed, Python treats them as text rather than quantities. Strings can be indexed, meaning individual characters can be accessed by position:\nprint(text[0])\nHere, Python returns the first character of the string. Indexing begins at zero, which is a common convention in programming.\nAn important property of strings is that they are immutable. Once a string is created, its individual characters cannot be changed. Operations on strings create new strings rather than modifying existing ones.\nStrings are used extensively in analytics and AI workflows to represent labels, categories, identifiers, and unstructured text. Treating strings as data rather than just “things to print” is an important conceptual shift.\n\n\n\n1.4 Booleans and logical values\nA boolean represents one of two logical values: True or False. Booleans often arise from comparisons, where Python evaluates whether a statement is correct:\nx = 5\nprint(x &gt; 3)\nIn this example, the comparison produces a boolean result. Booleans are fundamental to decision-making in programs because they control whether certain code paths are executed.\nAlthough booleans are simple, they play a central role in program logic. They act as the bridge between data and behavior, determining how a program responds to different conditions.\nLater sections will build on booleans to introduce conditional logic and loops. For now, the key idea is that booleans encode yes/no decisions that programs can act upon.\n\n\n\n1.5 Type behavior and common surprises\nPython uses dynamic typing, meaning that variable types are determined at runtime rather than declared in advance. A variable can be reassigned to a value of a different type without error:\nx = 5\nx = \"five\"\nprint(x)\nThis flexibility is powerful, but it can also lead to surprises—especially when mixing types in expressions. For example:\nprint(\"5\" + \"5\")\nprint(5 + 5)\nAlthough both lines use the + operator, they behave differently. In the first case, Python performs string concatenation. In the second, it performs numeric addition. The operator’s meaning depends on the data types involved.\nThese behaviors are not bugs; they are consistent rules applied by Python. Understanding them requires paying attention to type, not just appearance.\nCommon beginner surprises often come from assuming that values that look similar behave the same way. Developing the habit of asking “what type is this?” is one of the most effective ways to reason about Python programs and avoid errors."
  },
  {
    "objectID": "textbook_src/week02_main.html#working-with-strings",
    "href": "textbook_src/week02_main.html#working-with-strings",
    "title": "Week 2 – Python Foundations: Data, Control, and Functions",
    "section": "2. Working with Strings",
    "text": "2. Working with Strings\nStrings are one of the most common data types used in Python. They represent text, but they are also structured objects that can be indexed, sliced, transformed, and formatted. In analytics and AI contexts, strings are frequently used to represent labels, categories, identifiers, and unstructured text, making it important to understand how to work with them beyond simple printing.\nThis section builds fluency in treating strings as data rather than as static text.\n\n\n2.1 String creation and indexing\nA string is created by enclosing characters in quotation marks. Python allows both single and double quotation marks, as long as they are used consistently:\ntext = \"analytics\"\nlabel = 'AI'\nOnce created, a string behaves like a sequence of characters. This means each character has a position, known as an index. Python uses zero-based indexing, so the first character is at position 0, the second at position 1, and so on.\nCharacters can be accessed using square brackets:\ntext = \"analytics\"\nprint(text[0])\nprint(text[1])\nPython also allows negative indexing, which counts from the end of the string. An index of -1 refers to the last character:\nprint(text[-1])\nIndexing allows programs to inspect or extract specific parts of a string, but it must be done carefully. Attempting to access an index that does not exist results in an error. This reinforces the idea that strings have a fixed length and well-defined boundaries.\nAt a conceptual level, indexing answers the question: Which character is at this position in the string?\n\n\n\n2.2 String slicing\nWhile indexing accesses a single character, slicing extracts a range of characters from a string. A slice is specified using a start position and an end position. The start position is included, while the end position is excluded.\nFor example:\ntext = \"analytics\"\nprint(text[0:4])\nThis slice returns the characters at positions 0 through 3. Python also allows either the start or end to be omitted, which defaults to the beginning or end of the string:\nprint(text[:4])\nprint(text[4:])\nSlicing always returns a new string. The original string remains unchanged, which reflects the immutable nature of strings.\nSlicing is often safer and more expressive than manual indexing, especially when working with variable-length strings. Rather than counting exact positions, slices allow code to express intent more clearly, such as “everything before this point” or “everything after that point.”\nAt a conceptual level, slicing answers the question: Which portion of this string do I want to work with?\n\n\n\n2.3 Common string operations and methods\nPython provides many built-in operations and methods for working with strings. These allow programs to measure, transform, and search text without modifying the original string.\nOne common operation is measuring the length of a string:\ntext = \"Analytics and AI\"\nprint(len(text))\nOther common operations involve transforming the string, such as changing letter case:\nprint(text.lower())\nprint(text.upper())\nStrings can also be searched or modified using methods that return new strings:\nprint(text.replace(\"AI\", \"analytics\"))\nThese methods do not alter the original string. Instead, they produce a new string with the requested changes applied.\nBecause strings are immutable, all transformations result in new objects. This behavior is consistent and predictable, but it also means that results must be captured in variables if they are needed later.\nString operations are widely used in data cleaning, labeling, and preprocessing tasks. Understanding how these methods behave helps prevent subtle bugs and makes string manipulation more intentional.\n\n\n\n2.4 Formatting strings for output\nIn most programs, strings are not used in isolation. They are combined with variables to produce readable output for users, logs, or reports. String formatting is the process of embedding variable values into text.\nOne common and modern approach is the use of formatted strings, often called f-strings. These allow variables to be inserted directly into a string:\nname = \"Jordan\"\nscore = 92\nprint(f\"{name} scored {score} points\")\nFormatted strings improve readability by keeping text and values together in a single expression. They also reduce the need for manual concatenation, which can become error-prone as output becomes more complex.\nFormatting matters because output is often how results are interpreted by humans. Clear, well-formatted strings make it easier to understand what a program has done and what its results mean.\nAt a conceptual level, string formatting answers the question: How should this information be presented so it is easy to read and understand?"
  },
  {
    "objectID": "textbook_src/week02_main.html#data-structures-lists",
    "href": "textbook_src/week02_main.html#data-structures-lists",
    "title": "Week 2 – Python Foundations: Data, Control, and Functions",
    "section": "3. Data Structures: Lists",
    "text": "3. Data Structures: Lists\nLists allow programs to work with collections of values rather than single items. Instead of storing one number, one string, or one result at a time, lists make it possible to store many related values together and treat them as a unit. This capability is essential for working with real-world data, where information almost always comes in groups.\nThis section introduces lists as a natural extension of variables and strings, focusing on how they are created, accessed, and modified.\n\n\n3.1 What a list is and when to use one\nA list is an ordered collection of values. Each value in the list occupies a position, and those positions are preserved. This ordering allows values to be accessed, updated, and processed in a predictable way.\nLists are useful whenever a program needs to work with:\n- multiple related values,\n- values that should be processed together,\n- or data whose size may change over time.\nFor example, a list can represent a set of scores, a group of names, or a sequence of measurements. Rather than creating separate variables for each value, a list groups them into a single structure.\nnumbers = [1, 2, 3, 4]\nConceptually, lists answer the question: How do I represent many values as one thing? This makes lists a foundational structure for iteration, aggregation, and analysis later in the course.\n\n\n\n3.2 Creating lists\nLists are created using square brackets, with individual values separated by commas. A list can contain values of the same type or a mixture of different types.\nnames = [\"Alice\", \"Bob\", \"Charlie\"]\nmixed = [1, \"two\", True]\nAlthough Python allows mixed-type lists, using a consistent type within a list often improves readability and reduces confusion. For example, a list of numbers or a list of strings communicates intent more clearly than a list with unrelated values.\nLists can also be created empty and filled later as the program runs. This pattern is especially useful when values are generated dynamically.\nresults = []\nAt a conceptual level, creating a list establishes a container whose contents can grow, shrink, or change as the program executes.\n\n\n\n3.3 Indexing and slicing lists\nLike strings, lists are sequences, which means their elements are accessed using zero-based indexing. The first element is at index 0, the second at index 1, and so on.\nvalues = [10, 20, 30, 40, 50]\nprint(values[0])\nNegative indexing also works with lists, allowing access from the end:\nprint(values[-1])\nLists support slicing, which extracts a portion of the list and returns a new list containing those elements.\nprint(values[1:4])\nSlicing behavior for lists closely mirrors slicing for strings, but instead of returning a string, Python returns a list. The original list remains unchanged.\nIndexing and slicing allow programs to focus on specific elements or subsets of data, which is especially useful when analyzing or transforming collections.\n\n\n\n3.4 Modifying list contents\nOne key difference between lists and strings is that lists are mutable. This means their contents can be changed after the list is created.\nIndividual elements can be updated by assigning a new value to a specific index:\nvalues = [10, 20, 30]\nvalues[1] = 25\nElements can also be added to a list. A common pattern is appending new values to the end:\nvalues.append(40)\nRemoving elements is also possible, either by value or by position. Because lists can change over time, they are well suited for workflows where data is accumulated, filtered, or updated incrementally.\nMutability is powerful, but it also means that changes to a list affect all parts of the program that reference it. Understanding when and how list contents change is critical for reasoning about program behavior.\n\n\n\n3.5 Common list patterns and mistakes\nLists are frequently used in patterns that involve building up data step by step. A common approach is to start with an empty list and add values as they are produced.\nresults = []\nresults.append(10)\nresults.append(20)\nOne common mistake involves indexing errors, such as attempting to access an index that does not exist. These errors often arise from forgetting that indexing starts at zero or miscounting list length.\nAnother frequent source of confusion involves references. When one variable is assigned to another, both names may refer to the same list rather than creating a copy.\na = [1, 2, 3]\nb = a\nb.append(4)\nIn this example, both a and b refer to the same list. Modifying the list through one variable affects the other. This behavior is consistent but can be surprising if it is not anticipated.\nDeveloping a habit of thinking carefully about list creation, modification, and referencing helps prevent subtle bugs. Lists are powerful tools, but they require attention to how and when data changes."
  },
  {
    "objectID": "textbook_src/week02_main.html#data-structures-dictionaries",
    "href": "textbook_src/week02_main.html#data-structures-dictionaries",
    "title": "Week 2 – Python Foundations: Data, Control, and Functions",
    "section": "4. Data Structures: Dictionaries",
    "text": "4. Data Structures: Dictionaries\nDictionaries provide a way to store structured, labeled data. While lists organize values by position, dictionaries organize values by meaning. This makes dictionaries especially useful for representing real-world entities, records, and attributes where each value has a clear label.\nThis section introduces dictionaries as a complementary data structure to lists, emphasizing when dictionaries are the better choice and how they support clearer, more expressive programs.\n\n\n4.1 Why dictionaries exist\nLists are effective when values are naturally ordered and accessed by position. However, many real-world data problems are not about position—they are about association. For example, a person is not best described by “the first value, the second value, and the third value,” but by attributes such as name, age, or location.\nDictionaries exist to solve this problem. A dictionary allows values to be accessed using keys that describe what the values represent. This makes programs easier to read and reason about, especially as the number of attributes grows.\nperson = {\"name\": \"Alex\", \"age\": 30}\nIn this example, each value is paired with a descriptive label. The dictionary structure makes it immediately clear what each value represents, without relying on positional knowledge.\nConceptually, dictionaries answer the question: How do I store related pieces of information under meaningful names? This makes them a natural choice for representing records, configurations, and structured inputs.\n\n\n\n4.2 Key–value pairs\nA dictionary is composed of key–value pairs. The key acts as an identifier, and the value is the data associated with that identifier. Together, they form a mapping from meaning to data.\nrecord = {\"city\": \"Gainesville\", \"state\": \"FL\"}\nKeys are typically strings, although Python allows other immutable types to be used as keys. The important property is that keys must be unique within a dictionary. Each key identifies exactly one value.\nAccessing data through keys is fundamentally different from indexing into a list. Instead of asking “what is at position 0?” the program asks “what is the value associated with this label?” This shift from positional access to semantic access improves clarity and reduces errors.\nAt a conceptual level, key–value pairs encode relationships: this label corresponds to that value. This relational structure is central to many analytics and AI workflows.\n\n\n\n4.3 Creating and accessing dictionaries\nDictionaries are created using curly braces, with key–value pairs separated by commas. Each key is followed by a colon and its associated value.\nscores = {\"math\": 90, \"history\": 85}\nValues are accessed by referencing the key:\nprint(scores[\"math\"])\nIf the specified key exists, Python returns the associated value. If the key does not exist, Python raises an error. This behavior reinforces the idea that keys define the valid structure of the data.\nDictionary access is explicit and intentional. Unlike lists, where accessing an invalid index might result from a miscount, accessing a dictionary with a missing key usually indicates a mismatch between what the program expects and what the data contains.\nUnderstanding how dictionaries are created and accessed helps establish a clear mental model of structured data: keys define what can be asked, and values define what is returned.\n\n\n\n4.4 Updating and iterating over dictionaries\nDictionaries are mutable, meaning their contents can be changed after creation. Existing values can be updated by assigning a new value to an existing key:\nscores[\"math\"] = 92\nNew key–value pairs can also be added dynamically:\nscores[\"science\"] = 88\nThis flexibility allows dictionaries to evolve as a program runs, making them well suited for tasks where information is accumulated, updated, or refined over time.\nDictionaries are often iterated over to process their contents. While full loop syntax is introduced later, it is important to recognize that dictionaries can be traversed by their keys, values, or key–value pairs. This makes it possible to perform operations across structured data in a systematic way.\nBecause dictionaries are mutable and often shared across a program, changes to a dictionary affect all references to it. This behavior is powerful but requires careful reasoning about when and where updates occur.\n\n\n\n4.5 Structured data with dictionaries\nOne of the most common uses of dictionaries is to represent records—collections of related attributes that describe a single entity. For example, a dictionary can represent a student, a transaction, or a configuration setting.\nstudent = {\"name\": \"Alex\", \"score\": 90}\nWhen working with multiple records, it is common to use lists of dictionaries, where each dictionary represents one structured item:\nstudents = [\n    {\"name\": \"Alex\", \"score\": 90},\n    {\"name\": \"Jordan\", \"score\": 85}\n]\nThis pattern bridges the gap between simple Python data structures and more advanced representations such as tables, data frames, and JSON objects. It is widely used in data processing, APIs, and machine learning pipelines.\nDictionaries matter because they allow programs to work with data in a way that mirrors real-world structure. Instead of relying on position or implicit meaning, dictionaries make relationships explicit. This clarity becomes increasingly important as programs grow in size and complexity."
  },
  {
    "objectID": "textbook_src/week02_main.html#control-flow-conditional-logic",
    "href": "textbook_src/week02_main.html#control-flow-conditional-logic",
    "title": "Week 2 – Python Foundations: Data, Control, and Functions",
    "section": "5. Control Flow: Conditional Logic",
    "text": "5. Control Flow: Conditional Logic\nConditional logic allows programs to make decisions. Without conditional logic, a program would execute the same instructions in the same order every time, regardless of what data it receives. Real programs do not work that way. They respond differently depending on inputs, context, and state.\nThis section introduces conditional logic as a fundamental mechanism for decision-making in programs.\n\n\n5.1 Why programs need branching\nMany early programming examples look like calculators: values go in, a result comes out, and the program ends. That is useful for learning syntax, but it does not capture how most programs behave in practice.\nReal programs must handle situations where different cases require different actions. For example:\n- If a user enters invalid input, the program should respond differently than if the input is valid.\n- If a customer’s risk score is high, the program may trigger review, while a low score may be ignored.\n- If a file exists, the program may load it; otherwise it may create a new one.\nThis is the purpose of branching. Branching means that a program can follow one of multiple paths depending on a condition.\nConditional logic is also the foundation for many forms of decision logic in analytics and AI systems. Models may produce scores or probabilities, but something still has to decide what happens next. That decision is often implemented using thresholds and conditional statements. Even simple branching logic is therefore a core building block for larger systems.\n\n\n\n5.2 If, elif, and else\nIn Python, branching is expressed using the if statement. An if statement evaluates a condition. If the condition is true, Python executes the block of code under the if. If the condition is false, Python skips that block.\nA minimal example looks like this:\nx = 10\nif x &gt; 5:\n    print(\"Large value\")\nThe condition is x &gt; 5. Python evaluates it as either true or false. If it is true, the indented block runs.\nOften, programs need to handle more than two cases. Python supports this using elif (short for “else if”). The program checks conditions in order, and the first condition that evaluates to true determines which block runs.\nscore = 85\nif score &gt;= 90:\n    print(\"A\")\nelif score &gt;= 80:\n    print(\"B\")\nelse:\n    print(\"Below B\")\nIn this structure: - if checks the first condition. - elif checks additional conditions only if earlier ones were false. - else acts as a fallback when no conditions were met.\nA useful way to read this is: “check the first condition; if it fails, check the next; if all fail, use the default.”\n\n\n\n5.3 Boolean expressions in conditions\nConditions in if statements are boolean expressions. A boolean expression is any expression that evaluates to either True or False.\nThe most common boolean expressions are comparisons. Python supports standard comparison operators such as:\n- greater than and less than\n- equality and inequality\n- greater than or equal to, less than or equal to\nFor example:\nx = 5\nprint(x == 5)\nprint(x &gt; 10)\nThese expressions evaluate to booleans. That is why they can be used in conditions: the if statement is ultimately asking a yes/no question.\nA common source of confusion is that conditions often look like natural language, but they are not. They are strict expressions that must evaluate cleanly to true or false. Small mistakes in a condition can change program behavior dramatically, so it is important to write conditions clearly and test them when needed.\nAt this stage, the key idea is simple: conditional logic works because Python evaluates conditions as booleans and then decides which block of code to execute.\n\n\n\n5.4 Nested conditionals and readability\nA nested conditional is an if statement inside another if statement. Nesting is sometimes necessary when decisions depend on multiple layers of conditions.\nage = 20\nif age &gt;= 18:\n    if age &lt; 21:\n        print(\"Adult, but under 21\")\nThis structure expresses two related conditions:\n- The outer condition checks whether the person is an adult.\n- The inner condition refines the case for adults under 21.\nNesting can be useful, but it can also reduce readability if overused. Deeply nested logic is harder to follow, harder to debug, and easier to misunderstand. As conditional logic becomes more complex, readability becomes a design concern, not just a style preference.\nTwo practical habits help keep conditionals readable:\n- Use clear conditions that communicate intent.\n- Avoid unnecessary nesting when a simpler structure is possible.\nEven when nesting is appropriate, indentation must be treated as part of the logic. In Python, indentation is not cosmetic; it determines which code belongs to which branch. Learning to visually interpret indentation is therefore part of learning how conditional logic works."
  },
  {
    "objectID": "textbook_src/week02_main.html#control-flow-loops",
    "href": "textbook_src/week02_main.html#control-flow-loops",
    "title": "Week 2 – Python Foundations: Data, Control, and Functions",
    "section": "6. Control Flow: Loops",
    "text": "6. Control Flow: Loops\nLoops allow programs to repeat actions systematically. Instead of writing the same instruction multiple times, loops provide a way to apply the same logic across collections of data or across repeated conditions. This ability to repeat is essential for working with real data, where the number of values is often large or unknown in advance.\nThis section introduces looping as a fundamental control structure and emphasizes careful reasoning about how and when repetition occurs.\n\n\n6.1 Why repetition matters\nWithout loops, programs would be limited to one-time calculations. Any task that involved processing multiple values would require duplicated code, which is inefficient, error-prone, and difficult to maintain.\nMany real-world tasks involve repetition:\n- Checking each item in a list\n- Processing each record in a dataset\n- Repeating an action until a condition is met\n- Accumulating results across many values\nLoops make these tasks possible by allowing a program to apply the same operation repeatedly, while only writing the logic once.\nConceptually, loops are closely tied to data structures. When data is stored in collections such as lists or dictionaries, loops provide the mechanism to visit each element in turn. This connection between structure (data) and behavior (loops) is central to programming and analytics.\nAt a high level, loops answer the question:\nHow do I apply the same logic across many values or over time?\n\n\n\n6.2 For loops and iteration patterns\nA for loop is used when a program needs to iterate over a collection of values. The loop variable takes on each value in the collection, one at a time, and the loop body runs once for each value.\nvalues = [10, 20, 30]\nfor v in values:\n    print(v)\nThis loop can be read as: “for each value in the list, print the value.” The loop variable v is assigned a new value on each iteration.\nFor loops work naturally with sequences such as lists and strings. The number of iterations is determined by the size of the collection, which makes for loops predictable and easy to reason about.\nA helpful habit is to read for loops out loud in plain language. Doing so reinforces the intent of the loop and reduces confusion about what the code is doing.\nFor loops are especially useful when:\n- the collection is known,\n- every element should be processed,\n- and the order of processing matters.\n\n\n\n6.3 While loops and termination conditions\nA while loop repeats as long as a condition remains true. Instead of iterating over a collection, a while loop continues until a specific condition changes.\ncount = 0\nwhile count &lt; 3:\n    print(count)\n    count = count + 1\nThis loop runs while the condition count &lt; 3 is true. Each iteration updates count, and eventually the condition becomes false, causing the loop to stop.\nWhile loops are useful when:\n- the number of iterations is not known in advance,\n- repetition depends on a changing condition,\n- or the loop should continue until some state is reached.\nThe most important concept with while loops is termination. Every while loop must include logic that eventually makes the condition false. Without this, the loop will continue indefinitely.\nConceptually, while loops answer the question:\nShould I keep going right now?\n\n\n\n6.4 Common looping errors\nLoops are powerful, but they also introduce common sources of error. One of the most frequent mistakes is creating an infinite loop, where the condition never becomes false.\ncount = 0\nwhile count &lt; 3:\n    print(count)\n    # count is never updated\nIn this example, the condition remains true forever because count never changes. Infinite loops often occur when a loop variable is not updated or when the termination condition is incorrect.\nAnother common issue is off-by-one errors, where a loop runs one time too many or one time too few. These errors often arise from misunderstandings about starting values, ending conditions, or zero-based indexing.\nLoops can also behave unexpectedly when data is modified while being iterated over. Changing a list while looping through it can lead to skipped values or unintended behavior, which is why careful reasoning about loop structure is important.\nWhen debugging loops, a useful strategy is to:\n- trace the loop step by step,\n- track how loop variables change,\n- and verify exactly when the condition becomes false.\n\n\n\n6.5 Choosing between for and while\nBoth for and while loops enable repetition, but they serve different purposes. Choosing between them is a matter of intent, not preference.\nA for loop is usually the better choice when:\n- iterating over a known collection,\n- applying logic to each element,\n- or when the number of iterations is determined by the data.\nA while loop is usually the better choice when: - repetition depends on a condition, - the number of iterations is not known ahead of time, - or the loop should stop based on changing state.\n# for loop: known collection\nfor x in [1, 2, 3]:\n    print(x)\n\n# while loop: condition-based repetition\ncount = 0\nwhile count &lt; 3:\n    print(count)\n    count = count + 1\nIn many cases, either loop could be used, but one will communicate intent more clearly than the other. Readability and correctness are more important than cleverness.\nUnderstanding when and why to use each type of loop makes programs easier to understand, debug, and extend. Loops are not just a syntactic feature—they are a way of expressing repeated reasoning in code."
  },
  {
    "objectID": "textbook_src/week02_main.html#functions-and-modular-design",
    "href": "textbook_src/week02_main.html#functions-and-modular-design",
    "title": "Week 2 – Python Foundations: Data, Control, and Functions",
    "section": "7. Functions and Modular Design",
    "text": "7. Functions and Modular Design\nAs programs grow, writing code from top to bottom becomes difficult to manage. Logic is repeated, scripts become long, and small changes require edits in multiple places. Functions address this problem by allowing programs to be broken into reusable, named units of logic.\nThis section introduces functions not just as a Python feature, but as a way of thinking about program design. Functions help manage complexity, improve readability, and make programs easier to reason about and extend.\n\n\n7.1 Why functions exist\nOne of the earliest signs that a program needs functions is repetition. When the same logic appears multiple times, it becomes harder to maintain and easier to introduce errors.\nprint(\"Processing value\")\nprint(\"Processing value\")\nIf the message or behavior needs to change, every repeated instance must be updated. Functions solve this by allowing logic to be written once and reused many times.\nFunctions also support abstraction. By giving a block of code a meaningful name, the details of how something is done can be hidden behind what it does. This allows programs to be read at a higher level, focusing on intent rather than mechanics.\nConceptually, functions answer the question:\nHow can I name and reuse a piece of behavior?\nThis idea scales from small scripts to large systems. In analytics and AI workflows, functions often represent steps in a pipeline, transformations applied to data, or decisions applied consistently across many values.\n\n\n\n7.2 Defining a function\nA function is defined using the def keyword, followed by the function name, parentheses, and a colon. The body of the function is indented beneath the definition.\ndef greet():\n    print(\"Hello\")\nThis code defines a function named greet, but it does not execute it. Defining a function tells Python what the function does, not when it should run.\nThe function body contains the instructions that will be executed whenever the function is called. Indentation is critical here: it determines which statements belong to the function.\nFunction names should be descriptive and reflect behavior. A well-named function makes code easier to read, especially when functions are combined into larger programs.\nA useful way to read a function definition is:\n“Define a function called greet that performs these actions.”\n\n\n\n7.3 Calling functions\nA function runs only when it is called. Calling a function means telling Python to execute the instructions inside the function body.\ngreet()\ngreet()\nEach time the function is called, the same block of code runs. After the function finishes executing, control returns to the point where it was called.\nThis separation between definition and execution is essential. It allows functions to be defined once and used many times, in different parts of a program, or under different conditions.\nUnderstanding the difference between defining a function and calling a function is one of the most important conceptual steps in learning Python. Many early errors come from assuming that defining a function automatically runs it.\n\n\n\n7.4 Parameters and return values\nFunctions become more powerful when they can accept inputs and produce outputs. Inputs are specified using parameters, and outputs are produced using return values.\ndef add(a, b):\n    return a + b\nIn this example, a and b are parameters. When the function is called, concrete values—called arguments—are passed in.\nresult = add(2, 3)\nThe function computes a value and returns it to the caller. The returned value can then be stored in a variable, used in expressions, or passed to other functions.\nReturning values is often preferable to printing results inside functions. Printing is useful for communication with a user, but return values allow functions to be composed and reused as part of larger computations.\nConceptually, a function with parameters and a return value represents a transformation: it takes inputs, applies logic, and produces an output.\n\n\n\n7.5 Functions as building blocks\nAs programs grow, functions act as building blocks that can be combined with loops, conditionals, and data structures. Each function handles a specific task, making the overall program easier to understand.\ndef is_passing(score):\n    return score &gt;= 70\nThis function encodes a simple rule. Instead of repeating the condition score &gt;= 70 throughout a program, the logic is captured once and reused wherever needed.\nFunctions also improve testability and debugging. When behavior is isolated inside a function, it can be tested independently. If something goes wrong, the scope of the problem is smaller and easier to locate.\nAt a higher level, functions support modular design. Programs can be understood as collections of interacting functions, each with a clear purpose. This mirrors how larger analytics and AI systems are structured, where components are designed to do one thing well and interact through well-defined interfaces.\nThinking in terms of functions encourages a shift from writing code that merely works to writing code that is understandable, maintainable, and scalable."
  },
  {
    "objectID": "textbook_src/week02_main.html#debugging-fundamentals",
    "href": "textbook_src/week02_main.html#debugging-fundamentals",
    "title": "Week 2 – Python Foundations: Data, Control, and Functions",
    "section": "8. Debugging Fundamentals",
    "text": "8. Debugging Fundamentals\nDebugging is the process of identifying, understanding, and correcting problems in code. It is not a special activity reserved for advanced programmers, nor is it a sign that something has gone wrong in learning. Debugging is how programming actually happens.\nThis section reframes errors as information and introduces debugging as a systematic reasoning process, not a guessing game.\n\n\n8.1 What an error message is telling you\nWhen Python encounters a problem it cannot resolve, it produces an error message. That message is Python’s way of explaining what it expected, what it encountered instead, and where the problem occurred.\nAn error message is not a judgment about your ability. It is a report. Learning to debug begins with learning to read error messages rather than avoiding them.\nSome errors occur before the program runs at all. These are typically syntax errors, where Python cannot understand the structure of the code. For example:\nprint(\"Hello\"\nIn this case, Python reaches the end of the line and realizes something is missing. The error message points to the location where Python became confused.\nOther errors occur while the program is running. These runtime errors indicate that Python understood the code structurally, but something went wrong during execution.\nAt a high level, error messages answer three questions:\n- Where did the problem occur?\n- What kind of problem was it?\n- What was Python trying to do at the time?\nDebugging begins by treating error messages as clues rather than obstacles.\n\n\n\n8.2 Reading a traceback from top to bottom\nWhen a runtime error occurs, Python often produces a traceback. A traceback shows the sequence of steps Python followed before encountering the error.\ndef divide(a, b):\n    return a / b\n\ndivide(10, 0)\nThe traceback lists function calls from top to bottom, ending with the line where the error actually occurred. While the traceback may look intimidating at first, it follows a consistent structure.\nA useful strategy is to:\n- skim the traceback to understand the context,\n- then focus on the last line, which usually describes the actual error.\nThe file name and line number tell you where Python was executing when the problem occurred. This information narrows the search space and prevents unnecessary changes elsewhere in the code.\nOver time, reading tracebacks becomes a skill. Instead of seeing a wall of text, you begin to recognize patterns and quickly identify the relevant information.\n\n\n\n8.3 Common beginner errors\nCertain errors appear frequently when learning Python. These errors are predictable and shared by almost everyone at this stage.\nSyntax errors occur when Python cannot parse the code structure. Missing quotation marks, parentheses, or colons are common causes.\nName errors occur when a variable or function is used before it has been defined. This often results from spelling mistakes or assumptions about what exists in the current scope.\nType errors occur when an operation is applied to incompatible data types. These errors highlight the importance of understanding how different types behave.\nIndex and key errors occur when attempting to access elements that do not exist.\nvalues = [1, 2, 3]\nprint(values[3])\nThis error does not mean the list is broken. It means the program asked for something outside the valid range.\nRecognizing these errors as categories rather than isolated failures helps reduce frustration and speeds up debugging.\n\n\n\n8.4 Debugging as a systematic process\nEffective debugging is not about trying random fixes. It is about narrowing the problem space and testing assumptions deliberately.\nA systematic debugging process usually involves:\n- reproducing the error consistently,\n- identifying the exact location of failure,\n- inspecting the values of variables at that point,\n- and changing one thing at a time.\nOne of the simplest and most effective debugging tools is printing intermediate values.\nprint(\"Current value:\", x)\nBy inspecting program state as it runs, it becomes easier to understand how data flows through the code and where expectations diverge from reality.\nChanging multiple things at once makes debugging harder. When only one change is made, its effect is easier to interpret.\nDebugging rewards patience and methodical thinking. The goal is not to fix the error as quickly as possible, but to understand why the error occurred.\n\n\n\n8.5 When to fix code vs. rethink logic\nNot all bugs are caused by incorrect syntax or misuse of language features. Sometimes the code runs exactly as written, but the result is still wrong. In these cases, the issue lies in the logic, not the implementation.\ndef is_valid(score):\n    return score &gt; 100\nThis function works exactly as defined, but the condition may not reflect the intended rule. Debugging logic errors requires stepping back and examining assumptions.\nA useful question to ask is: Is Python doing something unexpected, or is Python doing exactly what I told it to do?\nIf the latter is true, the solution may involve redesigning conditions, rethinking data structures, or clarifying the problem definition rather than fixing syntax.\nDebugging is therefore part of program design. It is how understanding improves over time. Learning when to adjust code and when to rethink logic is one of the most valuable skills developed in this course."
  },
  {
    "objectID": "textbook_src/week02_main.html#debugging-fundamentals-1",
    "href": "textbook_src/week02_main.html#debugging-fundamentals-1",
    "title": "Week 2 – Python Foundations: Data, Control, and Functions",
    "section": "8. Debugging Fundamentals",
    "text": "8. Debugging Fundamentals\nDebugging is the process of identifying, understanding, and correcting problems in code. It is not a special activity reserved for advanced programmers, nor is it a sign that something has gone wrong in learning. Debugging is how programming actually happens.\nThis section reframes errors as information and introduces debugging as a systematic reasoning process, not a guessing game.\n\n\n8.1 What an error message is telling you\nWhen Python encounters a problem it cannot resolve, it produces an error message. That message is Python’s way of explaining what it expected, what it encountered instead, and where the problem occurred.\nAn error message is not a judgment about your ability. It is a report. Learning to debug begins with learning to read error messages rather than avoiding them.\nSome errors occur before the program runs at all. These are typically syntax errors, where Python cannot understand the structure of the code. For example:\nprint(\"Hello\"\nIn this case, Python reaches the end of the line and realizes something is missing. The error message points to the location where Python became confused.\nOther errors occur while the program is running. These runtime errors indicate that Python understood the code structurally, but something went wrong during execution.\nAt a high level, error messages answer three questions:\n- Where did the problem occur?\n- What kind of problem was it?\n- What was Python trying to do at the time?\nDebugging begins by treating error messages as clues rather than obstacles.\n\n\n\n8.2 Reading a traceback from top to bottom\nWhen a runtime error occurs, Python often produces a traceback. A traceback shows the sequence of steps Python followed before encountering the error.\ndef divide(a, b):\n    return a / b\n\ndivide(10, 0)\nThe traceback lists function calls from top to bottom, ending with the line where the error actually occurred. While the traceback may look intimidating at first, it follows a consistent structure.\nA useful strategy is to:\n- skim the traceback to understand the context,\n- then focus on the last line, which usually describes the actual error.\nThe file name and line number tell you where Python was executing when the problem occurred. This information narrows the search space and prevents unnecessary changes elsewhere in the code.\nOver time, reading tracebacks becomes a skill. Instead of seeing a wall of text, you begin to recognize patterns and quickly identify the relevant information.\n\n\n\n8.3 Common beginner errors\nCertain errors appear frequently when learning Python. These errors are predictable and shared by almost everyone at this stage.\nSyntax errors occur when Python cannot parse the code structure. Missing quotation marks, parentheses, or colons are common causes.\nName errors occur when a variable or function is used before it has been defined. This often results from spelling mistakes or assumptions about what exists in the current scope.\nType errors occur when an operation is applied to incompatible data types. These errors highlight the importance of understanding how different types behave.\nIndex and key errors occur when attempting to access elements that do not exist.\nvalues = [1, 2, 3]\nprint(values[3])\nThis error does not mean the list is broken. It means the program asked for something outside the valid range.\nRecognizing these errors as categories rather than isolated failures helps reduce frustration and speeds up debugging.\n\n\n\n8.4 Debugging as a systematic process\nEffective debugging is not about trying random fixes. It is about narrowing the problem space and testing assumptions deliberately.\nA systematic debugging process usually involves:\n- reproducing the error consistently,\n- identifying the exact location of failure,\n- inspecting the values of variables at that point,\n- and changing one thing at a time.\nOne of the simplest and most effective debugging tools is printing intermediate values.\nprint(\"Current value:\", x)\nBy inspecting program state as it runs, it becomes easier to understand how data flows through the code and where expectations diverge from reality.\nChanging multiple things at once makes debugging harder. When only one change is made, its effect is easier to interpret.\nDebugging rewards patience and methodical thinking. The goal is not to fix the error as quickly as possible, but to understand why the error occurred.\n\n\n\n8.5 When to fix code vs. rethink logic\nNot all bugs are caused by incorrect syntax or misuse of language features. Sometimes the code runs exactly as written, but the result is still wrong. In these cases, the issue lies in the logic, not the implementation.\ndef is_valid(score):\n    return score &gt; 100\nThis function works exactly as defined, but the condition may not reflect the intended rule. Debugging logic errors requires stepping back and examining assumptions.\nA useful question to ask is: Is Python doing something unexpected, or is Python doing exactly what I told it to do?\nIf the latter is true, the solution may involve redesigning conditions, rethinking data structures, or clarifying the problem definition rather than fixing syntax.\nDebugging is therefore part of program design. It is how understanding improves over time. Learning when to adjust code and when to rethink logic is one of the most valuable skills developed in this course."
  },
  {
    "objectID": "textbook_src/week02_main.html#week-2-summary",
    "href": "textbook_src/week02_main.html#week-2-summary",
    "title": "Week 2 – Python Foundations: Data, Control, and Functions",
    "section": "Week 2 Summary",
    "text": "Week 2 Summary\nThis chapter focused on building foundational Python skills by moving from individual values to structured data, decision-making, repetition, and modular program design. The goal was not just to learn Python syntax, but to develop a mental model of how programs reason about data and control their own behavior.\nConceptually, the chapter emphasized that data representation matters. Numbers, strings, booleans, lists, and dictionaries are not interchangeable; each type carries assumptions about how it can be used, combined, and transformed. Understanding these differences is essential for writing programs that behave predictably and for interpreting results correctly in analytics and AI contexts.\nThe introduction of control flow—conditional logic and loops—marked a shift from linear execution to decision-making and repetition. Programs were no longer treated as calculators that run once, but as systems that respond differently depending on conditions and that can operate over collections of data. This logic-based view of execution directly connects to how larger decision systems and algorithms function.\nFunctions extended this idea by introducing modularity and abstraction. By encapsulating logic into reusable units, programs become easier to read, test, and extend. Functions also reinforce the idea that complex behavior is built from smaller, well-defined components—a principle that scales from simple scripts to full analytics and AI systems.\nFinally, the chapter reframed debugging as a core reasoning skill rather than a remedial activity. Error messages, tracebacks, and unexpected results were treated as sources of information that guide understanding. Learning to debug systematically—by isolating problems, checking assumptions, and distinguishing between syntax issues and logic flaws—is a skill that will be used continuously in later weeks.\nBy the end of Week 2, you should be able to write Python programs that store and manipulate structured data, make decisions, repeat actions, and organize logic into functions. More importantly, you should be developing confidence in reading, reasoning about, and correcting code—skills that form the foundation for more advanced analytics and AI techniques in the weeks ahead."
  },
  {
    "objectID": "textbook_src/week04_main.html",
    "href": "textbook_src/week04_main.html",
    "title": "Week 4 – Simulation and Visualization: From Data to Insight",
    "section": "",
    "text": "How to generate synthetic data using random processes.\nWhy simulation is useful for understanding uncertainty and systems.\nHow visualization supports reasoning, exploration, and communication.\nHow to create and customize plots using matplotlib and seaborn.\nHow to move from cleaned data to visual insight in an integrated workflow.\nHow these skills connect to machine learning and LLM-based systems.\n\n\n\n\n\nPython scripts that simulate datasets.\nMultiple visualizations (line, bar, scatter, statistical plots).\nCustomized plots designed for clarity and interpretation.\nAn end-to-end visual analytics workflow.\nA mini-lab demonstrating raw data → insight.\n\n\n\n\nBy the end of this week, you should be able to:\n- Explain why simulation is useful for exploring uncertainty and behavior.\n- Describe how visualization supports analytical reasoning.\n- Interpret visual patterns and relate them to underlying data processes.\n- Explain how synthetic data is used in ML and AI development.\n\n\n\nBy the end of this week, you should be able to:\n- Generate synthetic datasets using Python.\n- Create basic plots using matplotlib.\n- Customize plots for readability and communication.\n- Use seaborn for statistical visualization.\n- Build a reproducible workflow from data generation to visualization."
  },
  {
    "objectID": "textbook_src/week04_main.html#week-4-overview",
    "href": "textbook_src/week04_main.html#week-4-overview",
    "title": "Week 4 – Simulation and Visualization: From Data to Insight",
    "section": "",
    "text": "How to generate synthetic data using random processes.\nWhy simulation is useful for understanding uncertainty and systems.\nHow visualization supports reasoning, exploration, and communication.\nHow to create and customize plots using matplotlib and seaborn.\nHow to move from cleaned data to visual insight in an integrated workflow.\nHow these skills connect to machine learning and LLM-based systems.\n\n\n\n\n\nPython scripts that simulate datasets.\nMultiple visualizations (line, bar, scatter, statistical plots).\nCustomized plots designed for clarity and interpretation.\nAn end-to-end visual analytics workflow.\nA mini-lab demonstrating raw data → insight.\n\n\n\n\nBy the end of this week, you should be able to:\n- Explain why simulation is useful for exploring uncertainty and behavior.\n- Describe how visualization supports analytical reasoning.\n- Interpret visual patterns and relate them to underlying data processes.\n- Explain how synthetic data is used in ML and AI development.\n\n\n\nBy the end of this week, you should be able to:\n- Generate synthetic datasets using Python.\n- Create basic plots using matplotlib.\n- Customize plots for readability and communication.\n- Use seaborn for statistical visualization.\n- Build a reproducible workflow from data generation to visualization."
  },
  {
    "objectID": "textbook_src/week04_main.html#data-simulation-simulating-data-and-random-processes",
    "href": "textbook_src/week04_main.html#data-simulation-simulating-data-and-random-processes",
    "title": "Week 4 – Simulation and Visualization: From Data to Insight",
    "section": "1. Data Simulation: Simulating Data and Random Processes",
    "text": "1. Data Simulation: Simulating Data and Random Processes\nSimulation is a powerful technique for exploring data-driven systems when real-world data is limited, expensive, sensitive, or simply unavailable. Rather than waiting for data to exist, simulation allows analysts to create data intentionally in order to understand behavior, uncertainty, and system dynamics.\nThis section introduces simulation as both a practical coding skill and a conceptual tool for reasoning about analytics and AI systems.\n\n\n1.1 Why simulate data\nIn an ideal world, every analytical question would be answered using high-quality, representative real-world data. In practice, this is rarely the case. Real datasets often come with limitations that make analysis difficult or incomplete.\nSome common challenges include:\n- data that is incomplete or missing important cases,\n- data that is biased due to how it was collected,\n- data that is proprietary, sensitive, or restricted,\n- data that does not yet exist because the system is new or hypothetical.\nSimulation provides a way to work around these limitations. By generating synthetic data, analysts can explore how systems behave under controlled assumptions. This makes it possible to ask “what if” questions, such as:\n- What happens if inputs change?\n- How sensitive are outcomes to randomness?\n- What patterns would we expect to see under certain conditions?\nIt is important to distinguish simulation from data collection. Data collection records what has already happened in the real world. Simulation, by contrast, creates plausible data based on assumptions about how a system might behave. Simulated data is not a replacement for real data, but it is a complementary tool.\nIn analytics and AI, simulation plays several important roles:\n- testing analysis logic before real data is available,\n- stress-testing models under different scenarios,\n- building intuition about variability and uncertainty,\n- generating training or evaluation data when real examples are scarce.\nConceptually, simulation answers the question:\nIf the world behaved according to these assumptions, what kinds of data and patterns would we observe?\nUnderstanding simulation helps shift thinking away from treating datasets as fixed artifacts and toward seeing data as the result of underlying processes.\n\n\n\n1.2 Randomness and stochastic processes\nAt the heart of most simulations is randomness. In everyday language, randomness often implies unpredictability or lack of structure. In computation, randomness has a more precise meaning.\nComputers do not generate truly random numbers. Instead, they use pseudo-random number generators, which produce sequences of numbers that appear random but are generated deterministically by an algorithm. These sequences are sufficiently unpredictable for most analytical purposes, while still being reproducible.\nPython provides built-in tools for generating pseudo-random values. For example:\nimport random\nrandom.random()\nEach call to random.random() returns a floating-point number between 0 and 1. Over many calls, these values form a distribution that can be used to simulate uncertainty, noise, or variability in a system.\nA process that involves randomness is often called a stochastic process. Stochastic processes are used to model systems where outcomes are influenced by chance, such as customer arrivals, demand fluctuations, or measurement error. Rather than producing a single deterministic outcome, stochastic processes produce distributions of possible outcomes.\nAn important concept related to pseudo-randomness is repeatability. Because random number generation is algorithmic, it can be controlled using a seed. Setting a seed causes the same sequence of “random” values to be generated each time the program runs. This is essential for debugging, testing, and scientific reproducibility.\nAt a conceptual level, randomness in simulation is not about chaos, it is about controlled variability. By introducing randomness deliberately, analysts can explore how systems behave across many possible realizations rather than relying on a single outcome.\nThis idea will recur throughout analytics and AI. Models often rely on random initialization, randomized sampling, or stochastic optimization. Understanding how randomness is generated and used in simulation builds intuition for why repeated runs may produce slightly different results and why variability itself is something to be analyzed rather than ignored.\n\n\n\n1.3 Generating synthetic datasets\nOnce randomness is available as a tool, the next step is to use it to generate synthetic datasets. A synthetic dataset is a collection of simulated observations designed to resemble data that might plausibly be observed from a real process.\nSynthetic datasets are especially useful for experimentation. They allow analysts to explore patterns, test code, and validate logic without relying on external data sources. The goal is not realism for its own sake, but control and clarity.\n\n\nSimulating numeric variables\nThe simplest form of simulation involves generating numeric values. For example, random numbers between 0 and 1 can be used to represent proportions, probabilities, noise, or normalized measurements.\nvalues = [random.random() for _ in range(100)]\nIn this example, a list of 100 simulated values is created. Each value is generated independently, but together they form a dataset that can be summarized, visualized, or transformed.\nAlthough this dataset is simple, it already supports many analytical questions:\n- What does the distribution look like?\n- How much variability is present?\n- How do summaries change as the number of observations increases?\nSimulated numeric variables provide a controlled environment for developing intuition about distributions and variability.\n\n\n\nSimulating categorical variables\nNot all data is numeric. Many datasets include categorical variables, such as labels, group memberships, or types. These can also be simulated by randomly assigning categories according to specified rules.\nFor example, categories might be assigned uniformly at random, or with different probabilities to reflect imbalance. Although the mechanics differ slightly from numeric simulation, the underlying idea is the same: define a process, then generate observations from it.\nCategorical simulation is useful for:\n- testing grouping and aggregation logic,\n- exploring how category imbalance affects summaries,\n- preparing for visualization and comparison tasks.\nEven when categories are simulated, they behave like real categorical data in downstream analysis.\n\n\n\nCombining multiple simulated variables into datasets\nReal datasets rarely consist of a single variable. More commonly, each observation includes multiple attributes. Synthetic datasets can be constructed by simulating several variables independently or jointly and then combining them into a structured dataset.\nFor example, one variable might represent a numeric measurement, while another represents a category or group. Together, these variables form a dataset that more closely resembles real analytical inputs.\nCombining simulated variables allows analysts to test how different components of an analysis interact:\n- filtering by category,\n- summarizing numeric values within groups,\n- visualizing relationships between variables.\nAt this stage, the emphasis is on structure rather than realism. The goal is to create datasets that support reasoning about code and analysis logic.\n\n\n\nUsing simulation to test analysis logic\nOne of the most important uses of simulation is testing analysis logic before applying it to real data. Because synthetic data is generated under known assumptions, it becomes easier to reason about whether code behaves as intended.\nFor example: - If a filter is applied, does it select the expected data?\n- If a summary statistic is computed, does it behave sensibly as data size changes?\n- If a visualization is produced, does it reflect the underlying data-generating process?\nSimulation turns analysis into an experiment. By controlling inputs and observing outputs, analysts can build confidence that their workflow is correct before introducing the messiness of real-world data.\nThis practice is common in analytics and AI development. Models, pipelines, and visualizations are often tested on synthetic data first, then refined using real data once logic is validated. Sometimes this is to test models viability for a certain task, and often this is related to the cost (in time, dollars, effort) to test a model on a real world dateset.\nConceptually, generating synthetic datasets answers the question: If my analysis pipeline is correct, does it behave sensibly on data generated from known processes? You could simply this to thinking about a synthetic data set as a data set you have created, with known relationships beween the variables. And now you want to understand if your model can identify/capture those relationships.\nThis mindset, testing logic under controlled conditions, will continue to be important as you continue to learn machine learning and AI."
  },
  {
    "objectID": "textbook_src/week04_main.html#visualization-foundations-why-visualization-matters",
    "href": "textbook_src/week04_main.html#visualization-foundations-why-visualization-matters",
    "title": "Week 4 – Simulation and Visualization: From Data to Insight",
    "section": "2. Visualization Foundations: Why Visualization Matters",
    "text": "2. Visualization Foundations: Why Visualization Matters\nAs datasets grow in size and complexity, tables of numbers alone become difficult to interpret. Visualization provides a way to translate data into forms that align with human perception, making patterns, relationships, and anomalies easier to detect. This section introduces visualization as a core analytical tool rather than an optional presentation step.\n\n\n2.1 Visualization as a reasoning tool\nVisualization supports reasoning, not just communication. While tables are precise and compact, they place a heavy cognitive burden on the reader. Detecting patterns in rows of numbers requires careful scanning and mental comparison, which becomes increasingly difficult as datasets grow.\nVisual representations leverage the strengths of human perception. Humans are especially good at detecting:\n- trends and changes over time,\n- differences in magnitude,\n- clusters and outliers,\n- and relationships between variables.\nBy mapping data values to visual properties such as position, length, or shape, visualizations make these patterns immediately apparent. A trend that might be difficult to notice in a table often becomes obvious when plotted.\nIt is also important to distinguish between visualization for exploration and visualization for presentation. Exploratory visualizations are created to help the analyst understand the data. They may be rough, iterative, and rapidly discarded. Presentation visualizations, by contrast, are designed to communicate a specific message clearly to an audience.\nIn analytics and AI workflows, visualization often begins as an exploratory activity. Before formal modeling or reporting, visual inspection helps clarify what the data contains, what questions are reasonable, and where potential issues may lie.\nConceptually, visualization answers the question:\nWhat patterns or structures might exist in this data that are hard to see numerically?\n\n\n\n2.2 Visualization in analytics and AI\nVisualization plays a central role throughout analytics and AI pipelines. Early in the process, visualizations are used to examine distributions, trends, and relationships within data. These views help analysts understand scale, variability, and potential anomalies before applying statistical or machine learning models.\nVisualization also functions as a diagnostic tool. Unexpected patterns in a plot may reveal data quality issues, incorrect assumptions, or errors in preprocessing. For example, a strange spike or gap in a distribution might indicate missing values, mis-coded categories, or unit mismatches.\nIn AI contexts, visualization remains important even after models are introduced. Visual diagnostics are often used to:\n- examine model inputs and outputs,\n- compare predicted values to actual outcomes,\n- assess residuals or errors,\n- and monitor model behavior over time.\nAlthough models may operate mathematically, their behavior must still be interpreted by humans. Visualization provides one of the most effective ways to bridge this gap between computation and understanding.\nAcross analytics and AI, visualization serves a consistent purpose: it reduces complexity by externalizing patterns that would otherwise remain hidden in raw data or numerical summaries. Used thoughtfully, visualization supports better reasoning, better debugging, and better decisions."
  },
  {
    "objectID": "textbook_src/week04_main.html#matplotlib-basics-simple-plots",
    "href": "textbook_src/week04_main.html#matplotlib-basics-simple-plots",
    "title": "Week 4 – Simulation and Visualization: From Data to Insight",
    "section": "3. Matplotlib Basics: Simple Plots",
    "text": "3. Matplotlib Basics: Simple Plots\nMatplotlib is one of the foundational libraries for creating visualizations in Python. While higher-level libraries exist, matplotlib provides the underlying plotting system on which many other visualization tools are built. Understanding its basics gives you direct control over how data is visualized and prepares you to reason about more advanced plotting tools later on.\nThis section introduces three of the most common plot types—line plots, bar plots, and scatter plots—and explains when each is appropriate.\n\n\n3.1 What matplotlib is\nMatplotlib is a general-purpose plotting library designed to create static, two-dimensional visualizations. It allows Python programs to translate numerical data into visual form by mapping values to positions, shapes, and other visual properties.\nAt a high level, matplotlib works by creating a figure, which acts as a container for one or more plots, and axes, which define the coordinate system within that figure. Although these concepts can be explored in more detail later, it is sufficient at this stage to understand that every plot exists within a figure and is drawn on axes.\nMatplotlib is intentionally flexible. This flexibility can make it feel verbose at first, but it also allows precise control over visual output. Many other visualization libraries build on top of matplotlib, so familiarity with its basic patterns is a long-term investment.\n\n\n\n3.2 Line plots\nA line plot connects a sequence of data points with lines. Line plots are commonly used to visualize trends over time or over an ordered sequence of observations.\nimport matplotlib.pyplot as plt\nplt.plot(values)\nIn this example, the values are plotted in the order they appear. The horizontal axis represents the position or index of each value, while the vertical axis represents the value itself.\nLine plots are appropriate when:\n- the order of observations matters,\n- data represents a progression over time or sequence,\n- or the goal is to see trends, patterns, or changes.\nUsing a line plot for unordered categories can be misleading, because the connecting lines imply continuity where none exists. Choosing a line plot therefore reflects an assumption about the underlying data structure.\n\n\n\n3.3 Bar plots\nA bar plot represents values associated with discrete categories using rectangular bars. Bar plots are commonly used to compare quantities across groups or categories.\nplt.bar([\"A\", \"B\", \"C\"], [10, 15, 7])\nIn this example, each category is represented by a bar whose height corresponds to a value. Unlike line plots, bar plots do not imply continuity between categories. Each bar stands on its own.\nBar plots are appropriate when:\n- comparing values across categories,\n- visualizing aggregated statistics,\n- or emphasizing differences between groups.\nBecause bar plots emphasize magnitude differences, they are often used in descriptive analysis and reporting. Care should be taken to ensure that axes and scales are chosen thoughtfully, as visual exaggeration or compression can distort interpretation.\n\n\n\n3.4 Scatter plots\nA scatter plot displays individual observations as points in a two-dimensional space. Each point represents a pair of values, one on the horizontal axis and one on the vertical axis.\nplt.scatter(x, y)\nScatter plots are used to visualize relationships between variables. Patterns such as clustering, trends, or outliers are often visible in scatter plots even when they are not obvious numerically.\nScatter plots are appropriate when:\n- examining the relationship between two numeric variables,\n- exploring correlation or association,\n- or identifying unusual observations.\nIt is important to distinguish between correlation and causation. A visible relationship in a scatter plot does not imply that one variable causes the other to change. Scatter plots support exploration and hypothesis generation, not definitive conclusions.\nTogether, line plots, bar plots, and scatter plots form a basic visual vocabulary. Choosing the appropriate plot type is a substantive analytical decision that reflects assumptions about data structure and the questions being asked."
  },
  {
    "objectID": "textbook_src/week04_main.html#matplotlib-basics-customizing-plots",
    "href": "textbook_src/week04_main.html#matplotlib-basics-customizing-plots",
    "title": "Week 4 – Simulation and Visualization: From Data to Insight",
    "section": "4. Matplotlib Basics: Customizing Plots",
    "text": "4. Matplotlib Basics: Customizing Plots\nCreating a plot is only the first step in visualization. A plot that is technically correct can still be confusing, misleading, or difficult to interpret. Customization is what turns a raw plot into a visual that supports reasoning and communication.\nThis section focuses on simple but essential customization techniques that improve clarity and help ensure that visualizations convey the intended message.\n\n\n4.1 Why customization matters\nVisualization works by mapping data to visual properties. If those properties are unclear or poorly chosen, interpretation suffers—even when the underlying data is sound.\nCustomization matters for two main reasons. First, it improves readability. Titles, labels, and legends help viewers understand what they are looking at without guessing. Second, it helps avoid misleading visuals. Poor choices in scale, labeling, or color can suggest patterns that are not actually present or hide patterns that are.\nUncustomized plots often rely on default settings that may not match the context of the data. Defaults are designed to work reasonably well in general, but they are not tailored to specific analytical questions. Customization allows the analyst to align the visual with intent.\n\n\n\n4.2 Titles, labels, and legends\nTitles, axis labels, and legends provide essential context. Without them, viewers must infer meaning from the shape of the plot alone, which increases cognitive load and the risk of misinterpretation.\nA title describes what the plot represents. It should be concise but informative, indicating what is being visualized and, when appropriate, under what conditions.\nAxis labels explain what each axis represents, including units when relevant. Clear labeling makes it easier to interpret magnitudes and compare values.\nplt.title(\"Example Plot\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nWhen a plot includes multiple series or categories, a legend helps distinguish between them. Legends map visual elements, such as colors or markers, to their meanings.\nTogether, titles, labels, and legends are used to turn a plot from a visual pattern into an interpretable artifact. They make the assumptions and structure of the visualization explicit. As your knowledge in analytics grows, it becomes more and more important to use these tools. It is not uncommon to see experts with extremely poorly annotated visuals, because they know them so well!\n\n\n\n4.3 Colors and styles\nColor is a powerful visual cue, but it must be used intentionally. Color choices can highlight differences, group related elements, or draw attention to specific features. At the same time, excessive or inconsistent use of color can create visual clutter and confusion.\nGood color usage follows a few general principles:\n- Use color to encode information, not decoration.\n- Keep color palettes simple and consistent.\n- Avoid using too many colors at once.\n- Ensure sufficient contrast for readability.\nColor can be problematic, if it is the only signal in a visualization. Imagine a chart with only red and green dots, the red dots are “bad” and the green dots are “good”. This chart would be entirely useless to someone who is color blind.\nConsistency across plots is especially important in analytical workflows. When similar plots use similar styles, viewers can focus on the data rather than re-learning how to read each visualization.\nStyles also include choices about markers, line thickness, and layout. These choices affect how easily patterns can be detected and compared. Small adjustments can significantly improve interpretability without adding complexity.\nCustomization is not about making plots look impressive. It is about making them honest, readable, and aligned with analytical intent. As visualizations become more central to reasoning and communication, thoughtful customization becomes a core analytical skill rather than a cosmetic one."
  },
  {
    "objectID": "textbook_src/week04_main.html#seaborn-statistical-visualization-optional",
    "href": "textbook_src/week04_main.html#seaborn-statistical-visualization-optional",
    "title": "Week 4 – Simulation and Visualization: From Data to Insight",
    "section": "5. Seaborn: Statistical Visualization (Optional)",
    "text": "5. Seaborn: Statistical Visualization (Optional)\nSeaborn is a higher-level visualization library built on top of matplotlib. While matplotlib provides fine-grained control over plots, seaborn focuses on making statistical patterns easier to visualize with less code. This makes Seaborn very useful as a tool. It is faster and easier to create reasonable looking graphics and charts in Seaborn, but you don’t get the same level of fine tuned control you get in matplotlib. As LLM and coding tools get more powerful, I think we will see more shift towards the highly customizable graphing solutions, as the code to create it becomes easier/faster/cheaper to generate.\nThe core ideas of simulation and visualization do not depend on seaborn, but seaborn provides a useful preview of how visualization tools evolve toward higher-level abstractions.\n\n\n5.1 Why seaborn exists\nMatplotlib is highly flexible, but that flexibility can require substantial setup when creating plots that emphasize statistical structure. Seaborn was designed to address this gap by providing functions that automatically apply common statistical conventions and aesthetically sensible defaults.\nSeaborn builds on matplotlib rather than replacing it. When seaborn creates a plot, it is still using matplotlib underneath, but it handles many details, such as color palettes, axes formatting, and statistical aggregation, automatically.\nThe key distinction is focus: - Matplotlib emphasizes low-level control and general-purpose plotting. - Seaborn emphasizes statistical visualization and common analytical patterns.\nBecause of this focus, seaborn is often used for exploratory data analysis, where quickly understanding distributions, relationships, and group differences is more important than precise customization.\n\n\n\n5.2 Distribution plots\nOne of seaborn’s strengths is its support for distribution plots. Distribution plots visualize how values are spread across a range, making it easier to reason about variability, skewness, and concentration.\nimport seaborn as sns\nsns.histplot(values)\nThis example creates a histogram that shows the distribution of simulated values. Seaborn automatically selects binning, applies sensible styling, and labels the axes in a way that emphasizes the statistical meaning of the plot.\nDistribution plots are useful for: - examining the shape of a variable, - comparing distributions across groups, - identifying skew, outliers, or unusual patterns.\nCompared to basic matplotlib histograms, seaborn’s distribution plots reduce setup effort and encourage consistent visual conventions. This makes it easier to focus on interpretation rather than configuration.\nAlthough seaborn is optional in this course, understanding why it exists reinforces a broader idea: as analytical tasks become more complex, tools evolve to capture common patterns and reduce repetitive work. Learning to recognize when higher-level tools are appropriate is part of developing analytical maturity."
  },
  {
    "objectID": "textbook_src/week04_main.html#integrated-workflow-from-data-to-visualization",
    "href": "textbook_src/week04_main.html#integrated-workflow-from-data-to-visualization",
    "title": "Week 4 – Simulation and Visualization: From Data to Insight",
    "section": "6. Integrated Workflow: From Data to Visualization",
    "text": "6. Integrated Workflow: From Data to Visualization\nUp to this point, simulation and visualization have been introduced as individual skills. In practice, however, these activities are rarely isolated. Analytics workflows typically involve moving back and forth between data generation, summarization, and visualization, using each step to inform the next.\nThis section brings those pieces together into a single, integrated workflow. The emphasis is not on writing new code, but on understanding how analytical intent guides the sequence of steps from data to insight.\n\n\n6.1 Simulate or load a dataset\nEvery analysis begins with a dataset. That dataset may come from a real source, such as a cleaned CSV file from Week 3, or it may be generated synthetically through simulation. The choice depends on the goal of the analysis.\nSimulation is often used when: - exploring hypothetical scenarios, - testing analysis logic, - or building intuition before working with real data.\nReal datasets are used when: - answering specific empirical questions, - validating assumptions against observed behavior, - or producing results tied to real-world systems.\nRegardless of origin, the key step is to establish analytical intent. Before summarizing or plotting, it should be clear what question the data is meant to support. This intent shapes every downstream decision, from which variables matter to which visualizations are appropriate.\nAt this stage, the dataset, simulated or loaded, becomes the shared object that connects all subsequent analysis.\n\n\n\n6.2 Summarize before visualizing\nVisualization is most effective when it is informed by prior understanding of the data. Jumping directly to plotting without summarization can lead to confusion, misinterpretation, or unnecessary visual complexity.\nSummaries provide essential context: - typical values, - variability and range, - presence of outliers, - and differences between variables.\nThese summaries help determine what kind of visualization makes sense. For example, understanding whether a variable is tightly clustered or widely dispersed influences choices about scale and plot type.\nThis step connects directly back to Week 3, where descriptive statistics were used to make sense of datasets before deeper analysis. Visualization builds on that foundation rather than replacing it.\nConceptually, summarization answers the question:\nWhat should I expect to see before I look at a plot?\nWhen visualizations align with prior summaries, confidence in interpretation increases. When they do not, it signals the need to revisit assumptions or investigate potential issues in the data.\n\n\n\n6.3 Visualize insights\nWith a clear dataset and informed expectations, visualization becomes a powerful tool for insight. At this stage, the goal is not to produce as many plots as possible, but to choose appropriate plot types that align with the analytical question.\nDifferent questions call for different visuals: - trends and sequences suggest line plots, - category comparisons suggest bar plots, - relationships between variables suggest scatter plots, - distributions suggest histograms or density plots.\nChoosing a plot type is a substantive analytical decision. It reflects assumptions about the data and about what patterns are meaningful. Once a plot is created, interpretation becomes the focus.\nInterpreting visuals involves asking: - What patterns are visible? - Do these patterns align with expectations? - Are there anomalies or surprises? - What explanations are plausible given how the data was generated or collected?\nIn an integrated workflow, visualization does not mark the end of analysis. Instead, it often leads back to earlier steps. A surprising plot may prompt additional summarization, refined filtering, or even changes to the simulation process.\nThis iterative movement from data → summary → visualization → reasoning, is characteristic of real analytics and AI work. Understanding this cycle is more important than mastering any single plotting function."
  },
  {
    "objectID": "textbook_src/week04_main.html#mini-lab-raw-data-to-visual-insights",
    "href": "textbook_src/week04_main.html#mini-lab-raw-data-to-visual-insights",
    "title": "Week 4 – Simulation and Visualization: From Data to Insight",
    "section": "7. Mini-Lab: Raw Data to Visual Insights",
    "text": "7. Mini-Lab: Raw Data to Visual Insights\nThis mini-lab brings together the key ideas from Week 4 into a single, end-to-end workflow. The objective is not to introduce new techniques, but to practice integrating simulation, summarization, and visualization in a way that mirrors real analytical work.\nRather than treating each step as isolated, this lab emphasizes how decisions made early in the process shape what can be learned later.\n\n\n7.1 Simulating a dataset\nThe workflow begins by generating a synthetic dataset. Simulation provides full control over how the data is created, which makes it easier to reason about expected patterns and variability.\nimport random\n\nvalues = [random.random() for _ in range(200)]\nThis simulated dataset represents a simple numeric variable. Although minimal, it is sufficient to explore distributional properties, variability, and visualization choices. In more complex scenarios, multiple simulated variables could be combined into a structured dataset.\nAt this stage, the goal is to establish a clear understanding of how the data was generated. Knowing the data-generating process makes later interpretation more grounded and less speculative.\n\n\n\n7.2 Cleaning and summarizing\nEven simulated data benefits from inspection and summarization. Before visualizing, it is important to understand basic properties such as scale, range, and typical values.\nmin_value = min(values)\nmax_value = max(values)\naverage = sum(values) / len(values)\nThese summaries provide expectations for what the visualizations should reveal. For example, if values are uniformly distributed between 0 and 1, the average should be near the midpoint, and the range should reflect the bounds of the simulation.\nThis step mirrors the Week 3 workflow, where inspection and descriptive statistics preceded visualization. The same discipline applies here: visualization is most useful when informed by prior understanding.\n\n\n\n7.3 Visual exploration\nWith a summarized dataset, the next step is visual exploration. Visualization allows patterns to be assessed quickly and intuitively.\nimport matplotlib.pyplot as plt\n\nplt.hist(values)\nThis plot provides a visual representation of how values are distributed. Adjustments such as adding titles, labels, or changing bin counts can improve clarity and make interpretation easier.\nplt.title(\"Distribution of Simulated Values\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nCreating multiple plots, such as line plots of cumulative values or comparisons across simulated groups, can further enrich understanding. The goal is not to generate as many plots as possible, but to create visuals that directly support reasoning about the data.\nRefinement is part of the process. If a plot is confusing or uninformative, it should be adjusted or replaced. Visualization is iterative, not one-shot.\n\n\n\n7.4 Interpreting visual results\nThe final step is interpretation. Visualizations do not produce answers on their own; they require reasoning about what is shown and how it connects to the data-generating process.\nKey questions to consider include: - Do the visual patterns align with how the data was simulated? - Are there unexpected features or anomalies? - How sensitive are the visuals to changes in sample size or plotting choices? - What additional questions does the visualization raise?\nBecause the data is synthetic, discrepancies between expectation and observation are especially informative. They may indicate randomness at work, limitations of small samples, or issues in how the visualization was constructed.\nThis mini-lab demonstrates a complete analytical cycle: generate data, summarize it, visualize it, and reason about the results. The same cycle will appear repeatedly in more advanced analytics and AI contexts, even as datasets and models become more complex."
  },
  {
    "objectID": "textbook_src/week04_main.html#week-4-summary",
    "href": "textbook_src/week04_main.html#week-4-summary",
    "title": "Week 4 – Simulation and Visualization: From Data to Insight",
    "section": "Week 4 Summary",
    "text": "Week 4 Summary\nThis chapter introduced two closely related ideas: simulation as a way to generate data deliberately, and visualization as a way to reason about data effectively. Together, these skills support a shift from treating datasets as fixed artifacts to understanding them as the output of underlying processes that can be explored, tested, and interpreted.\nSimulation was presented as a practical response to common limitations of real-world data. When data is incomplete, biased, restricted, or not yet available, synthetic data provides a controlled environment for experimentation. By using pseudo-random number generation, simulation makes uncertainty explicit and enables systematic “what if” exploration. The chapter emphasized that randomness in computation is not chaos, but controlled variability, a tool for understanding how outcomes can change across repeated realizations of the same process.\nVisualization was framed as a reasoning tool rather than a presentation artifact. Visual representations reduce cognitive load and leverage human strengths in detecting patterns, trends, and anomalies. The chapter distinguished exploratory visualization from presentation visualization and highlighted visualization’s role as a diagnostic mechanism in analytics and AI workflows. Matplotlib provided a practical foundation for producing basic plots, while customization was treated as a substantive analytical responsibility, necessary for clarity, interpretability, and avoiding misleading impressions. An optional seaborn section introduced how higher-level visualization tools capture common statistical patterns and reduce repetitive plotting work.\nThe integrated workflow and mini-lab reinforced a complete analytical cycle: generate or obtain data → summarize → visualize → interpret → iterate. This cycle mirrors how real analytics work is performed and prepares the ground for the next stage of the course, where the focus will shift from describing and visualizing data to modeling it. By the end of Week 4, you should be able to generate synthetic data intentionally, visualize it thoughtfully, and interpret what you see in a way that reflects both the data-generating process and the analytical choices made along the way."
  },
  {
    "objectID": "guides/install_python.html",
    "href": "guides/install_python.html",
    "title": "Setting Up Python",
    "section": "",
    "text": "This document will walk you through the steps to:\n\nInstall Python 3\n\nInstall Visual Studio Code (VS Code)\n\nConfigure VS Code to run .py files\n\nVerify that everything is working correctly\n\nYou only need to complete this setup once on your computer.\nAfter that, you will be able to run all course examples and labs.\n\n\n\n\n\n\nTip\n\n\n\nIf you get stuck at any point:\nInstalling this should be straightforward. LLMs can be a great way to problem solve an install. Use UFs Navigator Chat to help you. There is an optional video on this in the course files. Still stuck? Reach out to a TA and let them help you out. Use the course guidelines for how to ask good questions to request help."
  },
  {
    "objectID": "guides/install_python.html#on-windows",
    "href": "guides/install_python.html#on-windows",
    "title": "Setting Up Python",
    "section": "On Windows?",
    "text": "On Windows?\n\nOpen the Start Menu.\n\nType cmd and press Enter to open the Command Prompt.\n\nIn the black window, type:\n\npy --version\nor\npython --version\n\nPress Enter.\n\n\nIf you see something like Python 3.10.11, Python 3 is already installed.\n\nIf you see an error or a much older version (like Python 2.x), you should install a current version.\nMost MAC’s will have Python installed, but this is for your system, you want to install it again. We don’t want to be making changes to the factory installed Python."
  },
  {
    "objectID": "guides/install_python.html#on-macos",
    "href": "guides/install_python.html#on-macos",
    "title": "Setting Up Python",
    "section": "on macOS?",
    "text": "on macOS?\n\nOpen Terminal (Spotlight → type Terminal).\n\nType:\n\npython3 --version\n\nPress Enter.\n\n\nIf you see Python 3.x.x, you have Python 3.\n\nFor the most part, you should install a new version, even if you see this."
  },
  {
    "objectID": "guides/install_python.html#on-linux",
    "href": "guides/install_python.html#on-linux",
    "title": "Setting Up Python",
    "section": "on Linux?",
    "text": "on Linux?\n\nOpen a terminal.\n\nType:\n\npython3 --version\n\nPress Enter.\n\nIf Python 3 is not installed, follow the installation steps below."
  },
  {
    "objectID": "guides/install_python.html#windows-installation",
    "href": "guides/install_python.html#windows-installation",
    "title": "Setting Up Python",
    "section": "Windows Installation",
    "text": "Windows Installation\n\nClick Download Python 3.x.x for Windows.\n\nRun the installer (python-3.x.x.exe) after it downloads.\n\nOn the first screen:\n\nCheck the box that says:\n“Add Python 3.x to PATH”\n\nThen click “Install Now”.\n\nWait for the installation to finish.\n\nClick “Close” when done.\n\n\nVerify Python on Windows\nOpen Command Prompt again and type:\npy --version\nor\npython --version\nYou should now see Python 3.x.x.\nIf you don’t, restart your computer and try again."
  },
  {
    "objectID": "guides/install_python.html#macos-installation",
    "href": "guides/install_python.html#macos-installation",
    "title": "Setting Up Python",
    "section": "macOS Installation",
    "text": "macOS Installation\n\nOn https://www.python.org/downloads/, choose “Download Python 3.x.x for macOS”.\n\nOpen the downloaded .pkg file.\n\nFollow the installer prompts and accept the defaults.\n\n\nVerify Python on macOS\nOpen Terminal and type:\npython3 --version\nYou should see Python 3.x.x.\nIf you don’t, log out and back in, or restart your machine."
  },
  {
    "objectID": "guides/install_python.html#linux-installation-brief",
    "href": "guides/install_python.html#linux-installation-brief",
    "title": "Setting Up Python",
    "section": "Linux Installation (brief)",
    "text": "Linux Installation (brief)\nOn many Linux distributions, Python 3 is already installed.\nIf not, you can install it with your package manager. For example:\n\nUbuntu / Debian:\nsudo apt-get update\nsudo apt-get install python3 python3-pip\nFedora:\nsudo dnf install python3 python3-pip\n\nAfter installation, check:\npython3 --version"
  },
  {
    "objectID": "guides/install_python.html#download-and-install-vs-code",
    "href": "guides/install_python.html#download-and-install-vs-code",
    "title": "Setting Up Python",
    "section": "Download and Install VS Code",
    "text": "Download and Install VS Code\n\nGo to: https://code.visualstudio.com/\n\nClick Download for your operating system (Windows, macOS, or Linux).\n\nRun the installer and accept the defaults."
  },
  {
    "objectID": "guides/install_python.html#install-the-python-extension-in-vs-code",
    "href": "guides/install_python.html#install-the-python-extension-in-vs-code",
    "title": "Setting Up Python",
    "section": "Install the Python Extension in VS Code",
    "text": "Install the Python Extension in VS Code\n\nOpen VS Code.\n\nOn the left sidebar, click the Extensions icon (four squares).\n\nIn the search bar, type: Python.\n\nFind the extension published by Microsoft (it should be at the top).\n\nClick Install.\n\nVS Code will now understand and support Python files."
  },
  {
    "objectID": "guides/install_python.html#running-the-script-vs-code-integrated-terminal",
    "href": "guides/install_python.html#running-the-script-vs-code-integrated-terminal",
    "title": "Setting Up Python",
    "section": "Running the Script (VS Code Integrated Terminal)",
    "text": "Running the Script (VS Code Integrated Terminal)\n\nIn VS Code, open the Terminal:\n\nView → Terminal, or\n\nPress Ctrl+` (Windows/Linux)\n\nPress Ctrl+Shift+` (macOS, depending on keybindings)\n\nMake sure the terminal is using the correct folder. You should see a path that ends with week01.\nType one of the following commands and press Enter:\n\nOn Windows:\npy test_setup.py\nor\npython test_setup.py\nOn macOS/Linux:\npython3 test_setup.py\n\nYou should see:\n\nPython is working, awesome!\nIf you see that message, your environment is ready."
  },
  {
    "objectID": "guides/install_python.html#python-is-not-recognized-on-windows",
    "href": "guides/install_python.html#python-is-not-recognized-on-windows",
    "title": "Setting Up Python",
    "section": "“Python is not recognized” on Windows",
    "text": "“Python is not recognized” on Windows\nIf you see:\n'python' is not recognized as an internal or external command\nor\n'py' is not recognized...\nTry:\n\nRestarting your computer.\n\nRunning the installer again and making sure “Add Python to PATH” is checked."
  },
  {
    "objectID": "guides/install_python.html#scripts-not-finding-data-files",
    "href": "guides/install_python.html#scripts-not-finding-data-files",
    "title": "Setting Up Python",
    "section": "Scripts not finding data files",
    "text": "Scripts not finding data files\nMake sure:\n\nYour .py script and your data files are in the same folder, or\n\nYour script uses the correct relative path, like:\n\nimport pandas as pd\n\ndf = pd.read_csv(\"data/housing_prices.csv\")\nIf you move files around, update the paths accordingly."
  }
]